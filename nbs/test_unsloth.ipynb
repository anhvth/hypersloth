{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "171d506f-0422-4160-94e3-556d450f77e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd /home/anhvth5/hypersloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05b3046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "# Install latest Hugging Face for Gemma-3!\n",
    "!pip install --no-deps git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d11a5bf7-0a4e-440d-87ba-ecbe8d441959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0' # Choose any GPU you want to use\n",
    "# import torch\n",
    "# from unsloth import FastModel\n",
    "\n",
    "\n",
    "# fourbit_models = [\n",
    "#     # 4bit dynamic quants for superior accuracy and low memory use\n",
    "#     \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
    "#     \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "#     \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "#     \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "\n",
    "#     # Other popular models!\n",
    "#     \"unsloth/Llama-3.1-8B\",\n",
    "#     \"unsloth/Llama-3.2-3B\",\n",
    "#     \"unsloth/Llama-3.3-70B\",\n",
    "#     \"unsloth/mistral-7b-instruct-v0.3\",\n",
    "#     \"unsloth/Phi-4\",\n",
    "# ] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "# model, tokenizer = FastModel.from_pretrained(\n",
    "#     model_name = \"unsloth/Qwen2.5-1.5B-Instruct\",\n",
    "#     max_seq_length = 16_000, # Choose any for long context!\n",
    "#     load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "#     load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "#     full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b076f97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.3.16: Fast Qwen2 patching. Transformers: 4.50.0.dev0.\n",
      "   \\\\   /|    NVIDIA L40S. Num GPUs = 8. Max memory: 44.403 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edad20e157f844e7b54fdb2617012af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/950 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ac9b1d6f7a4ea69d3c4046655a6151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-19 16:40:59.019\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mHyperSloth.transformer_trainer_setup\u001b[0m:\u001b[36msetup_model_and_training\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mModel setup complete for GPU 0\u001b[0m\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXAMPLE #1 ===\n",
      "\u001b[93m<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "{\n",
      "  \"source_language\": \"Chinese\",\n",
      "  \"target_language\": \"Vietnamese\",\n",
      "  \"metadata\": {\n",
      "    \"game_name\": \"LC_GS1_JX1M_VI\"\n",
      "  },\n",
      "  \"glossary\": [\n",
      "    {\n",
      "      \"source\": \"Â≥®ÁúâÂ•≥\",\n",
      "      \"target\": \"Nga Mi N·ªØ\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"10Èò∂\",\n",
      "      \"target\": \"B·∫≠c 10\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Â≥®Áúâ\",\n",
      "      \"target\": \"Nga Mi\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"ÈûãÂ≠ê\",\n",
      "      \"target\": \"Gi√†y\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"ÊôÆÈÄö\",\n",
      "      \"target\": \"Th∆∞·ªùng\"\n",
      "    }\n",
      "  ],\n",
      "  \"examples\": [\n",
      "    {\n",
      "      \"source\": \"10Èò∂ÂêÉÈ∏°ÈûãÂ≠ê\",\n",
      "      \"target\": \"Gi√†y T·ª≠ Chi·∫øn B·∫≠c 10\"\n",
      "    }\n",
      "  ],\n",
      "  \"text_to_translate\": \"[ÊôÆÈÄö]10Èò∂:Â≥®ÁúâÂ•≥ÈûãÂ≠ê\"\n",
      "}<|im_end|>\n",
      "<|im_start|>assistant\n",
      "{\n",
      "  \"translation\": \"[Th∆∞·ªùng]Gi√†y Nga Mi (N·ªØ) B·∫≠c 10\"\n",
      "}<|im_end|>\n",
      "\u001b[0m\u001b[92m<|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|><|vision_pad|>\u001b[0m\n",
      "\n",
      "More training debug examples written to .log/dataloader_examples.html\n"
     ]
    }
   ],
   "source": [
    "from HyperSloth.transformer_trainer_setup import setup_model_and_training\n",
    "from HyperSloth.hypersloth_config import *\n",
    "\n",
    "# Main configuration using Pydantic models\n",
    "hyper_config_model = HyperConfig(\n",
    "    grad_dir=\"/dev/shm/hypersloth\",\n",
    "    data=DataConfig(\n",
    "        dataset_name_or_path=\"data/sharegpt/train_233k.json\",\n",
    "        num_samples=1000,\n",
    "        group_by_length=True\n",
    "    ),\n",
    "    training=TrainingConfig(\n",
    "        gpus=range(2),  # Change this to the number of GPUs you have\n",
    "        loss_type=\"all\",  # all or response_only, the loss will only be calculated on the response part of the input\n",
    "    ),\n",
    "    fast_model_args=FastModelArgs(\n",
    "        model_name=\"unsloth/qwen2.5-0.5b-instruct\",\n",
    "        max_seq_length=16_000,\n",
    "    ),\n",
    "    lora_args=LoraArgs(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Training arguments using Pydantic model\n",
    "training_config_model = TrainingArgsConfig(\n",
    "    output_dir=\"saves/loras/250312/LC_EN_VI_TH_27B_233k/\",\n",
    "    per_device_train_batch_size=1,  #\n",
    "    gradient_accumulation_steps=16,  # More GA help to reduce total communication time\n",
    "    learning_rate=0.0002,\n",
    "    per_device_eval_batch_size=2,\n",
    "    eval_steps=100000,\n",
    "    logging_steps=1,\n",
    "    report_to=\"tensorboard\",\n",
    "    num_train_epochs=1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=5,\n",
    "    seed=42,\n",
    "    save_total_limit=2,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "\n",
    "output = setup_model_and_training(0, hyper_config_model, training_config_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89c8d55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.16: Fast Qwen2 patching. Transformers: 4.50.0.dev0.\n",
      "   \\\\   /|    NVIDIA L40S. Num GPUs = 8. Max memory: 44.403 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "output1 = setup_model_and_training(1, hyper_config_model, training_config_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe639b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273 274\n",
      "245 246\n",
      "127 128\n",
      "136 141\n",
      "404 408\n",
      "440 440\n",
      "259 259\n",
      "186 186\n",
      "143 144\n",
      "313 315\n",
      "183 184\n",
      "166 166\n",
      "516 516\n",
      "235 235\n",
      "93 93\n",
      "184 184\n",
      "265 266\n",
      "261 261\n",
      "374 375\n",
      "374 374\n",
      "310 310\n",
      "545 546\n",
      "434 436\n",
      "223 223\n",
      "323 324\n",
      "927 938\n",
      "356 356\n",
      "805 809\n",
      "263 264\n",
      "184 185\n",
      "218 218\n",
      "312 313\n",
      "249 249\n",
      "168 169\n",
      "607 607\n",
      "497 499\n",
      "402 402\n",
      "180 180\n",
      "665 666\n",
      "514 514\n",
      "244 245\n",
      "508 508\n",
      "278 279\n",
      "288 288\n",
      "489 490\n",
      "567 569\n",
      "190 190\n",
      "121 123\n",
      "686 688\n",
      "87 88\n",
      "413 415\n",
      "428 429\n",
      "193 194\n",
      "88 88\n",
      "89 89\n",
      "333 333\n",
      "153 153\n",
      "292 293\n",
      "132 133\n",
      "1273 1283\n",
      "481 482\n",
      "225 225\n",
      "240 240\n",
      "243 243\n",
      "395 395\n",
      "675 676\n",
      "286 286\n",
      "401 402\n",
      "409 410\n",
      "256 256\n",
      "89 90\n",
      "88 89\n",
      "1183 1214\n",
      "297 297\n",
      "195 196\n",
      "343 343\n",
      "208 208\n",
      "353 354\n",
      "97 97\n",
      "169 170\n",
      "677 679\n",
      "120 120\n",
      "342 343\n",
      "381 381\n",
      "179 180\n",
      "343 344\n",
      "285 285\n",
      "226 227\n",
      "287 287\n",
      "315 316\n",
      "335 336\n",
      "1035 1040\n",
      "1598 1611\n",
      "522 526\n",
      "216 216\n",
      "505 506\n",
      "372 373\n",
      "284 285\n",
      "289 292\n",
      "162 162\n",
      "331 331\n",
      "276 276\n",
      "521 521\n",
      "227 228\n",
      "101 117\n",
      "317 318\n",
      "1437 1453\n",
      "526 527\n",
      "215 216\n",
      "1691 1732\n",
      "280 281\n",
      "161 161\n",
      "318 319\n",
      "237 237\n",
      "163 165\n",
      "761 766\n",
      "256 257\n",
      "420 420\n",
      "361 361\n",
      "982 994\n",
      "187 187\n",
      "652 652\n",
      "235 236\n",
      "321 321\n",
      "123 123\n",
      "367 369\n",
      "176 176\n",
      "304 304\n",
      "608 610\n",
      "251 251\n",
      "340 341\n",
      "1423 1435\n",
      "725 736\n",
      "160 160\n",
      "239 239\n",
      "218 219\n",
      "541 541\n",
      "300 301\n",
      "894 906\n",
      "266 266\n",
      "100 100\n",
      "342 342\n",
      "259 260\n",
      "227 227\n",
      "321 321\n",
      "712 717\n",
      "92 92\n",
      "216 217\n",
      "238 238\n",
      "1001 1005\n",
      "623 623\n",
      "402 402\n",
      "639 641\n",
      "232 233\n",
      "263 263\n",
      "241 242\n",
      "404 404\n",
      "254 254\n",
      "389 390\n",
      "575 578\n",
      "90 90\n",
      "145 146\n",
      "166 167\n",
      "296 297\n",
      "124 125\n",
      "257 258\n",
      "559 559\n",
      "269 270\n",
      "446 446\n",
      "581 582\n",
      "160 161\n",
      "189 190\n",
      "365 366\n",
      "187 187\n",
      "441 441\n",
      "283 284\n",
      "344 345\n",
      "866 869\n",
      "355 356\n",
      "500 503\n",
      "295 295\n",
      "129 129\n",
      "403 404\n",
      "592 595\n",
      "853 861\n",
      "1612 1622\n",
      "482 483\n",
      "2404 2523\n",
      "284 284\n",
      "2256 2282\n",
      "198 198\n",
      "92 92\n",
      "455 456\n",
      "205 205\n",
      "310 310\n",
      "452 453\n",
      "345 346\n",
      "366 367\n",
      "1868 1877\n",
      "473 474\n",
      "390 390\n",
      "692 695\n",
      "844 845\n",
      "97 97\n",
      "176 176\n",
      "287 287\n",
      "191 192\n",
      "351 352\n",
      "96 96\n",
      "363 363\n",
      "247 248\n",
      "738 740\n",
      "1136 1138\n",
      "845 853\n",
      "570 570\n",
      "192 193\n",
      "345 345\n",
      "180 181\n",
      "756 760\n",
      "248 248\n",
      "293 294\n",
      "417 419\n",
      "134 135\n",
      "192 192\n",
      "283 283\n",
      "297 299\n",
      "277 278\n",
      "156 157\n",
      "208 209\n",
      "229 230\n",
      "228 229\n",
      "296 296\n",
      "359 359\n",
      "469 470\n",
      "494 495\n",
      "432 433\n",
      "211 212\n",
      "251 252\n",
      "533 539\n",
      "154 155\n",
      "217 217\n",
      "327 328\n",
      "465 465\n",
      "293 293\n",
      "89 89\n",
      "341 342\n",
      "91 91\n",
      "400 401\n",
      "1562 1573\n",
      "1052 1063\n",
      "544 545\n",
      "91 92\n",
      "385 387\n",
      "493 494\n",
      "391 392\n",
      "605 606\n",
      "89 89\n",
      "253 253\n",
      "518 519\n",
      "200 201\n",
      "441 441\n",
      "156 156\n",
      "302 303\n",
      "295 296\n",
      "369 370\n",
      "486 488\n",
      "241 241\n",
      "191 191\n",
      "463 463\n",
      "412 413\n",
      "444 445\n",
      "174 175\n",
      "316 316\n",
      "159 160\n",
      "158 159\n",
      "347 348\n",
      "595 599\n",
      "322 322\n",
      "131 131\n",
      "580 580\n",
      "211 211\n",
      "98 99\n",
      "210 210\n",
      "239 239\n",
      "430 430\n",
      "1143 1155\n",
      "89 89\n",
      "325 327\n",
      "246 246\n",
      "87 87\n",
      "347 347\n",
      "350 350\n",
      "173 173\n",
      "266 266\n",
      "416 416\n",
      "1064 1069\n",
      "586 592\n",
      "235 235\n",
      "145 145\n",
      "338 338\n",
      "182 183\n",
      "334 334\n",
      "440 440\n",
      "87 87\n",
      "750 750\n",
      "172 172\n",
      "295 295\n",
      "634 637\n",
      "2943 3017\n",
      "782 792\n",
      "527 528\n",
      "667 669\n",
      "150 152\n",
      "88 88\n",
      "358 358\n",
      "216 216\n",
      "347 347\n",
      "311 312\n",
      "411 412\n",
      "268 268\n",
      "271 272\n",
      "118 118\n",
      "390 390\n",
      "798 798\n",
      "812 816\n",
      "625 625\n",
      "170 170\n",
      "147 148\n",
      "95 96\n",
      "133 134\n",
      "329 330\n",
      "339 339\n",
      "162 163\n",
      "489 489\n",
      "616 620\n",
      "281 281\n",
      "378 379\n",
      "293 293\n",
      "380 381\n",
      "219 220\n",
      "349 350\n",
      "1090 1101\n",
      "214 215\n",
      "1805 1825\n",
      "673 673\n",
      "165 165\n",
      "305 306\n",
      "419 420\n",
      "279 280\n",
      "333 333\n",
      "430 432\n",
      "467 468\n",
      "476 477\n",
      "366 366\n",
      "195 195\n",
      "118 119\n",
      "221 221\n",
      "281 282\n",
      "242 243\n",
      "1306 1317\n",
      "204 204\n",
      "97 98\n",
      "246 247\n",
      "459 462\n",
      "951 980\n",
      "360 360\n",
      "464 465\n",
      "3150 3957\n",
      "221 223\n",
      "560 563\n",
      "309 309\n",
      "546 546\n",
      "2002 2212\n",
      "661 664\n",
      "348 348\n",
      "177 177\n",
      "550 554\n",
      "395 398\n",
      "121 121\n",
      "421 424\n",
      "459 459\n",
      "1022 1033\n",
      "220 221\n",
      "443 444\n",
      "194 195\n",
      "259 259\n",
      "265 265\n",
      "174 174\n",
      "262 262\n",
      "415 416\n",
      "303 304\n",
      "776 780\n",
      "703 704\n",
      "141 141\n",
      "1254 1258\n",
      "642 650\n",
      "231 232\n",
      "571 571\n",
      "202 203\n",
      "437 439\n",
      "95 95\n",
      "120 120\n",
      "479 481\n",
      "188 189\n",
      "373 373\n",
      "261 262\n",
      "403 403\n",
      "410 411\n",
      "234 234\n",
      "357 358\n",
      "126 127\n",
      "266 267\n",
      "157 158\n",
      "349 349\n",
      "306 308\n",
      "509 510\n",
      "93 94\n",
      "657 659\n",
      "220 220\n",
      "394 394\n",
      "254 255\n",
      "261 261\n",
      "136 136\n",
      "268 268\n",
      "1288 1298\n",
      "1663 1687\n",
      "252 252\n",
      "556 558\n",
      "196 196\n",
      "171 171\n",
      "880 893\n",
      "253 253\n",
      "1325 1414\n",
      "474 475\n",
      "172 172\n",
      "653 656\n",
      "547 548\n",
      "88 88\n",
      "660 661\n",
      "225 226\n",
      "381 382\n",
      "300 300\n",
      "197 197\n",
      "287 288\n",
      "1489 1528\n",
      "479 479\n",
      "427 427\n",
      "491 492\n",
      "190 191\n",
      "362 363\n",
      "90 90\n",
      "392 393\n",
      "336 338\n",
      "205 207\n",
      "196 197\n",
      "134 134\n",
      "90 91\n",
      "91 91\n",
      "289 289\n",
      "384 385\n",
      "131 132\n",
      "354 355\n",
      "1300 1300\n",
      "2311 2346\n",
      "448 449\n",
      "143 143\n",
      "520 520\n",
      "163 163\n",
      "199 200\n",
      "213 213\n",
      "223 223\n",
      "522 522\n",
      "90 90\n",
      "148 149\n",
      "416 417\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i1, i2 in zip(output.train_dataset, output1.train_dataset):\n",
    "    print(len(i1['input_ids']), len(i2['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a46c7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyexpat.errors import messages\n",
    "from typing import Any\n",
    "from speedy_utils.all import *\n",
    "from datasets import load_dataset\n",
    "from unsloth.chat_templates import standardize_data_formats\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def get_chat_dataset(\n",
    "    dataset_name: str, split: str = None, num_samples: int=None, tokenizer: Any = None\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Load and preprocess the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): The name of the dataset to load.\n",
    "        split (str): The dataset split to load.\n",
    "        num_samples (int): The number of samples to select from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        Any: The preprocessed dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(dataset_name):\n",
    "        dataset = Dataset.from_json(dataset_name)\n",
    "    else:\n",
    "        dataset = load_dataset(dataset_name, split=split)\n",
    "    dataset = standardize_data_formats(dataset)\n",
    "\n",
    "    def apply_chat_template(examples):\n",
    "        messages_key = \"messages\" if \"messages\" in examples else \"conversations\"\n",
    "        texts = tokenizer.apply_chat_template(examples[messages_key], tokenize=False)\n",
    "        return {\"text\": texts}\n",
    "    if num_samples:\n",
    "        num_samples = min(num_samples, len(dataset))\n",
    "        dataset = dataset.shuffle(seed=42)\n",
    "        dataset = dataset.select(range(num_samples))\n",
    "    if tokenizer:\n",
    "        dataset = dataset.map(apply_chat_template, batched=True)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Example usage\n",
    "dataset = get_chat_dataset(\"./data/cod_6k5.json\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e693a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    eval_dataset = None, # Can set up evaluation!\n",
    "    args = SFTConfig(\n",
    "        dataset_num_proc=4,\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 8, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        # max_steps = 1000,\n",
    "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cd842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(trainer.train_dataset[0]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a68de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|im_start|>user\",\n",
    "    response_part = \"<|im_start|>assistant\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc10dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cce39a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4090. Max memory = 23.643 GB.\n",
      "2.645 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec09490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2c5135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863d5ed395914df1b46c0ff07524f532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model.save_pretrained('./outputs/lora/Qwen2.5-1.5B-Instruct-LORA-MATH')\n",
    "# model.save_pretrained_merged('./outputs/lora/Qwen2.5-1.5B-Instruct-LORA-MATH-merged', tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d0973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llm_utils import get_conversation_one_turn\n",
    "\n",
    "\n",
    "# item = dataset[0]\n",
    "# # messages = item[\"messages\"][:-1]\n",
    "# messages = get_conversation_one_turn(None, 'ho√†ng sa c·ªßa n∆∞·ªõc n√†o, h√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng vi·ªát')\n",
    "\n",
    "# text = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     add_generation_prompt = True, # Must add for generation\n",
    "#     tokenize=False,nbs/test_unsloth.ipynbnbs/test_unsloth.ipynbÀõÀõ\n",
    "# )\n",
    "# text += '<think>\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473e2cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd9e5d23",
   "metadata": {},
   "source": [
    "### VLLM INFerence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0020b46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
