{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd /home/ubuntu/projects/hyper-sloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.all import *\n",
    "from speedy_utils.all import *\n",
    "from llm_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-16 17:00:46 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 03-16 17:00:46 config.py:685] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 03-16 17:00:46 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 03-16 17:00:47 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...\n",
      "INFO 03-16 17:00:47 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "INFO 03-16 17:00:48 weight_utils.py:304] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29bf1a90060948f09c3b8b10263cf38c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-16 17:00:49 model_runner.py:1115] Loading model weights took 2.8787 GB\n",
      "INFO 03-16 17:00:49 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 03-16 17:00:51 worker.py:267] Memory profiling takes 1.59 seconds\n",
      "INFO 03-16 17:00:51 worker.py:267] the current vLLM instance can use total_gpu_memory (23.64GiB) x gpu_memory_utilization (0.90) = 21.28GiB\n",
      "INFO 03-16 17:00:51 worker.py:267] model weights take 2.88GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.42GiB; the rest of the memory reserved for KV Cache is 16.98GiB.\n",
      "INFO 03-16 17:00:51 executor_base.py:111] # cuda blocks: 39731, # CPU blocks: 9362\n",
      "INFO 03-16 17:00:51 executor_base.py:116] Maximum concurrency for 16384 tokens per request: 38.80x\n",
      "INFO 03-16 17:00:51 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 2.61 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"01\"  # Choose any GPU you want to use\n",
    "from vllm.lora.request import LoRARequest\n",
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "\n",
    "if \"llm\" in locals():\n",
    "    del llm  #\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "llm = LLM(\n",
    "    # model=\"outputs/lora/Qwen2.5-1.5B-Instruct-LORA-MATH-merge\",\n",
    "    model=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    tensor_parallel_size=1,\n",
    "    task=\"generate\",\n",
    "    enforce_eager=True,\n",
    "    dtype=torch.bfloat16,\n",
    "    max_model_len=16384,\n",
    "    enable_lora=True,\n",
    "    # quantization=\"bitsandbytes\", load_format=\"bitsandbytes\",gpu_memory_utilization=0.95\n",
    ")\n",
    "\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the GSM8K dataset\n",
    "gsm8k = load_dataset(\"gsm8k\", \"main\")\n",
    "test = gsm8k[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  60%|██████    | 60/100 [00:59<00:54,  1.35s/it, est. speed input: 97.95 toks/s, output: 831.39 toks/s] "
     ]
    }
   ],
   "source": [
    "# Prepare prompts for GSM8K evaluation\n",
    "all_questions = [item[\"question\"] for item in test][:100]\n",
    "standardized_prompts = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{question}\\nSolve step by step and put your final numerical answer inside \\\\boxed{{}}\",\n",
    "            }\n",
    "        ],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    for question in all_questions\n",
    "]\n",
    "\n",
    "# Set sampling parameters for deterministic generation\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=64,\n",
    "    max_tokens=10000,\n",
    ")\n",
    "# Generate responses for all questions\n",
    "outputs = llm.generate(\n",
    "    standardized_prompts,\n",
    "    sampling_params,\n",
    "    lora_request=LoRARequest(\n",
    "        \"math\", 1, \"./outputs/loras/qwen1.5-openr1/checkpoint-732/\"\n",
    "    ),\n",
    ")\n",
    "all_outputs = [output.outputs[0].text for output in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_output(response):\n",
    "    try:\n",
    "        return int(response.split(\"\\\\boxed{\")[1].split(\"}\")[0])\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_outputs = [get_final_output(response) for response in all_outputs]\n",
    "accs = []\n",
    "num_error = 0\n",
    "for i, gt in enumerate(test):\n",
    "    if i >= len(final_outputs):\n",
    "        break\n",
    "    pred = final_outputs[i]\n",
    "    try:\n",
    "        num = gt['answer'].split('####')[1]\n",
    "        num = int(num)\n",
    "        pred = int(pred)\n",
    "        accs.append(num == pred)\n",
    "    except:\n",
    "        num_error += 1\n",
    "        accs.append(0)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.65, 0.12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accs), num_error/len(final_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
