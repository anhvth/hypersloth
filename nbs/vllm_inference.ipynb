{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd /home/ubuntu/projects/hyper-sloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speedy_utils.all import *\n",
    "from llm_utils import *\n",
    "from fastcore.all import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-15 09:29:17 [__init__.py:256] Automatically detected platform cuda.\n",
      "INFO 03-15 09:29:27 [arg_utils.py:1770] LORA is experimental on VLLM_USE_V1=1. Falling back to V0 Engine.\n",
      "WARNING 03-15 09:29:27 [cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 03-15 09:29:27 [llm_engine.py:241] Initializing a V0 LLM engine (v0.7.4.dev473+g9ed6ee92) with config: model='unsloth/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=unsloth/Qwen2.5-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 03-15 09:29:30 [cuda.py:285] Using Flash Attention backend.\n",
      "INFO 03-15 09:29:30 [parallel_state.py:948] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 03-15 09:29:30 [model_runner.py:1110] Starting to load model unsloth/Qwen2.5-1.5B-Instruct...\n",
      "INFO 03-15 09:29:31 [weight_utils.py:257] Using model weights format ['*.safetensors']\n",
      "INFO 03-15 09:29:32 [weight_utils.py:307] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7490669d9224ffabe4efe07e7dd0c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-15 09:29:32 [loader.py:429] Loading weights took 0.63 seconds\n",
      "INFO 03-15 09:29:32 [punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 03-15 09:29:33 [model_runner.py:1146] Model loading took 2.9227 GB and 2.148398 seconds\n",
      "INFO 03-15 09:29:39 [worker.py:267] Memory profiling takes 6.17 seconds\n",
      "INFO 03-15 09:29:39 [worker.py:267] the current vLLM instance can use total_gpu_memory (23.64GiB) x gpu_memory_utilization (0.90) = 21.28GiB\n",
      "INFO 03-15 09:29:39 [worker.py:267] model weights take 2.92GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 1.43GiB; the rest of the memory reserved for KV Cache is 16.85GiB.\n",
      "INFO 03-15 09:29:39 [executor_base.py:111] # cuda blocks: 39431, # CPU blocks: 9362\n",
      "INFO 03-15 09:29:39 [executor_base.py:116] Maximum concurrency for 16384 tokens per request: 38.51x\n",
      "INFO 03-15 09:29:43 [llm_engine.py:447] init engine (profile, create kv cache, warmup model) took 10.90 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # Choose any GPU you want to use\n",
    "from vllm.lora.request import LoRARequest\n",
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "# setup logger\n",
    "import logging\n",
    "from os import pipe\n",
    "\n",
    "if \"llm\" in locals():\n",
    "    del llm  #\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "llm = LLM(\n",
    "    model=\"unsloth/Qwen2.5-1.5B-Instruct\",\n",
    "    tensor_parallel_size=1,\n",
    "    task=\"generate\",\n",
    "    enforce_eager=True,\n",
    "    dtype=torch.bfloat16,\n",
    "    max_model_len=16384,\n",
    "    enable_lora=True,\n",
    "    # quantization=\"bitsandbytes\", load_format=\"bitsandbytes\",gpu_memory_utilization=0.95\n",
    ")\n",
    "\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the GSM8K dataset\n",
    "gsm8k = load_dataset(\"gsm8k\", \"main\")\n",
    "test = gsm8k[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 1/1319 [00:05<1:56:43,  5.31s/it, est. speed input: 16.75 toks/s, output: 15.62 toks/s]"
     ]
    }
   ],
   "source": [
    "# Prepare prompts for GSM8K evaluation\n",
    "all_questions = [item[\"question\"] for item in test]\n",
    "standardized_prompts = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": f\"{question}\\nSolve step by step and put your final numerical answer inside \\\\boxed{{}}\"}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    for question in all_questions\n",
    "]\n",
    "\n",
    "# Set sampling parameters for deterministic generation\n",
    "sampling_params = SamplingParams(temperature=0.0, top_p=0.95, top_k=64, max_tokens=5000)\n",
    "\n",
    "# Generate responses for all questions\n",
    "outputs = llm.generate(standardized_prompts, sampling_params, lora_request=LoRARequest(\"think_math\", 1, 'outputs/lora/Qwen2.5-1.5B-Instruct-LORA-MATH/'))\n",
    "all_outputs = [output.outputs[0].text for output in outputs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Janet's ducks: 16 eggs/day. Breakfast: 3, muffins: 4, sells: 16-3-4=9 eggs/day.\n",
      "\n",
      "Eggs sold: 9. Price: $2/egg. Total: 9 * $2 = $18.\n",
      "\n",
      "**Final Answer**\n",
      "\\boxed{18}\n",
      "</think>\n",
      "\n",
      "\n",
      "Janet's ducks lay 16 eggs per day. She eats 3 eggs for breakfast and bakes muffins for her friends with 4 eggs. The remaining eggs are sold at the farmers' market.\n",
      "\n",
      "First, we calculate the number of eggs left after Janet eats and bakes:\n",
      "\\[\n",
      "16 - 3 - 4 = 9 \\text{ eggs}\n",
      "\\]\n",
      "\n",
      "Next, we determine the revenue from selling these 9 eggs at $2 each:\n",
      "\\[\n",
      "9 \\times 2 = 18 \\text{ dollars}\n",
      "\\]\n",
      "\n",
      "Thus, Janet makes \\(\\boxed{18}\\) dollars every day at the farmers' market.\n"
     ]
    }
   ],
   "source": [
    "print(all_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_output(response):\n",
    "    try:\n",
    "        return int(response.split(\"\\\\boxed{\")[1].split(\"}\")[0])\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_outputs = [get_final_output(response) for response in all_outputs]\n",
    "accs = []\n",
    "num_error = 0\n",
    "for i, gt in enumerate(test):\n",
    "    pred = final_outputs[i]\n",
    "    try:\n",
    "        num = gt['answer'].split('####')[1]\n",
    "        # num = all_answers.append(int(num))\n",
    "        # all_answers.append(num)\n",
    "        num = int(num)\n",
    "        pred = int(pred)\n",
    "        accs.append(num == pred)\n",
    "    except:\n",
    "        num_error += 1\n",
    "        accs.append(0)\n",
    "        pass\n",
    "        # all_answers.append(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.689158453373768, 0.11902956785443518)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accs), num_error/len(final_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
