{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd /home/ubuntu/projects/hyper-sloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speedy_utils.all import *\n",
    "from llm_utils import *\n",
    "from fastcore.all import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-15 08:13:26 [__init__.py:256] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # Choose any GPU you want to use\n",
    "from vllm import LLM, SamplingParams\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-15 08:13:29 [config.py:2579] Downcasting torch.float32 to torch.bfloat16.\n",
      "WARNING 03-15 08:13:37 [config.py:662] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "WARNING 03-15 08:13:37 [arg_utils.py:1759] --quantization bitsandbytes is not supported by the V1 Engine. Falling back to V0. \n",
      "WARNING 03-15 08:13:37 [cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 03-15 08:13:38 [llm_engine.py:241] Initializing a V0 LLM engine (v0.7.4.dev473+g9ed6ee92) with config: model='google/gemma-3-27b-it', speculative_config=None, tokenizer='google/gemma-3-27b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=google/gemma-3-27b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 03-15 08:13:40 [cuda.py:285] Using Flash Attention backend.\n",
      "INFO 03-15 08:13:41 [parallel_state.py:948] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 03-15 08:13:41 [model_runner.py:1110] Starting to load model google/gemma-3-27b-it...\n",
      "INFO 03-15 08:13:41 [cuda.py:269] Cannot use FlashAttention-2 backend for head size 72.\n",
      "INFO 03-15 08:13:41 [cuda.py:282] Using XFormers backend.\n",
      "INFO 03-15 08:13:42 [config.py:3206] cudagraph sizes specified by model runner [] is overridden by config []\n",
      "INFO 03-15 08:13:42 [loader.py:1118] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 03-15 08:13:43 [weight_utils.py:257] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09874a226b354b94b8e7f84a2e5df0f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/12 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-15 08:13:53 [model_runner.py:1146] Model loading took 15.5312 GB and 12.066275 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-15 08:14:08 [worker.py:267] Memory profiling takes 14.58 seconds\n",
      "INFO 03-15 08:14:08 [worker.py:267] the current vLLM instance can use total_gpu_memory (23.64GiB) x gpu_memory_utilization (0.90) = 21.28GiB\n",
      "INFO 03-15 08:14:08 [worker.py:267] model weights take 15.53GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 2.19GiB; the rest of the memory reserved for KV Cache is 3.48GiB.\n",
      "INFO 03-15 08:14:09 [executor_base.py:111] # cuda blocks: 459, # CPU blocks: 528\n",
      "INFO 03-15 08:14:09 [executor_base.py:116] Maximum concurrency for 1024 tokens per request: 7.17x\n",
      "INFO 03-15 08:14:12 [llm_engine.py:447] init engine (profile, create kv cache, warmup model) took 18.44 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# setup logger\n",
    "import logging\n",
    "\n",
    "if \"llm\" in locals():\n",
    "    del llm  #\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "llm = LLM(\n",
    "    model=\"google/gemma-3-27b-it\",\n",
    "    tensor_parallel_size=1,\n",
    "    task=\"generate\",\n",
    "    enforce_eager=True,\n",
    "    dtype=torch.bfloat16,\n",
    "    max_model_len=1024,\n",
    "    quantization=\"bitsandbytes\", load_format=\"bitsandbytes\"\n",
    ")\n",
    "\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.22s/it, est. speed input: 131.93 toks/s, output: 9.59 toks/s]\n"
     ]
    }
   ],
   "source": [
    "data = load_by_ext(\n",
    "    \"/home/ubuntu/projects/localization/data/sharegpt/LC_GS1_JX1M_VI_test.json\"\n",
    ")\n",
    "msgs = data[5][\"messages\"]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    msgs[:-1], tokenize=False, add_generation_prompt=True\n",
    ") + 'Translation: '\n",
    "sampling_params = SamplingParams(temperature=0.2, top_p=0.95, top_k=64, max_tokens=50)\n",
    "outputs = llm.generate([prompt], sampling_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GT\n",
    "`'%</color> tỉ lệ thi triển <color=#1560b5>『Lệ Hồn Đoạt Phách』</color>, thời gian chờ <color=#d15e00>'`\n",
    "##### 4B output\n",
    "`\"Có xác suất tự động kích hoạt <color=#1560b5>『Lì Hồn Dấu Phách』</color>, thời gian hồi chiên <color=#d15e00>\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "**%</color> xác suất kích hoạt tự động <color=#1560b5>『Lệ Hồn Đoạt Phách』</color>, thời gian hồi chiêu <color=#d15e00>**\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "    <head>\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    </head>\n",
       "    <body>\n",
       "        <div style=\"background-color: #AAFFAA; color: #000000; padding: 10px; margin-bottom: 10px;\"><strong>User:</strong><br><pre id=\"user-0\">{<br>&nbsp;&nbsp;\"source_language\": \"Chinese\",<br>&nbsp;&nbsp;\"target_language\": \"Vietnamese\",<br>&nbsp;&nbsp;\"metadata\": {<br>&nbsp;&nbsp;&nbsp;&nbsp;\"game_name\": \"LC_GS1_JX1M_VI\"<br>&nbsp;&nbsp;},<br>&nbsp;&nbsp;\"examples\": [<br>&nbsp;&nbsp;&nbsp;&nbsp;{<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"source\": \"施放&lt;color=#1560b5&gt;『魂影丛生』&lt;/color&gt;对目标敌人立即造成&lt;color=#d15e00&gt;\",<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"target\": \"Thi triển &lt;color=#1560b5&gt;『Hồn Ảnh Tùng Sinh』&lt;/color&gt; gây cho kẻ địch &lt;color=#d15e00&gt;\"<br>&nbsp;&nbsp;&nbsp;&nbsp;},<br>&nbsp;&nbsp;&nbsp;&nbsp;{<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"source\": \"&lt;color=#8AFF2F&gt;%s&lt;/color&gt;请求一张&lt;color=#ECF8A7&gt;[%s]&lt;/color&gt;&lt;color=#F5BE5E&gt;%s&lt;/color&gt;\",<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"target\": \"&lt;color=#8AFF2F&gt;%s&lt;/color&gt; xin 1 &lt;color=#ECF8A7&gt;[%s]&lt;/color&gt;&lt;color=#F5BE5E&gt;%s&lt;/color&gt;\"<br>&nbsp;&nbsp;&nbsp;&nbsp;},<br>&nbsp;&nbsp;&nbsp;&nbsp;{<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"source\": \"&lt;color=#EB561B&gt;%s&lt;/color&gt;终结了&lt;color=#EB561B&gt;%s&lt;/color&gt;的傲视群雄\",<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"target\": \"&lt;color=#EB561B&gt;%s&lt;/color&gt; đã kết thúc Ngạo Thị Quần Hùng của &lt;color=#EB561B&gt;%s&lt;/color&gt;\"<br>&nbsp;&nbsp;&nbsp;&nbsp;},<br>&nbsp;&nbsp;&nbsp;&nbsp;{<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"source\": \"&lt;color=#FDFF9B&gt;圆圆满满&lt;/color&gt;\",<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"target\": \"&lt;color=#FDFF9B&gt;Viên Mãn&lt;/color&gt;\"<br>&nbsp;&nbsp;&nbsp;&nbsp;},<br>&nbsp;&nbsp;&nbsp;&nbsp;{<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"source\": \"抵抗虚弱时间&lt;color=#28ff28&gt;%s&lt;/color&gt;\",<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"target\": \"Thời gian kháng Suy Yếu: &lt;color=#28ff28&gt;%s&lt;/color&gt;\"<br>&nbsp;&nbsp;&nbsp;&nbsp;},<br>&nbsp;&nbsp;&nbsp;&nbsp;{<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"source\": \"&lt;color=#FDFF9B&gt;全家欢乐&lt;/color&gt;\",<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"target\": \"&lt;color=#FDFF9B&gt;Cả Nhà Vui Vẻ&lt;/color&gt;\"<br>&nbsp;&nbsp;&nbsp;&nbsp;},<br>&nbsp;&nbsp;&nbsp;&nbsp;{<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"source\": \"&lt;color=#B60000&gt;强化失败&lt;/color&gt;\",<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"target\": \"&lt;color=#B60000&gt;Cường hóa thất bại&lt;/color&gt;\"<br>&nbsp;&nbsp;&nbsp;&nbsp;},<br>&nbsp;&nbsp;&nbsp;&nbsp;{<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"source\": \"上交随机属性包含&lt;color=#ffb431&gt;%s&lt;/color&gt;的&lt;color=#ffb431&gt;%s阶%s&lt;/color&gt;\",<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"target\": \"Nộp thuộc tính ngẫu nhiên bao gồm &lt;color=#ffb431&gt;%s&lt;/color&gt; &lt;color=#ffb431&gt;Bậc %s %s&lt;/color&gt;\"<br>&nbsp;&nbsp;&nbsp;&nbsp;}<br>&nbsp;&nbsp;],<br>&nbsp;&nbsp;\"text_to_translate\": \"%&lt;/color&gt;的概率自动触发&lt;color=#1560b5&gt;『厉魂夺魄』&lt;/color&gt;,冷却时间&lt;color=#d15e00&gt;\"<br>}</pre></div><div style=\"background-color: #AAAAFF; color: #000000; padding: 10px; margin-bottom: 10px;\"><strong>Assistant:</strong><br><pre id=\"assistant-1\">{<br>&nbsp;&nbsp;\"translation\": \"%&lt;/color&gt; tỉ lệ thi triển &lt;color=#1560b5&gt;『Lệ Hồn Đoạt Phách』&lt;/color&gt;, thời gian chờ &lt;color=#d15e00&gt;\"<br>}</pre></div>\n",
       "        <script>\n",
       "            function copyContent(elementId) {\n",
       "                var element = document.getElementById(elementId);\n",
       "                var text = element.innerText;\n",
       "                navigator.clipboard.writeText(text)\n",
       "                    .then(function() {\n",
       "                        alert(\"Content copied to clipboard!\");\n",
       "                    })\n",
       "                    .catch(function(error) {\n",
       "                        console.error(\"Error copying content: \", error);\n",
       "                    });\n",
       "            }\n",
       "        </script>\n",
       "    </body>\n",
       "    </html>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_chat_messages_as_html(msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
