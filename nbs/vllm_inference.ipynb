{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd /home/ubuntu/projects/hyper-sloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.all import *\n",
    "from speedy_utils.all import *\n",
    "from llm_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"01\"  # Choose any GPU you want to use\n",
    "from vllm.lora.request import LoRARequest\n",
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "\n",
    "if \"llm\" in locals():\n",
    "    del llm  #\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "llm = LLM(\n",
    "    model=\"outputs/lora/Qwen2.5-1.5B-Instruct-LORA-MATH-merge\",\n",
    "    tensor_parallel_size=4,\n",
    "    task=\"generate\",\n",
    "    enforce_eager=True,\n",
    "    dtype=torch.bfloat16,\n",
    "    max_model_len=16384,\n",
    "    enable_lora=False,\n",
    "    # quantization=\"bitsandbytes\", load_format=\"bitsandbytes\",gpu_memory_utilization=0.95\n",
    ")\n",
    "\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the GSM8K dataset\n",
    "gsm8k = load_dataset(\"gsm8k\", \"main\")\n",
    "test = gsm8k[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [01:41<00:00,  1.02s/it, est. speed input: 101.39 toks/s, output: 452.55 toks/s] \n"
     ]
    }
   ],
   "source": [
    "# Prepare prompts for GSM8K evaluation\n",
    "all_questions = [item[\"question\"] for item in test][:100]\n",
    "standardized_prompts = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": f\"{question}\\nSolve step by step and put your final numerical answer inside \\\\boxed{{}}\"}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    for question in all_questions\n",
    "]\n",
    "\n",
    "# Set sampling parameters for deterministic generation\n",
    "sampling_params = SamplingParams(temperature=0.7, top_p=0.95, top_k=64, max_tokens=10000)\n",
    "\n",
    "# Generate responses for all questions\n",
    "outputs = llm.generate(standardized_prompts, sampling_params)\n",
    "all_outputs = [output.outputs[0].text for output in outputs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_output(response):\n",
    "    try:\n",
    "        return int(response.split(\"\\\\boxed{\")[1].split(\"}\")[0])\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_outputs = [get_final_output(response) for response in all_outputs]\n",
    "accs = []\n",
    "num_error = 0\n",
    "for i, gt in enumerate(test):\n",
    "    if i >= len(final_outputs):\n",
    "        break\n",
    "    pred = final_outputs[i]\n",
    "    try:\n",
    "        num = gt['answer'].split('####')[1]\n",
    "        num = int(num)\n",
    "        pred = int(pred)\n",
    "        accs.append(num == pred)\n",
    "    except:\n",
    "        num_error += 1\n",
    "        accs.append(0)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.59, 0.1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accs), num_error/len(final_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
