{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd /home/ubuntu/projects/hyper-sloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.all import *\n",
    "from speedy_utils.all import *\n",
    "from llm_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-15 10:50:40 [__init__.py:256] Automatically detected platform cuda.\n",
      "INFO 03-15 10:50:49 [config.py:1499] Defaulting to use mp for distributed inference\n",
      "INFO 03-15 10:50:49 [config.py:1677] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 03-15 10:50:49 [cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 03-15 10:50:51 [core.py:53] Initializing a V1 LLM engine (v0.7.4.dev473+g9ed6ee92) with config: model='outputs/lora/Qwen2.5-1.5B-Instruct-LORA-MATH-merge', speculative_config=None, tokenizer='outputs/lora/Qwen2.5-1.5B-Instruct-LORA-MATH-merge', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=outputs/lora/Qwen2.5-1.5B-Instruct-LORA-MATH-merge, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}\n",
      "WARNING 03-15 10:50:51 [multiproc_worker_utils.py:310] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 03-15 10:50:51 [custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 03-15 10:50:51 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 10485760, 10, 'psm_c62ed3f9'), local_subscribe_addr='ipc:///tmp/75eaeb87-ee86-4712-abf6-2c3bbfbb35d5', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 03-15 10:50:52 [utils.py:2298] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f901b04fc20>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=30440)\u001b[0;0m INFO 03-15 10:50:52 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2776fd51'), local_subscribe_addr='ipc:///tmp/2cf430bc-b04b-49d7-8aca-fb635471edfa', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 03-15 10:50:53 [utils.py:2298] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f904e9c3350>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=30464)\u001b[0;0m INFO 03-15 10:50:53 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_044cdac3'), local_subscribe_addr='ipc:///tmp/dbb4e8f2-9762-48cf-a274-a94069d66cb3', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 03-15 10:50:53 [utils.py:2298] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f901399a750>\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=30485)\u001b[0;0m INFO 03-15 10:50:53 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1afce494'), local_subscribe_addr='ipc:///tmp/1a17f277-b6a8-412f-bae4-4a736817de5b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 03-15 10:50:54 [utils.py:2298] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f901399a690>\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=30508)\u001b[0;0m INFO 03-15 10:50:54 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_36e4b588'), local_subscribe_addr='ipc:///tmp/e22024c8-5745-4ac7-b443-a73e84079e1b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=30508)\u001b[0;0m INFO 03-15 10:50:55 [utils.py:941] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=30508)\u001b[0;0m INFO 03-15 10:50:55 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=30464)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=30440)\u001b[0;0m INFO 03-15 10:50:55 [utils.py:941] Found nccl from library libnccl.so.2\n",
      "INFO 03-15 10:50:55 [utils.py:941] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=30464)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=30440)\u001b[0;0m INFO 03-15 10:50:55 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 03-15 10:50:55 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=30485)\u001b[0;0m INFO 03-15 10:50:55 [utils.py:941] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=30485)\u001b[0;0m INFO 03-15 10:50:55 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=30508)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=30485)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=30464)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=30440)\u001b[0;0m WARNING 03-15 10:50:56 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 03-15 10:50:56 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 03-15 10:50:56 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 03-15 10:50:56 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=30440)\u001b[0;0m INFO 03-15 10:50:56 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_40b81994'), local_subscribe_addr='ipc:///tmp/b3482c6b-88b5-4ebe-bf7b-1213876355cc', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=30508)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=30485)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=30464)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=30440)\u001b[0;0m INFO 03-15 10:50:56 [parallel_state.py:948] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2\n",
      "INFO 03-15 10:50:56 [parallel_state.py:948] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3\n",
      "INFO 03-15 10:50:56 [parallel_state.py:948] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "INFO 03-15 10:50:56 [parallel_state.py:948] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=30485)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=30440)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=30508)\u001b[0;0m INFO 03-15 10:50:56 [cuda.py:215] Using Flash Attention backend on V1 engine.\n",
      "INFO 03-15 10:50:56 [cuda.py:215] Using Flash Attention backend on V1 engine.\n",
      "INFO 03-15 10:50:56 [cuda.py:215] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=30464)\u001b[0;0m INFO 03-15 10:50:56 [cuda.py:215] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=30440)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=30485)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=30508)\u001b[0;0m INFO 03-15 10:50:56 [gpu_model_runner.py:1112] Starting to load model outputs/lora/Qwen2.5-1.5B-Instruct-LORA-MATH-merge...\n",
      "INFO 03-15 10:50:56 [gpu_model_runner.py:1112] Starting to load model outputs/lora/Qwen2.5-1.5B-Instruct-LORA-MATH-merge...\n",
      "INFO 03-15 10:50:56 [gpu_model_runner.py:1112] Starting to load model outputs/lora/Qwen2.5-1.5B-Instruct-LORA-MATH-merge...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=30464)\u001b[0;0m INFO 03-15 10:50:56 [gpu_model_runner.py:1112] Starting to load model outputs/lora/Qwen2.5-1.5B-Instruct-LORA-MATH-merge...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=30485)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=30508)\u001b[0;0m WARNING 03-15 10:50:57 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=30464)\u001b[0;0m WARNING 03-15 10:50:57 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 03-15 10:50:57 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=30440)\u001b[0;0m WARNING 03-15 10:50:57 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b833568788a40aaaaf1078e78a34035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=30485)\u001b[0;0m INFO 03-15 10:50:57 [loader.py:429] Loading weights took 0.25 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=30464)\u001b[0;0m INFO 03-15 10:50:57 [loader.py:429] Loading weights took 0.27 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=30440)\u001b[0;0m INFO 03-15 10:50:57 [loader.py:429] Loading weights took 0.29 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=30508)\u001b[0;0m INFO 03-15 10:50:57 [loader.py:429] Loading weights took 0.34 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=30440)\u001b[0;0m INFO 03-15 10:50:57 [gpu_model_runner.py:1124] Model loading took 0.7719 GB and 0.736790 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=30464)\u001b[0;0m INFO 03-15 10:50:57 [gpu_model_runner.py:1124] Model loading took 0.7719 GB and 0.718700 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=30508)\u001b[0;0m INFO 03-15 10:50:57 [gpu_model_runner.py:1124] Model loading took 0.7719 GB and 0.781302 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=30485)\u001b[0;0m INFO 03-15 10:50:57 [gpu_model_runner.py:1124] Model loading took 0.7719 GB and 0.692692 seconds\n",
      "INFO 03-15 10:51:01 [kv_cache_utils.py:537] GPU KV cache size: 1,056,672 tokens\n",
      "INFO 03-15 10:51:01 [kv_cache_utils.py:540] Maximum concurrency for 16,384 tokens per request: 64.49x\n",
      "INFO 03-15 10:51:01 [kv_cache_utils.py:537] GPU KV cache size: 1,056,672 tokens\n",
      "INFO 03-15 10:51:01 [kv_cache_utils.py:540] Maximum concurrency for 16,384 tokens per request: 64.49x\n",
      "INFO 03-15 10:51:01 [kv_cache_utils.py:537] GPU KV cache size: 1,056,672 tokens\n",
      "INFO 03-15 10:51:01 [kv_cache_utils.py:540] Maximum concurrency for 16,384 tokens per request: 64.49x\n",
      "INFO 03-15 10:51:01 [kv_cache_utils.py:537] GPU KV cache size: 1,056,672 tokens\n",
      "INFO 03-15 10:51:01 [kv_cache_utils.py:540] Maximum concurrency for 16,384 tokens per request: 64.49x\n",
      "INFO 03-15 10:51:01 [core.py:138] init engine (profile, create kv cache, warmup model) took 4.07 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"01\"  # Choose any GPU you want to use\n",
    "from vllm.lora.request import LoRARequest\n",
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "\n",
    "if \"llm\" in locals():\n",
    "    del llm  #\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "llm = LLM(\n",
    "    model=\"outputs/lora/Qwen2.5-1.5B-Instruct-LORA-MATH-merge\",\n",
    "    tensor_parallel_size=4,\n",
    "    task=\"generate\",\n",
    "    enforce_eager=True,\n",
    "    dtype=torch.bfloat16,\n",
    "    max_model_len=16384,\n",
    "    enable_lora=False,\n",
    "    # quantization=\"bitsandbytes\", load_format=\"bitsandbytes\",gpu_memory_utilization=0.95\n",
    ")\n",
    "\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the GSM8K dataset\n",
    "gsm8k = load_dataset(\"gsm8k\", \"main\")\n",
    "test = gsm8k[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [01:41<00:00,  1.02s/it, est. speed input: 101.39 toks/s, output: 452.55 toks/s] \n"
     ]
    }
   ],
   "source": [
    "# Prepare prompts for GSM8K evaluation\n",
    "all_questions = [item[\"question\"] for item in test][:100]\n",
    "standardized_prompts = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": f\"{question}\\nSolve step by step and put your final numerical answer inside \\\\boxed{{}}\"}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    for question in all_questions\n",
    "]\n",
    "\n",
    "# Set sampling parameters for deterministic generation\n",
    "sampling_params = SamplingParams(temperature=0.7, top_p=0.95, top_k=64, max_tokens=10000)\n",
    "\n",
    "# Generate responses for all questions\n",
    "outputs = llm.generate(standardized_prompts, sampling_params)\n",
    "all_outputs = [output.outputs[0].text for output in outputs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_output(response):\n",
    "    try:\n",
    "        return int(response.split(\"\\\\boxed{\")[1].split(\"}\")[0])\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_outputs = [get_final_output(response) for response in all_outputs]\n",
    "accs = []\n",
    "num_error = 0\n",
    "for i, gt in enumerate(test):\n",
    "    if i >= len(final_outputs):\n",
    "        break\n",
    "    pred = final_outputs[i]\n",
    "    try:\n",
    "        num = gt['answer'].split('####')[1]\n",
    "        num = int(num)\n",
    "        pred = int(pred)\n",
    "        accs.append(num == pred)\n",
    "    except:\n",
    "        num_error += 1\n",
    "        accs.append(0)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.59, 0.1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accs), num_error/len(final_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
