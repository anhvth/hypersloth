{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34618/409776915.py:3: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da6b3121d444d3d90e53dc648e7dfa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chat template\n",
      "==((====))==  Unsloth 2025.3.8: Fast Gemma2 patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 8. Max memory: 23.643 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb85c940ebc945cda8454ced6fa37ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from speedy_utils.all import *\n",
    "\n",
    "def prepare_model(\n",
    "    model_name, output_dir, tokenizer_name=None, chat_template=None, save_in_4bit=True\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    if tokenizer_name is None:\n",
    "        tokenizer_name = model_name\n",
    "    model = AutoModel.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    if chat_template is not None:\n",
    "        print(\"Loading chat template\")\n",
    "        tokenizer.chat_template = AutoTokenizer.from_pretrained(\n",
    "            chat_template, torch_dtype=torch.bfloat16\n",
    "        ).chat_template\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    if save_in_4bit:\n",
    "        print(\"Saving model in 4bit\")\n",
    "        if output_dir.endswith(\"/\"):\n",
    "            output_dir = output_dir[:-1]\n",
    "        output_dir_4bit = output_dir + \"-bnb-4bit\"\n",
    "        os.makedirs(output_dir_4bit, exist_ok=True)\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=output_dir,\n",
    "        )\n",
    "        model.save_pretrained(output_dir_4bit)\n",
    "        tokenizer.save_pretrained(output_dir_4bit)\n",
    "\n",
    "\n",
    "model = \"ModelSpace/GemmaX2-28-9B-v0.1\"\n",
    "tokenizer = None\n",
    "chat_template = \"google/gemma-2-9b-it\"\n",
    "output_dir = \"/mnt/data/huggingface-models/ModelSpace/GemmaX2-28-9B-v0.1\"\n",
    "prepare_model(model, output_dir, tokenizer, chat_template, save_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
