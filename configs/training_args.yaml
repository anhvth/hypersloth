# Training arguments configuration for HyperSloth

per_device_train_batch_size: 1
per_device_eval_batch_size: 2
gradient_accumulation_steps: 16
logging_steps: 1
eval_steps: 10000
warmup_steps: 5
num_train_epochs: 1
learning_rate: 0.0002  # 2e-4
bf16: true
optim: adamw_8bit
weight_decay: 0.01
lr_scheduler_type: linear
seed: 3407
output_dir: model_training_outputs/debug
save_total_limit: 2
report_to: tensorboard
