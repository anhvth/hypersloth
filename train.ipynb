{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "658abd4f-17e4-4fd3-8770-cfe9d60f34a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[36mhp_trainer.py:144 (train)\u001b[0m - \u001b[1m\n",
      "Key                            Value\n",
      "-----------------------------  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "grad_dir                       /dev/shm/hypersloth\n",
      "data                           {'dataset_name_or_path': 'data/OpenO1-SFT', 'test_ratio': 0.0, 'dataset_num_proc': 16, 'instruction_part': '<start_of_turn>user\\n', 'response_part': '<start_of_turn>model\\n', 'num_samples': 1000, 'group_by_length': True}\n",
      "training                       {'gpus': [0, 1, 2, 3, 4, 5, 6, 7], 'loss_type': 'response_only', 'packing': False}\n",
      "fast_model_args                {'model_name': 'unsloth/gemma-3-1b-it-bnb-4bit', 'max_seq_length': 16000, 'load_in_4bit': True, 'load_in_8bit': False, 'full_finetuning': False, 'token': None}\n",
      "lora_args                      {'finetune_vision_layers': False, 'finetune_language_layers': True, 'finetune_attention_modules': True, 'finetune_mlp_modules': True, 'r': 16, 'lora_alpha': 16, 'lora_dropout': 0.0, 'bias': 'none', 'random_state': 3407}\n",
      "hps_version                    2\n",
      "output_dir                     /data-4090/anhvth5/hypersloth_output/loras/gemma-3-1b-it/openo1\n",
      "per_device_train_batch_size    1\n",
      "learning_rate                  0.0001\n",
      "gradient_accumulation_steps    1\n",
      "per_device_eval_batch_size     4\n",
      "eval_steps                     100000\n",
      "logging_steps                  1\n",
      "report_to                      tensorboard\n",
      "num_train_epochs               1\n",
      "lr_scheduler_type              linear\n",
      "warmup_steps                   100\n",
      "seed                           42\n",
      "save_total_limit               2\n",
      "bf16                           True\n",
      "fp16                           False\n",
      "optim                          adamw_8bit\n",
      "weight_decay                   0.01\n",
      "packing                        False\n",
      "save_only_model                True\n",
      "save_steps                     200\n",
      "include_num_input_tokens_seen  True\u001b[0m\n",
      "Logging to .log/process_0.log\n",
      "Logging to .log/process_1.log\n",
      "Logging to .log/process_2.log\n",
      "Logging to .log/process_3.log\n",
      "Logging to .log/process_4.log\n",
      "Logging to .log/process_5.log\n",
      "Logging to .log/process_6.log\n",
      "Logging to .log/process_7.log\n",
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtransformer_trainer_setup.py:81 (_initialize_model_and_tokenizer)\u001b[0m - \u001b[1mUsing compiler location: .cache/unsloth_compiled_cache_0\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtransformer_trainer_setup.py:81 (_initialize_model_and_tokenizer)\u001b[0m - \u001b[1mUsing compiler location: .cache/unsloth_compiled_cache_7\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtransformer_trainer_setup.py:81 (_initialize_model_and_tokenizer)\u001b[0m - \u001b[1mUsing compiler location: .cache/unsloth_compiled_cache_4\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtransformer_trainer_setup.py:81 (_initialize_model_and_tokenizer)\u001b[0m - \u001b[1mUsing compiler location: .cache/unsloth_compiled_cache_5\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtransformer_trainer_setup.py:81 (_initialize_model_and_tokenizer)\u001b[0m - \u001b[1mUsing compiler location: .cache/unsloth_compiled_cache_1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtransformer_trainer_setup.py:81 (_initialize_model_and_tokenizer)\u001b[0m - \u001b[1mUsing compiler location: .cache/unsloth_compiled_cache_6\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtransformer_trainer_setup.py:81 (_initialize_model_and_tokenizer)\u001b[0m - \u001b[1mUsing compiler location: .cache/unsloth_compiled_cache_3\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtransformer_trainer_setup.py:81 (_initialize_model_and_tokenizer)\u001b[0m - \u001b[1mUsing compiler location: .cache/unsloth_compiled_cache_2\u001b[0m\n",
      "==((====))==  Unsloth 2025.3.18: Fast Gemma3 patching. Transformers: 4.50.0.dev0.\n",
      "   \\\\   /|    NVIDIA L40S. Num GPUs = 1. Max memory: 44.403 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "==((====))==  Unsloth 2025.3.18: Fast Gemma3 patching. Transformers: 4.50.0.dev0.\n",
      "   \\\\   /|    NVIDIA L40S. Num GPUs = 1. Max memory: 44.403 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "==((====))==  Unsloth 2025.3.18: Fast Gemma3 patching. Transformers: 4.50.0.dev0.\n",
      "   \\\\   /|    NVIDIA L40S. Num GPUs = 1. Max memory: 44.403 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "==((====))==  Unsloth 2025.3.18: Fast Gemma3 patching. Transformers: 4.50.0.dev0.\n",
      "   \\\\   /|    NVIDIA L40S. Num GPUs = 1. Max memory: 44.403 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "==((====))==  Unsloth 2025.3.18: Fast Gemma3 patching. Transformers: 4.50.0.dev0.\n",
      "   \\\\   /|    NVIDIA L40S. Num GPUs = 1. Max memory: 44.403 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "==((====))==  Unsloth 2025.3.18: Fast Gemma3 patching. Transformers: 4.50.0.dev0.\n",
      "   \\\\   /|    NVIDIA L40S. Num GPUs = 1. Max memory: 44.403 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "==((====))==  Unsloth 2025.3.18: Fast Gemma3 patching. Transformers: 4.50.0.dev0.\n",
      "   \\\\   /|    NVIDIA L40S. Num GPUs = 1. Max memory: 44.403 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "==((====))==  Unsloth 2025.3.18: Fast Gemma3 patching. Transformers: 4.50.0.dev0.\n",
      "   \\\\   /|    NVIDIA L40S. Num GPUs = 1. Max memory: 44.403 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtransformer_trainer_setup.py:113 (_create_trainer)\u001b[0m - \u001b[1mGPU 6: Loading dataset from .cache/dataset_42e24240d589ee13_08be46f0bfd7e179.cache\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtransformer_trainer_setup.py:113 (_create_trainer)\u001b[0m - \u001b[1mGPU 2: Loading dataset from .cache/dataset_42e24240d589ee13_08be46f0bfd7e179.cache\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtransformer_trainer_setup.py:113 (_create_trainer)\u001b[0m - \u001b[1mGPU 4: Loading dataset from .cache/dataset_42e24240d589ee13_08be46f0bfd7e179.cache\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:159 (select_dataset_by_length)\u001b[0m - \u001b[1mGPU 6: Selected 125 samples for training\u001b[0m\n",
      "num_proc must be <= 125. Reducing num_proc to 125 for dataset of size 125.\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:159 (select_dataset_by_length)\u001b[0m - \u001b[1mGPU 4: Selected 125 samples for training\u001b[0m\n",
      "num_proc must be <= 125. Reducing num_proc to 125 for dataset of size 125.\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:159 (select_dataset_by_length)\u001b[0m - \u001b[1mGPU 2: Selected 125 samples for training\u001b[0m\n",
      "num_proc must be <= 125. Reducing num_proc to 125 for dataset of size 125.\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtransformer_trainer_setup.py:113 (_create_trainer)\u001b[0m - \u001b[1mGPU 0: Loading dataset from .cache/dataset_42e24240d589ee13_08be46f0bfd7e179.cache\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtransformer_trainer_setup.py:113 (_create_trainer)\u001b[0m - \u001b[1mGPU 3: Loading dataset from .cache/dataset_42e24240d589ee13_08be46f0bfd7e179.cache\u001b[0m\n",
      "Map (num_proc=125): 100%|█████████████| 125/125 [00:00<00:00, 226.99 examples/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mhp_trainer.py:70 (_train)\u001b[0m - \u001b[1mUsing gradient sync callback v2\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mmmap_gradient_sync_v2.py:88 (__init__)\u001b[0m - \u001b[1m[Init GPU=6] Memmap opened: total_size=104366105\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mmmap_gradient_sync_v2.py:97 (__init__)\u001b[0m - \u001b[1m[Init GPU=6] local_rank=6, gpus=[0, 1, 2, 3, 4, 5, 6, 7], is_main=False, starting global_step=0\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mhp_trainer.py:80 (_train)\u001b[0m - \u001b[1mUsing gradient sync callback for GPU 6\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtransformer_trainer_setup.py:113 (_create_trainer)\u001b[0m - \u001b[1mGPU 7: Loading dataset from .cache/dataset_42e24240d589ee13_08be46f0bfd7e179.cache\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtransformer_trainer_setup.py:113 (_create_trainer)\u001b[0m - \u001b[1mGPU 1: Loading dataset from .cache/dataset_42e24240d589ee13_08be46f0bfd7e179.cache\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:364 (_inner_training_loop)\u001b[0m - \u001b[1m***** Running training *****\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:365 (_inner_training_loop)\u001b[0m - \u001b[1m  Num examples = 125\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:366 (_inner_training_loop)\u001b[0m - \u001b[1m  Num Epochs = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:367 (_inner_training_loop)\u001b[0m - \u001b[1m  Instantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:374 (_inner_training_loop)\u001b[0m - \u001b[1m  Total train batch size (w. parallel, distributed & accumulation) = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:377 (_inner_training_loop)\u001b[0m - \u001b[1m  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:380 (_inner_training_loop)\u001b[0m - \u001b[1m  Total optimization steps = 125\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:381 (_inner_training_loop)\u001b[0m - \u001b[1m  Number of trainable parameters = 13,045,760\u001b[0m\n",
      "  0%|                                                   | 0/125 [00:00<?, ?it/s]\u001b[1mINFO    \u001b[0m | \u001b[36mtransformer_trainer_setup.py:113 (_create_trainer)\u001b[0m - \u001b[1mGPU 5: Loading dataset from .cache/dataset_42e24240d589ee13_08be46f0bfd7e179.cache\u001b[0m\n",
      "Map (num_proc=125):   0%|                        | 0/125 [00:00<?, ? examples/s]\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:159 (select_dataset_by_length)\u001b[0m - \u001b[1mGPU 3: Selected 125 samples for training\u001b[0m\n",
      "num_proc must be <= 125. Reducing num_proc to 125 for dataset of size 125.\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:149 (select_dataset_by_length)\u001b[0m - \u001b[1mBelow is the table of the selected samples for each GPU\n",
      "  GPU  Lens      Total Len\n",
      "-----  ------  -----------\n",
      "    0  [2067]         2067\n",
      "    1  [1633]         1633\n",
      "    2  [1229]         1229\n",
      "    3  [1224]         1224\n",
      "    4  [1128]         1128\n",
      "    5  [1024]         1024\n",
      "    6  [793]           793\n",
      "    7  [729]           729\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:159 (select_dataset_by_length)\u001b[0m - \u001b[1mGPU 0: Selected 125 samples for training\u001b[0m\n",
      "Map (num_proc=125):   3%|▌               | 4/125 [00:00<00:03, 37.09 examples/s]num_proc must be <= 125. Reducing num_proc to 125 for dataset of size 125.\n",
      "Map (num_proc=125): 100%|█████████████| 125/125 [00:00<00:00, 231.54 examples/s]\n",
      "Map (num_proc=125):   2%|▍               | 3/125 [00:00<00:05, 24.38 examples/s]\u001b[1mINFO    \u001b[0m | \u001b[36mhp_trainer.py:70 (_train)\u001b[0m - \u001b[1mUsing gradient sync callback v2\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mmmap_gradient_sync_v2.py:88 (__init__)\u001b[0m - \u001b[1m[Init GPU=4] Memmap opened: total_size=104366105\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mmmap_gradient_sync_v2.py:97 (__init__)\u001b[0m - \u001b[1m[Init GPU=4] local_rank=4, gpus=[0, 1, 2, 3, 4, 5, 6, 7], is_main=False, starting global_step=0\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mhp_trainer.py:80 (_train)\u001b[0m - \u001b[1mUsing gradient sync callback for GPU 4\u001b[0m\n",
      "Map (num_proc=125): 100%|█████████████| 125/125 [00:00<00:00, 200.62 examples/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:364 (_inner_training_loop)\u001b[0m - \u001b[1m***** Running training *****\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:365 (_inner_training_loop)\u001b[0m - \u001b[1m  Num examples = 125\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:366 (_inner_training_loop)\u001b[0m - \u001b[1m  Num Epochs = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:367 (_inner_training_loop)\u001b[0m - \u001b[1m  Instantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:374 (_inner_training_loop)\u001b[0m - \u001b[1m  Total train batch size (w. parallel, distributed & accumulation) = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:377 (_inner_training_loop)\u001b[0m - \u001b[1m  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:380 (_inner_training_loop)\u001b[0m - \u001b[1m  Total optimization steps = 125\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:381 (_inner_training_loop)\u001b[0m - \u001b[1m  Number of trainable parameters = 13,045,760\u001b[0m\n",
      "  0%|                                                   | 0/125 [00:00<?, ?it/s]\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:159 (select_dataset_by_length)\u001b[0m - \u001b[1mGPU 5: Selected 125 samples for training\u001b[0m\n",
      "num_proc must be <= 125. Reducing num_proc to 125 for dataset of size 125.\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:159 (select_dataset_by_length)\u001b[0m - \u001b[1mGPU 7: Selected 125 samples for training\u001b[0m\n",
      "num_proc must be <= 125. Reducing num_proc to 125 for dataset of size 125.\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:159 (select_dataset_by_length)\u001b[0m - \u001b[1mGPU 1: Selected 125 samples for training\u001b[0m\n",
      "num_proc must be <= 125. Reducing num_proc to 125 for dataset of size 125.\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mhp_trainer.py:70 (_train)\u001b[0m - \u001b[1mUsing gradient sync callback v2\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mmmap_gradient_sync_v2.py:88 (__init__)\u001b[0m - \u001b[1m[Init GPU=2] Memmap opened: total_size=104366105\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mmmap_gradient_sync_v2.py:97 (__init__)\u001b[0m - \u001b[1m[Init GPU=2] local_rank=2, gpus=[0, 1, 2, 3, 4, 5, 6, 7], is_main=False, starting global_step=0\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mhp_trainer.py:80 (_train)\u001b[0m - \u001b[1mUsing gradient sync callback for GPU 2\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:364 (_inner_training_loop)\u001b[0m - \u001b[1m***** Running training *****\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:365 (_inner_training_loop)\u001b[0m - \u001b[1m  Num examples = 125\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:366 (_inner_training_loop)\u001b[0m - \u001b[1m  Num Epochs = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:367 (_inner_training_loop)\u001b[0m - \u001b[1m  Instantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:374 (_inner_training_loop)\u001b[0m - \u001b[1m  Total train batch size (w. parallel, distributed & accumulation) = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:377 (_inner_training_loop)\u001b[0m - \u001b[1m  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:380 (_inner_training_loop)\u001b[0m - \u001b[1m  Total optimization steps = 125\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:381 (_inner_training_loop)\u001b[0m - \u001b[1m  Number of trainable parameters = 13,045,760\u001b[0m\n",
      "Map (num_proc=125): 100%|█████████████| 125/125 [00:00<00:00, 226.75 examples/s]\n",
      "Map (num_proc=125): 100%|█████████████| 125/125 [00:00<00:00, 180.58 examples/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mhp_trainer.py:70 (_train)\u001b[0m - \u001b[1mUsing gradient sync callback v2\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mmmap_gradient_sync_v2.py:88 (__init__)\u001b[0m - \u001b[1m[Init GPU=3] Memmap opened: total_size=104366105\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mmmap_gradient_sync_v2.py:97 (__init__)\u001b[0m - \u001b[1m[Init GPU=3] local_rank=3, gpus=[0, 1, 2, 3, 4, 5, 6, 7], is_main=False, starting global_step=0\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mhp_trainer.py:80 (_train)\u001b[0m - \u001b[1mUsing gradient sync callback for GPU 3\u001b[0m\n",
      "Map (num_proc=125):  42%|█████▊        | 52/125 [00:00<00:00, 173.06 examples/s]\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:364 (_inner_training_loop)\u001b[0m - \u001b[1m***** Running training *****\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:365 (_inner_training_loop)\u001b[0m - \u001b[1m  Num examples = 125\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:366 (_inner_training_loop)\u001b[0m - \u001b[1m  Num Epochs = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:367 (_inner_training_loop)\u001b[0m - \u001b[1m  Instantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:374 (_inner_training_loop)\u001b[0m - \u001b[1m  Total train batch size (w. parallel, distributed & accumulation) = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:377 (_inner_training_loop)\u001b[0m - \u001b[1m  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:380 (_inner_training_loop)\u001b[0m - \u001b[1m  Total optimization steps = 125\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:381 (_inner_training_loop)\u001b[0m - \u001b[1m  Number of trainable parameters = 13,045,760\u001b[0m\n",
      "Map (num_proc=125): 100%|█████████████| 125/125 [00:00<00:00, 213.25 examples/s]\n",
      "Map (num_proc=125):  10%|█▍             | 12/125 [00:00<00:03, 33.18 examples/s]\u001b[1mINFO    \u001b[0m | \u001b[36mhp_trainer.py:70 (_train)\u001b[0m - \u001b[1mUsing gradient sync callback v2\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mmmap_gradient_sync_v2.py:88 (__init__)\u001b[0m - \u001b[1m[Init GPU=5] Memmap opened: total_size=104366105\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mmmap_gradient_sync_v2.py:97 (__init__)\u001b[0m - \u001b[1m[Init GPU=5] local_rank=5, gpus=[0, 1, 2, 3, 4, 5, 6, 7], is_main=False, starting global_step=0\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mhp_trainer.py:80 (_train)\u001b[0m - \u001b[1mUsing gradient sync callback for GPU 5\u001b[0m\n",
      "Map (num_proc=125): 100%|█████████████| 125/125 [00:00<00:00, 214.64 examples/s]\n",
      "Map (num_proc=125):  35%|████▉         | 44/125 [00:00<00:00, 107.42 examples/s]\u001b[1mINFO    \u001b[0m | \u001b[36mhp_trainer.py:70 (_train)\u001b[0m - \u001b[1mUsing gradient sync callback v2\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mmmap_gradient_sync_v2.py:88 (__init__)\u001b[0m - \u001b[1m[Init GPU=7] Memmap opened: total_size=104366105\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mmmap_gradient_sync_v2.py:97 (__init__)\u001b[0m - \u001b[1m[Init GPU=7] local_rank=7, gpus=[0, 1, 2, 3, 4, 5, 6, 7], is_main=False, starting global_step=0\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mhp_trainer.py:80 (_train)\u001b[0m - \u001b[1mUsing gradient sync callback for GPU 7\u001b[0m\n",
      "Map (num_proc=125): 100%|█████████████| 125/125 [00:00<00:00, 198.09 examples/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:364 (_inner_training_loop)\u001b[0m - \u001b[1m***** Running training *****\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:365 (_inner_training_loop)\u001b[0m - \u001b[1m  Num examples = 125\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:366 (_inner_training_loop)\u001b[0m - \u001b[1m  Num Epochs = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:367 (_inner_training_loop)\u001b[0m - \u001b[1m  Instantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:374 (_inner_training_loop)\u001b[0m - \u001b[1m  Total train batch size (w. parallel, distributed & accumulation) = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:377 (_inner_training_loop)\u001b[0m - \u001b[1m  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:380 (_inner_training_loop)\u001b[0m - \u001b[1m  Total optimization steps = 125\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:381 (_inner_training_loop)\u001b[0m - \u001b[1m  Number of trainable parameters = 13,045,760\u001b[0m\n",
      "  0%|                                                   | 0/125 [00:00<?, ?it/s]\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:364 (_inner_training_loop)\u001b[0m - \u001b[1m***** Running training *****\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:365 (_inner_training_loop)\u001b[0m - \u001b[1m  Num examples = 125\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:366 (_inner_training_loop)\u001b[0m - \u001b[1m  Num Epochs = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:367 (_inner_training_loop)\u001b[0m - \u001b[1m  Instantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:374 (_inner_training_loop)\u001b[0m - \u001b[1m  Total train batch size (w. parallel, distributed & accumulation) = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:377 (_inner_training_loop)\u001b[0m - \u001b[1m  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:380 (_inner_training_loop)\u001b[0m - \u001b[1m  Total optimization steps = 125\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:381 (_inner_training_loop)\u001b[0m - \u001b[1m  Number of trainable parameters = 13,045,760\u001b[0m\n",
      "  0%|                                                   | 0/125 [00:00<?, ?it/s]\u001b[1mINFO    \u001b[0m | \u001b[36mhp_trainer.py:70 (_train)\u001b[0m - \u001b[1mUsing gradient sync callback v2\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mmmap_gradient_sync_v2.py:88 (__init__)\u001b[0m - \u001b[1m[Init GPU=1] Memmap opened: total_size=104366105\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mmmap_gradient_sync_v2.py:97 (__init__)\u001b[0m - \u001b[1m[Init GPU=1] local_rank=1, gpus=[0, 1, 2, 3, 4, 5, 6, 7], is_main=False, starting global_step=0\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mhp_trainer.py:80 (_train)\u001b[0m - \u001b[1mUsing gradient sync callback for GPU 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:364 (_inner_training_loop)\u001b[0m - \u001b[1m***** Running training *****\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:365 (_inner_training_loop)\u001b[0m - \u001b[1m  Num examples = 125\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:366 (_inner_training_loop)\u001b[0m - \u001b[1m  Num Epochs = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:367 (_inner_training_loop)\u001b[0m - \u001b[1m  Instantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:374 (_inner_training_loop)\u001b[0m - \u001b[1m  Total train batch size (w. parallel, distributed & accumulation) = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:377 (_inner_training_loop)\u001b[0m - \u001b[1m  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:380 (_inner_training_loop)\u001b[0m - \u001b[1m  Total optimization steps = 125\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:381 (_inner_training_loop)\u001b[0m - \u001b[1m  Number of trainable parameters = 13,045,760\u001b[0m\n",
      "Map (num_proc=176): 100%|██████████| 1000/1000 [00:00<00:00, 1546.37 examples/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtransformer_trainer_setup.py:45 (setup_model_and_training)\u001b[0m - \u001b[1mModel setup complete for GPU 0\u001b[0m\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "=== EXAMPLE #1 ===\n",
      "\u001b[92m<bos><start_of_turn>user\n",
      "编写一个 C++ 程序，实现 Dijkstra 算法来找到图中两个节点之间的最短路径。要求能够处理有向图和无向图，并输出最短路径及其长度。图数据输入应该支持用户手动输入和自动读取图文件，图结构要带权重。<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\u001b[0m\u001b[93m<Thought>\n",
      "Alright, I need to write a C++ program that implements Dijkstra's algorithm to find the shortest path between two nodes in a graph. The program should handle both directed and undirected graphs, output the shortest path along with its length, and support graph data input either manually by the user or by reading from a file. Additionally, the graph structure must include weights on its edges.\n",
      "\n",
      "First, I'll start by considering how to represent the graph. Since the graph can be either directed or undirected, I need a flexible structure. An adjacency list is a good choice because it's efficient for sparse graphs and can easily represent both types of graphs. Each node will have a list of its adjacent nodes along with the corresponding edge weights.\n",
      "\n",
      "Next, I'll think about how to implement Dijkstra's algorithm. The algorithm requires maintaining a priority queue to select the next node with the smallest tentative distance. In C++, I can use a priority queue where each element is a pair containing the distance and the node identifier. However, the default priority queue in C++ is a max-heap, so I'll need to modify it to function as a min-heap by using a custom comparator.\n",
      "\n",
      "I also need to decide how to handle user input and file input. For manual input, I'll prompt the user to enter the number of nodes, the number of edges, and then enter each edge with its weight. For file input, I'll allow the user to specify a filename, and the program will read the graph data from that file. The file should be formatted in a way that the program can parse easily, such as having the number of nodes and edges on the first lines, followed by lines representing each edge with two node identifiers and a weight.\n",
      "\n",
      "After reading the graph data, the program should prompt the user to enter the start and end nodes for which they want to find the shortest path. I'll need to validate these inputs to ensure they refer to existing nodes in the graph.\n",
      "\n",
      "Implementing Dijkstra's algorithm involves initializing distances to all nodes as infinity except for the start node, which has a distance of zero. I'll use a vector to keep track of the distances and another vector to keep track of the previous nodes in the path. As the algorithm progresses, I'll update these vectors accordingly.\n",
      "\n",
      "Once the algorithm finishes, I'll reconstruct the shortest path by backtracking from the end node to the start node using the previous nodes vector. I'll also calculate the total length of the path by summing the weights of the edges along the path.\n",
      "\n",
      "For the output, the program should display both the sequence of nodes in the shortest path and the total length of that path. If there is no path between the two nodes, the program should inform the user accordingly.\n",
      "\n",
      "I also need to handle edge cases and potential errors, such as invalid user inputs, disconnected graphs, and negative edge weights (though Dijkstra's algorithm doesn't handle negative weights). To ensure robustness, I'll include checks and provide meaningful error messages to guide the user in correcting any issues.\n",
      "\n",
      "Finally, to make the program user-friendly, I'll structure the code with functions handling specific tasks like reading input, implementing the algorithm, and displaying the output. Adding comments throughout the code will help in understanding the flow and purpose of each part.\n",
      "\n",
      "Overall, the program will be modular, efficient, and user-friendly, effectively finding the shortest path between two nodes in both directed and undirected weighted graphs, with flexible input options and clear output.\n",
      "\n",
      "</Thought>\n",
      "<Output>\n",
      "```cpp\n",
      "#include <iostream>\n",
      "#include <vector>\n",
      "#include <queue>\n",
      "#include <fstream>\n",
      "#include <limits>\n",
      "#include <unordered_map>\n",
      "#include <algorithm>\n",
      "\n",
      "using namespace std;\n",
      "\n",
      "// 定义一个无限大的距离\n",
      "const int INF = numeric_limits<int>::max();\n",
      "\n",
      "// 使用邻接表表示图\n",
      "typedef pair<int, int> pii; // (邻接点, 权重)\n",
      "unordered_map<int, vector<pii>> graph;\n",
      "\n",
      "// 读取手动输入的图\n",
      "void readGraphManually(int numNodes, int numEdges, bool isDirected) {\n",
      "    cout << \"请输入每条边的起点、终点和权重（例如：1 2 10）:\\n\";\n",
      "    for(int i = 0; i < numEdges; ++i){\n",
      "        int u, v, w;\n",
      "        cin >> u >> v >> w;\n",
      "        graph[u].emplace_back(v, w);\n",
      "        if(!isDirected){\n",
      "            graph[v].emplace_back(u, w);\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "// 从文件读取图\n",
      "bool readGraphFromFile(const string &filename, bool isDirected) {\n",
      "    ifstream infile(filename);\n",
      "    if(!infile){\n",
      "        cout << \"无法打开文件: \" << filename << endl;\n",
      "        return false;\n",
      "    }\n",
      "    int numNodes, numEdges;\n",
      "    infile >> numNodes >> numEdges;\n",
      "    for(int i = 0; i < numEdges; ++i){\n",
      "        int u, v, w;\n",
      "        infile >> u >> v >> w;\n",
      "        graph[u].emplace_back(v, w);\n",
      "        if(!isDirected){\n",
      "            graph[v].emplace_back(u, w);\n",
      "        }\n",
      "    }\n",
      "    infile.close();\n",
      "    return true;\n",
      "}\n",
      "\n",
      "// Dijkstra算法\n",
      "pair<vector<int>, int> dijkstra(int start, int end) {\n",
      "    // 初始化\n",
      "    unordered_map<int, int> dist;\n",
      "    unordered_map<int, int> prev;\n",
      "    for(auto &[node, _] : graph){\n",
      "        dist[node] = INF;\n",
      "        prev[node] = -1;\n",
      "    }\n",
      "    dist[start] = 0;\n",
      "\n",
      "    // 优先队列（最小堆）\n",
      "    priority_queue<pii, vector<pii>, std::greater<pii>> pq;\n",
      "    pq.emplace(0, start);\n",
      "\n",
      "    while(!pq.empty()){\n",
      "        auto [currentDist, u] = pq.top();\n",
      "        pq.pop();\n",
      "\n",
      "        if(u == end){\n",
      "            break;\n",
      "        }\n",
      "\n",
      "        // 如果当前距离大于已知最短距离，跳过\n",
      "        if(currentDist > dist[u]){\n",
      "            continue;\n",
      "        }\n",
      "\n",
      "        for(auto &[v, w] : graph[u]){\n",
      "            if(dist[u] + w < dist[v]){\n",
      "                dist[v] = dist[u] + w;\n",
      "                prev[v] = u;\n",
      "                pq.emplace(dist[v], v);\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "\n",
      "    // 重构路径\n",
      "    vector<int> path;\n",
      "    if(dist[end] == INF){\n",
      "        return {path, -1}; // 无路径\n",
      "    }\n",
      "    for(int at = end; at != -1; at = prev[at]){\n",
      "        path.push_back(at);\n",
      "    }\n",
      "    reverse(path.begin(), path.end());\n",
      "\n",
      "    return {path, dist[end]};\n",
      "}\n",
      "\n",
      "int main(){\n",
      "    int choice;\n",
      "    cout << \"选择输入方式: 1. 手动输入 2. 从文件读取\\n\";\n",
      "    cin >> choice;\n",
      "\n",
      "    int numNodes, numEdges;\n",
      "    bool isDirected;\n",
      "    cout << \"请输入图的类型: 1. 有向图 2. 无向图\\n\";\n",
      "    int type;\n",
      "    cin >> type;\n",
      "    isDirected = (type == 1);\n",
      "\n",
      "    if(choice == 1){\n",
      "        cout << \"请输入节点数和边数:\\n\";\n",
      "        cin >> numNodes >> numEdges;\n",
      "        readGraphManually(numNodes, numEdges, isDirected);\n",
      "    }\n",
      "    else if(choice == 2){\n",
      "        string filename;\n",
      "        cout << \"请输入文件名:\\n\";\n",
      "        cin >> filename;\n",
      "        if(!readGraphFromFile(filename, isDirected)){\n",
      "            return 1;\n",
      "        }\n",
      "    }\n",
      "    else{\n",
      "        cout << \"无效的选择。\\n\";\n",
      "        return 1;\n",
      "    }\n",
      "\n",
      "    int start, end;\n",
      "    cout << \"请输入起点和终点:\\n\";\n",
      "    cin >> start >> end;\n",
      "\n",
      "    if(graph.find(start) == graph.end() || graph.find(end) == graph.end()){\n",
      "        cout << \"起点或终点不存在于图中。\\n\";\n",
      "        return 1;\n",
      "    }\n",
      "\n",
      "    pair<vector<int>, int> result = dijkstra(start, end);\n",
      "    if(result.second == -1){\n",
      "        cout << \"从 \" << start << \" 到 \" << end << \" 没有路径。\\n\";\n",
      "    }\n",
      "    else{\n",
      "        cout << \"最短路径长度: \" << result.second << \"\\n路径: \";\n",
      "        for(auto it = result.first.begin(); it != result.first.end(); ++it){\n",
      "            if(it != result.first.begin()) cout << \" -> \";\n",
      "            cout << *it;\n",
      "        }\n",
      "        cout << endl;\n",
      "    }\n",
      "\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "</Output><end_of_turn>\n",
      "\u001b[0m\n",
      "\n",
      "More training debug examples written to .log/dataloader_examples.html\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mhp_trainer.py:70 (_train)\u001b[0m - \u001b[1mUsing gradient sync callback v2\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mmmap_gradient_sync_v2.py:88 (__init__)\u001b[0m - \u001b[1m[Init GPU=0] Memmap opened: total_size=104366105\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mmmap_gradient_sync_v2.py:97 (__init__)\u001b[0m - \u001b[1m[Init GPU=0] local_rank=0, gpus=[0, 1, 2, 3, 4, 5, 6, 7], is_main=True, starting global_step=0\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mhp_trainer.py:80 (_train)\u001b[0m - \u001b[1mUsing gradient sync callback for GPU 0\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:364 (_inner_training_loop)\u001b[0m - \u001b[1m***** Running training *****\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:365 (_inner_training_loop)\u001b[0m - \u001b[1m  Num examples = 125\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:366 (_inner_training_loop)\u001b[0m - \u001b[1m  Num Epochs = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:367 (_inner_training_loop)\u001b[0m - \u001b[1m  Instantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:374 (_inner_training_loop)\u001b[0m - \u001b[1m  Total train batch size (w. parallel, distributed & accumulation) = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:377 (_inner_training_loop)\u001b[0m - \u001b[1m  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:380 (_inner_training_loop)\u001b[0m - \u001b[1m  Total optimization steps = 125\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:381 (_inner_training_loop)\u001b[0m - \u001b[1m  Number of trainable parameters = 13,045,760\u001b[0m\n",
      "{'loss': 1.4768, 'grad_norm': 1.3547143936157227, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.01, 'num_input_tokens_seen': 1224, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4522, 'grad_norm': 1.3547143936157227, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.01, 'num_input_tokens_seen': 1024, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2084, 'grad_norm': 1.3547143936157227, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.01, 'num_input_tokens_seen': 2067, 'trained_token_ratio': 1.0}\n",
      "  1%|▎                                          | 1/125 [00:16<34:03, 16.48s/it]\u001b[1mINFO    \u001b[0m | \u001b[36mtrainer.py:2250 (train)\u001b[0m - \u001b[1m\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| Task                 | File:Line       |   Depth | Time (s)   | Percentage (%)   |\n",
      "+======================+=================+=========+============+==================+\n",
      "| 1. get_batch_samples | patching.py:501 |       0 | 0.05 s     | \u001b[94m0.31 %\u001b[0m           |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 2. forward           | patching.py:597 |       0 | 15.16 s    | \u001b[91m91.87 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 3. sync gradients    | patching.py:601 |       0 | 0.10 s     | \u001b[94m0.59 %\u001b[0m           |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 4. optimizer step    | patching.py:666 |       0 | 1.19 s     | \u001b[94m7.23 %\u001b[0m           |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "\n",
      "Total time elapsed: 16.50 seconds.\u001b[0m\n",
      "Unsloth: Will smartly offload gradients to save VRAM!\n",
      "Unsloth: Will smartly offload gradients to save VRAM!\n",
      "{'loss': 1.4103, 'grad_norm': 1.3547143936157227, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.01, 'num_input_tokens_seen': 1229, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.7696, 'grad_norm': 1.3547143936157227, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.01, 'num_input_tokens_seen': 793, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.0608, 'grad_norm': 1.3547143936157227, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.01, 'num_input_tokens_seen': 729, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5152, 'grad_norm': 1.3547143936157227, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.01, 'num_input_tokens_seen': 1128, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.8312, 'grad_norm': 1.3547143936157227, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.01, 'num_input_tokens_seen': 1633, 'trained_token_ratio': 1.0}\n",
      "  1%|▎                                          | 1/125 [00:20<43:20, 20.97s/it]Unsloth: Will smartly offload gradients to save VRAM!\n",
      "Unsloth: Will smartly offload gradients to save VRAM!\n",
      "{'loss': 0.843, 'grad_norm': 1.7961305379867554, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.02, 'num_input_tokens_seen': 3644, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.8258, 'grad_norm': 1.7961305379867554, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.02, 'num_input_tokens_seen': 1886, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.531, 'grad_norm': 1.7961305379867554, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.02, 'num_input_tokens_seen': 2189, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5375, 'grad_norm': 1.7961305379867554, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.02, 'num_input_tokens_seen': 1057, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.3516, 'grad_norm': 1.7961305379867554, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.02, 'num_input_tokens_seen': 1769, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5619, 'grad_norm': 1.7961305379867554, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.02, 'num_input_tokens_seen': 2997, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.4821, 'grad_norm': 1.7961305379867554, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.02, 'num_input_tokens_seen': 2408, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.7046, 'grad_norm': 1.7961305379867554, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.02, 'num_input_tokens_seen': 1401, 'trained_token_ratio': 1.0}\n",
      "  2%|▋                                          | 2/125 [00:29<25:24, 12.39s/it]Unsloth: Will smartly offload gradients to save VRAM!\n",
      "Unsloth: Will smartly offload gradients to save VRAM!\n",
      "{'loss': 1.2308, 'grad_norm': 1.3860015869140625, 'learning_rate': 3e-06, 'epoch': 0.02, 'num_input_tokens_seen': 7033, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.2239, 'grad_norm': 1.3860015869140625, 'learning_rate': 3e-06, 'epoch': 0.02, 'num_input_tokens_seen': 2284, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6116, 'grad_norm': 1.3860015869140625, 'learning_rate': 3e-06, 'epoch': 0.02, 'num_input_tokens_seen': 2891, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4202, 'grad_norm': 1.3860015869140625, 'learning_rate': 3e-06, 'epoch': 0.02, 'num_input_tokens_seen': 1506, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6379, 'grad_norm': 1.3860015869140625, 'learning_rate': 3e-06, 'epoch': 0.02, 'num_input_tokens_seen': 2708, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2519, 'grad_norm': 1.3860015869140625, 'learning_rate': 3e-06, 'epoch': 0.02, 'num_input_tokens_seen': 3256, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1477, 'grad_norm': 1.3860015869140625, 'learning_rate': 3e-06, 'epoch': 0.02, 'num_input_tokens_seen': 3794, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.7122, 'grad_norm': 1.3860015869140625, 'learning_rate': 3e-06, 'epoch': 0.02, 'num_input_tokens_seen': 4457, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.9933, 'grad_norm': 1.6097633838653564, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.03, 'num_input_tokens_seen': 4123, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3067, 'grad_norm': 1.6097633838653564, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.03, 'num_input_tokens_seen': 9488, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.6563, 'grad_norm': 1.6097633838653564, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.03, 'num_input_tokens_seen': 3788, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4703, 'grad_norm': 1.6097633838653564, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.03, 'num_input_tokens_seen': 2254, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.115, 'grad_norm': 1.6097633838653564, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.03, 'num_input_tokens_seen': 4580, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.3237, 'grad_norm': 1.6097633838653564, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.03, 'num_input_tokens_seen': 3110, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.9681, 'grad_norm': 1.6097633838653564, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.03, 'num_input_tokens_seen': 5226, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9813, 'grad_norm': 1.6097633838653564, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.03, 'num_input_tokens_seen': 6064, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3665, 'grad_norm': 1.7833386659622192, 'learning_rate': 5e-06, 'epoch': 0.04, 'num_input_tokens_seen': 3105, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.0646, 'grad_norm': 1.7833386659622192, 'learning_rate': 5e-06, 'epoch': 0.04, 'num_input_tokens_seen': 3962, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.817, 'grad_norm': 1.7833386659622192, 'learning_rate': 5e-06, 'epoch': 0.04, 'num_input_tokens_seen': 10941, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.4743, 'grad_norm': 1.7833386659622192, 'learning_rate': 5e-06, 'epoch': 0.04, 'num_input_tokens_seen': 6550, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.2546, 'grad_norm': 1.7833386659622192, 'learning_rate': 5e-06, 'epoch': 0.04, 'num_input_tokens_seen': 7418, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3725, 'grad_norm': 1.7833386659622192, 'learning_rate': 5e-06, 'epoch': 0.04, 'num_input_tokens_seen': 5833, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2568, 'grad_norm': 1.7833386659622192, 'learning_rate': 5e-06, 'epoch': 0.04, 'num_input_tokens_seen': 4938, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.1611, 'grad_norm': 1.7833386659622192, 'learning_rate': 5e-06, 'epoch': 0.04, 'num_input_tokens_seen': 5329, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.765, 'grad_norm': 1.5584380626678467, 'learning_rate': 6e-06, 'epoch': 0.05, 'num_input_tokens_seen': 13667, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3503, 'grad_norm': 1.5584380626678467, 'learning_rate': 6e-06, 'epoch': 0.05, 'num_input_tokens_seen': 5623, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.8886, 'grad_norm': 1.5584380626678467, 'learning_rate': 6e-06, 'epoch': 0.05, 'num_input_tokens_seen': 3649, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4458, 'grad_norm': 1.5584380626678467, 'learning_rate': 6e-06, 'epoch': 0.05, 'num_input_tokens_seen': 6206, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5184, 'grad_norm': 1.5584380626678467, 'learning_rate': 6e-06, 'epoch': 0.05, 'num_input_tokens_seen': 4521, 'trained_token_ratio': 1.0}\n",
      "  5%|██                                         | 6/125 [00:30<04:27,  2.25s/it]{'loss': 1.8398, 'grad_norm': 1.5584380626678467, 'learning_rate': 6e-06, 'epoch': 0.05, 'num_input_tokens_seen': 8872, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4217, 'grad_norm': 1.5584380626678467, 'learning_rate': 6e-06, 'epoch': 0.05, 'num_input_tokens_seen': 7817, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.6901, 'grad_norm': 1.5584380626678467, 'learning_rate': 6e-06, 'epoch': 0.05, 'num_input_tokens_seen': 6900, 'trained_token_ratio': 0.9999999403953552}\n",
      "  5%|██                                         | 6/125 [00:28<04:12,  2.12s/it]Unsloth: Will smartly offload gradients to save VRAM!\n",
      "{'loss': 1.6134, 'grad_norm': 1.1673915386199951, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.06, 'num_input_tokens_seen': 7284, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2045, 'grad_norm': 1.1673915386199951, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.06, 'num_input_tokens_seen': 15718, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.6359, 'grad_norm': 1.1673915386199951, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.06, 'num_input_tokens_seen': 6615, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3125, 'grad_norm': 1.1673915386199951, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.06, 'num_input_tokens_seen': 9022, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8879, 'grad_norm': 1.1673915386199951, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.06, 'num_input_tokens_seen': 10567, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2134, 'grad_norm': 1.1673915386199951, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.06, 'num_input_tokens_seen': 8034, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9236, 'grad_norm': 1.1673915386199951, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.06, 'num_input_tokens_seen': 4469, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6478, 'grad_norm': 1.1673915386199951, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.06, 'num_input_tokens_seen': 5479, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.9541, 'grad_norm': 1.32787024974823, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.06, 'num_input_tokens_seen': 9651, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2827, 'grad_norm': 1.32787024974823, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.06, 'num_input_tokens_seen': 12991, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.5535, 'grad_norm': 1.32787024974823, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.06, 'num_input_tokens_seen': 18342, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.8089, 'grad_norm': 1.32787024974823, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.06, 'num_input_tokens_seen': 11031, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.5858, 'grad_norm': 1.32787024974823, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.06, 'num_input_tokens_seen': 8671, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.8205, 'grad_norm': 1.32787024974823, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.06, 'num_input_tokens_seen': 7476, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.8326, 'grad_norm': 1.32787024974823, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.06, 'num_input_tokens_seen': 4943, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8919, 'grad_norm': 1.32787024974823, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.06, 'num_input_tokens_seen': 6306, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6058, 'grad_norm': 18.870725631713867, 'learning_rate': 9e-06, 'epoch': 0.07, 'num_input_tokens_seen': 20515, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6252, 'grad_norm': 18.870725631713867, 'learning_rate': 9e-06, 'epoch': 0.07, 'num_input_tokens_seen': 8279, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4702, 'grad_norm': 18.870725631713867, 'learning_rate': 9e-06, 'epoch': 0.07, 'num_input_tokens_seen': 10591, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.359, 'grad_norm': 18.870725631713867, 'learning_rate': 9e-06, 'epoch': 0.07, 'num_input_tokens_seen': 12057, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.7515, 'grad_norm': 18.870725631713867, 'learning_rate': 9e-06, 'epoch': 0.07, 'num_input_tokens_seen': 14780, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.5057, 'grad_norm': 18.870725631713867, 'learning_rate': 9e-06, 'epoch': 0.07, 'num_input_tokens_seen': 9534, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1084, 'grad_norm': 18.870725631713867, 'learning_rate': 9e-06, 'epoch': 0.07, 'num_input_tokens_seen': 6672, 'trained_token_ratio': 1.0}\n",
      "{'loss': 21.4743, 'grad_norm': 18.870725631713867, 'learning_rate': 9e-06, 'epoch': 0.07, 'num_input_tokens_seen': 5136, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.318, 'grad_norm': 1.2800825834274292, 'learning_rate': 1e-05, 'epoch': 0.08, 'num_input_tokens_seen': 23457, 'trained_token_ratio': 1.0}\n",
      "  8%|███▎                                      | 10/125 [00:32<02:49,  1.47s/it]\u001b[1mINFO    \u001b[0m | \u001b[36mtrainer.py:2250 (train)\u001b[0m - \u001b[1m\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| Task                 | File:Line       |   Depth | Time (s)   | Percentage (%)   |\n",
      "+======================+=================+=========+============+==================+\n",
      "| 1. get_batch_samples | patching.py:501 |       0 | 0.10 s     | \u001b[94m0.36 %\u001b[0m           |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 2. forward           | patching.py:597 |       0 | 20.08 s    | \u001b[93m73.42 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 3. sync gradients    | patching.py:601 |       0 | 4.96 s     | \u001b[94m18.14 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 4. optimizer step    | patching.py:666 |       0 | 2.21 s     | \u001b[94m8.08 %\u001b[0m           |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "\n",
      "Total time elapsed: 27.35 seconds.\u001b[0m\n",
      "{'loss': 1.8007, 'grad_norm': 1.2800825834274292, 'learning_rate': 1e-05, 'epoch': 0.08, 'num_input_tokens_seen': 9053, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2977, 'grad_norm': 1.2800825834274292, 'learning_rate': 1e-05, 'epoch': 0.08, 'num_input_tokens_seen': 17058, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6667, 'grad_norm': 1.2800825834274292, 'learning_rate': 1e-05, 'epoch': 0.08, 'num_input_tokens_seen': 10853, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6274, 'grad_norm': 1.2800825834274292, 'learning_rate': 1e-05, 'epoch': 0.08, 'num_input_tokens_seen': 13666, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.7877, 'grad_norm': 1.2800825834274292, 'learning_rate': 1e-05, 'epoch': 0.08, 'num_input_tokens_seen': 12104, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7438, 'grad_norm': 1.2800825834274292, 'learning_rate': 1e-05, 'epoch': 0.08, 'num_input_tokens_seen': 7406, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.3511, 'grad_norm': 1.2800825834274292, 'learning_rate': 1e-05, 'epoch': 0.08, 'num_input_tokens_seen': 5536, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.7454, 'grad_norm': 1.311460256576538, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.09, 'num_input_tokens_seen': 13406, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.2354, 'grad_norm': 1.311460256576538, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.09, 'num_input_tokens_seen': 8138, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2749, 'grad_norm': 1.311460256576538, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.09, 'num_input_tokens_seen': 25757, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2448, 'grad_norm': 1.311460256576538, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.09, 'num_input_tokens_seen': 9818, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9666, 'grad_norm': 1.311460256576538, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.09, 'num_input_tokens_seen': 12152, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2449, 'grad_norm': 1.311460256576538, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.09, 'num_input_tokens_seen': 6074, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.3185, 'grad_norm': 1.311460256576538, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.09, 'num_input_tokens_seen': 18901, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3901, 'grad_norm': 1.311460256576538, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.09, 'num_input_tokens_seen': 15372, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.1744, 'grad_norm': 1.4068208932876587, 'learning_rate': 1.2e-05, 'epoch': 0.1, 'num_input_tokens_seen': 27520, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.8634, 'grad_norm': 1.4068208932876587, 'learning_rate': 1.2e-05, 'epoch': 0.1, 'num_input_tokens_seen': 13079, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6896, 'grad_norm': 1.4068208932876587, 'learning_rate': 1.2e-05, 'epoch': 0.1, 'num_input_tokens_seen': 8972, 'trained_token_ratio': 1.0}\n",
      " 10%|████                                      | 12/125 [00:39<02:06,  1.12s/it]{'loss': 0.9094, 'grad_norm': 1.4068208932876587, 'learning_rate': 1.2e-05, 'epoch': 0.1, 'num_input_tokens_seen': 6795, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1901, 'grad_norm': 1.4068208932876587, 'learning_rate': 1.2e-05, 'epoch': 0.1, 'num_input_tokens_seen': 16843, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0734, 'grad_norm': 1.4068208932876587, 'learning_rate': 1.2e-05, 'epoch': 0.1, 'num_input_tokens_seen': 20585, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.181, 'grad_norm': 1.4068208932876587, 'learning_rate': 1.2e-05, 'epoch': 0.1, 'num_input_tokens_seen': 14498, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6102, 'grad_norm': 1.4068208932876587, 'learning_rate': 1.2e-05, 'epoch': 0.1, 'num_input_tokens_seen': 10662, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.3058, 'grad_norm': 1.4393038749694824, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.1, 'num_input_tokens_seen': 29350, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.3188, 'grad_norm': 1.4393038749694824, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.1, 'num_input_tokens_seen': 11612, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7846, 'grad_norm': 1.4393038749694824, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.1, 'num_input_tokens_seen': 14226, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4577, 'grad_norm': 1.4393038749694824, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.1, 'num_input_tokens_seen': 9833, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3563, 'grad_norm': 1.4393038749694824, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.1, 'num_input_tokens_seen': 7500, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4137, 'grad_norm': 1.4393038749694824, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.1, 'num_input_tokens_seen': 22321, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3345, 'grad_norm': 1.4393038749694824, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.1, 'num_input_tokens_seen': 18258, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3923, 'grad_norm': 1.4393038749694824, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.1, 'num_input_tokens_seen': 15802, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1289, 'grad_norm': 1.9860292673110962, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.11, 'num_input_tokens_seen': 30723, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6746, 'grad_norm': 1.9860292673110962, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.11, 'num_input_tokens_seen': 23665, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 2.2529, 'grad_norm': 1.9860292673110962, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.11, 'num_input_tokens_seen': 19407, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.8143, 'grad_norm': 1.9860292673110962, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.11, 'num_input_tokens_seen': 12424, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.9477, 'grad_norm': 1.9860292673110962, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.11, 'num_input_tokens_seen': 16733, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.4894, 'grad_norm': 1.9860292673110962, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.11, 'num_input_tokens_seen': 15044, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.8078, 'grad_norm': 1.9860292673110962, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.11, 'num_input_tokens_seen': 8181, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1577, 'grad_norm': 1.9860292673110962, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.11, 'num_input_tokens_seen': 10617, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2772, 'grad_norm': 1.5236878395080566, 'learning_rate': 1.5e-05, 'epoch': 0.12, 'num_input_tokens_seen': 32575, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.0068, 'grad_norm': 1.5236878395080566, 'learning_rate': 1.5e-05, 'epoch': 0.12, 'num_input_tokens_seen': 20540, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4566, 'grad_norm': 1.5236878395080566, 'learning_rate': 1.5e-05, 'epoch': 0.12, 'num_input_tokens_seen': 25373, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0415, 'grad_norm': 1.5236878395080566, 'learning_rate': 1.5e-05, 'epoch': 0.12, 'num_input_tokens_seen': 13220, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5106, 'grad_norm': 1.5236878395080566, 'learning_rate': 1.5e-05, 'epoch': 0.12, 'num_input_tokens_seen': 17852, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0693, 'grad_norm': 1.5236878395080566, 'learning_rate': 1.5e-05, 'epoch': 0.12, 'num_input_tokens_seen': 15858, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0774, 'grad_norm': 1.5236878395080566, 'learning_rate': 1.5e-05, 'epoch': 0.12, 'num_input_tokens_seen': 11305, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.6968, 'grad_norm': 1.5236878395080566, 'learning_rate': 1.5e-05, 'epoch': 0.12, 'num_input_tokens_seen': 8832, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5367, 'grad_norm': 1.7080764770507812, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.13, 'num_input_tokens_seen': 21677, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.7398, 'grad_norm': 1.7080764770507812, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.13, 'num_input_tokens_seen': 26952, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.5802, 'grad_norm': 1.7080764770507812, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.13, 'num_input_tokens_seen': 11998, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1756, 'grad_norm': 1.7080764770507812, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.13, 'num_input_tokens_seen': 34734, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.4018, 'grad_norm': 1.7080764770507812, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.13, 'num_input_tokens_seen': 9299, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0533, 'grad_norm': 1.7080764770507812, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.13, 'num_input_tokens_seen': 16792, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.3901, 'grad_norm': 1.7080764770507812, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.13, 'num_input_tokens_seen': 18838, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.5381, 'grad_norm': 1.7080764770507812, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.13, 'num_input_tokens_seen': 14095, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3703, 'grad_norm': 2.077577829360962, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.14, 'num_input_tokens_seen': 19629, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0683, 'grad_norm': 2.077577829360962, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.14, 'num_input_tokens_seen': 36814, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0781, 'grad_norm': 2.077577829360962, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.14, 'num_input_tokens_seen': 14624, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.6879, 'grad_norm': 2.077577829360962, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.14, 'num_input_tokens_seen': 9411, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.5686, 'grad_norm': 2.077577829360962, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.14, 'num_input_tokens_seen': 17543, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.3759, 'grad_norm': 2.077577829360962, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.14, 'num_input_tokens_seen': 22479, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5345, 'grad_norm': 2.077577829360962, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.14, 'num_input_tokens_seen': 12295, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9761, 'grad_norm': 2.077577829360962, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.14, 'num_input_tokens_seen': 28316, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0919, 'grad_norm': 2.024508476257324, 'learning_rate': 1.8e-05, 'epoch': 0.14, 'num_input_tokens_seen': 38983, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9828, 'grad_norm': 2.024508476257324, 'learning_rate': 1.8e-05, 'epoch': 0.14, 'num_input_tokens_seen': 29658, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2998, 'grad_norm': 2.024508476257324, 'learning_rate': 1.8e-05, 'epoch': 0.14, 'num_input_tokens_seen': 23570, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.5688, 'grad_norm': 2.024508476257324, 'learning_rate': 1.8e-05, 'epoch': 0.14, 'num_input_tokens_seen': 20533, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6906, 'grad_norm': 2.024508476257324, 'learning_rate': 1.8e-05, 'epoch': 0.14, 'num_input_tokens_seen': 15249, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 2.89, 'grad_norm': 2.024508476257324, 'learning_rate': 1.8e-05, 'epoch': 0.14, 'num_input_tokens_seen': 9936, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6678, 'grad_norm': 2.024508476257324, 'learning_rate': 1.8e-05, 'epoch': 0.14, 'num_input_tokens_seen': 18269, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8004, 'grad_norm': 2.024508476257324, 'learning_rate': 1.8e-05, 'epoch': 0.14, 'num_input_tokens_seen': 12902, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6352, 'grad_norm': 1.2923781871795654, 'learning_rate': 1.9e-05, 'epoch': 0.15, 'num_input_tokens_seen': 26066, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1368, 'grad_norm': 1.2923781871795654, 'learning_rate': 1.9e-05, 'epoch': 0.15, 'num_input_tokens_seen': 42138, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0357, 'grad_norm': 1.2923781871795654, 'learning_rate': 1.9e-05, 'epoch': 0.15, 'num_input_tokens_seen': 32188, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.7731, 'grad_norm': 1.2923781871795654, 'learning_rate': 1.9e-05, 'epoch': 0.15, 'num_input_tokens_seen': 16128, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8797, 'grad_norm': 1.2923781871795654, 'learning_rate': 1.9e-05, 'epoch': 0.15, 'num_input_tokens_seen': 10468, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5608, 'grad_norm': 1.2923781871795654, 'learning_rate': 1.9e-05, 'epoch': 0.15, 'num_input_tokens_seen': 19158, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5932, 'grad_norm': 1.2923781871795654, 'learning_rate': 1.9e-05, 'epoch': 0.15, 'num_input_tokens_seen': 13671, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.7161, 'grad_norm': 1.2923781871795654, 'learning_rate': 1.9e-05, 'epoch': 0.15, 'num_input_tokens_seen': 21597, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.389, 'grad_norm': 1.396720290184021, 'learning_rate': 2e-05, 'epoch': 0.16, 'num_input_tokens_seen': 16984, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6993, 'grad_norm': 1.396720290184021, 'learning_rate': 2e-05, 'epoch': 0.16, 'num_input_tokens_seen': 47266, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0517, 'grad_norm': 1.396720290184021, 'learning_rate': 2e-05, 'epoch': 0.16, 'num_input_tokens_seen': 11074, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.5728, 'grad_norm': 1.396720290184021, 'learning_rate': 2e-05, 'epoch': 0.16, 'num_input_tokens_seen': 22570, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2634, 'grad_norm': 1.396720290184021, 'learning_rate': 2e-05, 'epoch': 0.16, 'num_input_tokens_seen': 27758, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.1789, 'grad_norm': 1.396720290184021, 'learning_rate': 2e-05, 'epoch': 0.16, 'num_input_tokens_seen': 14307, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2076, 'grad_norm': 1.396720290184021, 'learning_rate': 2e-05, 'epoch': 0.16, 'num_input_tokens_seen': 34452, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.07, 'grad_norm': 1.396720290184021, 'learning_rate': 2e-05, 'epoch': 0.16, 'num_input_tokens_seen': 20017, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1307, 'grad_norm': 1.4323477745056152, 'learning_rate': 2.1e-05, 'epoch': 0.17, 'num_input_tokens_seen': 49446, 'trained_token_ratio': 1.0}\n",
      " 17%|███████                                   | 21/125 [00:37<02:12,  1.28s/it]\u001b[1mINFO    \u001b[0m | \u001b[36mtrainer.py:2250 (train)\u001b[0m - \u001b[1m\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| Task                 | File:Line       |   Depth | Time (s)   | Percentage (%)   |\n",
      "+======================+=================+=========+============+==================+\n",
      "| 1. get_batch_samples | patching.py:501 |       0 | 0.16 s     | \u001b[94m0.41 %\u001b[0m           |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 2. forward           | patching.py:597 |       0 | 25.99 s    | \u001b[93m68.64 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 3. sync gradients    | patching.py:601 |       0 | 8.09 s     | \u001b[94m21.38 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 4. optimizer step    | patching.py:666 |       0 | 3.63 s     | \u001b[94m9.57 %\u001b[0m           |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "\n",
      "Total time elapsed: 37.87 seconds.\u001b[0m\n",
      "{'loss': 2.4206, 'grad_norm': 1.4323477745056152, 'learning_rate': 2.1e-05, 'epoch': 0.17, 'num_input_tokens_seen': 15314, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2626, 'grad_norm': 1.4323477745056152, 'learning_rate': 2.1e-05, 'epoch': 0.17, 'num_input_tokens_seen': 21144, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0475, 'grad_norm': 1.4323477745056152, 'learning_rate': 2.1e-05, 'epoch': 0.17, 'num_input_tokens_seen': 29163, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.5014, 'grad_norm': 1.4323477745056152, 'learning_rate': 2.1e-05, 'epoch': 0.17, 'num_input_tokens_seen': 36106, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2656, 'grad_norm': 1.4323477745056152, 'learning_rate': 2.1e-05, 'epoch': 0.17, 'num_input_tokens_seen': 23785, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1871, 'grad_norm': 1.4323477745056152, 'learning_rate': 2.1e-05, 'epoch': 0.17, 'num_input_tokens_seen': 18046, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6678, 'grad_norm': 1.4323477745056152, 'learning_rate': 2.1e-05, 'epoch': 0.17, 'num_input_tokens_seen': 11552, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9578, 'grad_norm': 1.17359459400177, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.18, 'num_input_tokens_seen': 51920, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9034, 'grad_norm': 1.17359459400177, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.18, 'num_input_tokens_seen': 12316, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.652, 'grad_norm': 1.17359459400177, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.18, 'num_input_tokens_seen': 19100, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.2776, 'grad_norm': 1.17359459400177, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.18, 'num_input_tokens_seen': 16178, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6854, 'grad_norm': 1.17359459400177, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.18, 'num_input_tokens_seen': 22218, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.2417, 'grad_norm': 1.17359459400177, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.18, 'num_input_tokens_seen': 30381, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0391, 'grad_norm': 1.17359459400177, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.18, 'num_input_tokens_seen': 24869, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5562, 'grad_norm': 1.17359459400177, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.18, 'num_input_tokens_seen': 37405, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3593, 'grad_norm': 1.1647499799728394, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.18, 'num_input_tokens_seen': 53555, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.8156, 'grad_norm': 1.1647499799728394, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.18, 'num_input_tokens_seen': 16986, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5615, 'grad_norm': 1.1647499799728394, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.18, 'num_input_tokens_seen': 20135, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2957, 'grad_norm': 1.1647499799728394, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.18, 'num_input_tokens_seen': 23328, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4535, 'grad_norm': 1.1647499799728394, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.18, 'num_input_tokens_seen': 31684, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1563, 'grad_norm': 1.1647499799728394, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.18, 'num_input_tokens_seen': 38855, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2174, 'grad_norm': 1.1647499799728394, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.18, 'num_input_tokens_seen': 26000, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7917, 'grad_norm': 1.1647499799728394, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.18, 'num_input_tokens_seen': 12998, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.247, 'grad_norm': 1.696742057800293, 'learning_rate': 2.4e-05, 'epoch': 0.19, 'num_input_tokens_seen': 55115, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.6458, 'grad_norm': 1.696742057800293, 'learning_rate': 2.4e-05, 'epoch': 0.19, 'num_input_tokens_seen': 24281, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.297, 'grad_norm': 1.696742057800293, 'learning_rate': 2.4e-05, 'epoch': 0.19, 'num_input_tokens_seen': 40251, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.0169, 'grad_norm': 1.696742057800293, 'learning_rate': 2.4e-05, 'epoch': 0.19, 'num_input_tokens_seen': 32764, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.9319, 'grad_norm': 1.696742057800293, 'learning_rate': 2.4e-05, 'epoch': 0.19, 'num_input_tokens_seen': 21070, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.8514, 'grad_norm': 1.696742057800293, 'learning_rate': 2.4e-05, 'epoch': 0.19, 'num_input_tokens_seen': 13559, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.1835, 'grad_norm': 1.696742057800293, 'learning_rate': 2.4e-05, 'epoch': 0.19, 'num_input_tokens_seen': 27056, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.0034, 'grad_norm': 1.696742057800293, 'learning_rate': 2.4e-05, 'epoch': 0.19, 'num_input_tokens_seen': 17616, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2577, 'grad_norm': 1.2911088466644287, 'learning_rate': 2.5e-05, 'epoch': 0.2, 'num_input_tokens_seen': 41570, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1409, 'grad_norm': 1.2911088466644287, 'learning_rate': 2.5e-05, 'epoch': 0.2, 'num_input_tokens_seen': 56729, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0105, 'grad_norm': 1.2911088466644287, 'learning_rate': 2.5e-05, 'epoch': 0.2, 'num_input_tokens_seen': 14232, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.5612, 'grad_norm': 1.2911088466644287, 'learning_rate': 2.5e-05, 'epoch': 0.2, 'num_input_tokens_seen': 34040, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2967, 'grad_norm': 1.2911088466644287, 'learning_rate': 2.5e-05, 'epoch': 0.2, 'num_input_tokens_seen': 18380, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5422, 'grad_norm': 1.2911088466644287, 'learning_rate': 2.5e-05, 'epoch': 0.2, 'num_input_tokens_seen': 28300, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.3367, 'grad_norm': 1.2911088466644287, 'learning_rate': 2.5e-05, 'epoch': 0.2, 'num_input_tokens_seen': 21876, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.1218, 'grad_norm': 1.2911088466644287, 'learning_rate': 2.5e-05, 'epoch': 0.2, 'num_input_tokens_seen': 25112, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4639, 'grad_norm': 1.1379554271697998, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.21, 'num_input_tokens_seen': 14960, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4358, 'grad_norm': 1.1379554271697998, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.21, 'num_input_tokens_seen': 19189, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4069, 'grad_norm': 1.1379554271697998, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.21, 'num_input_tokens_seen': 35462, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2528, 'grad_norm': 1.1379554271697998, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.21, 'num_input_tokens_seen': 43193, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.1587, 'grad_norm': 1.1379554271697998, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.21, 'num_input_tokens_seen': 58366, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2588, 'grad_norm': 1.1379554271697998, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.21, 'num_input_tokens_seen': 29548, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.5182, 'grad_norm': 1.1379554271697998, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.21, 'num_input_tokens_seen': 26121, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2818, 'grad_norm': 1.1379554271697998, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.21, 'num_input_tokens_seen': 22821, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.841, 'grad_norm': 1.3276675939559937, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.22, 'num_input_tokens_seen': 60590, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.4461, 'grad_norm': 1.3276675939559937, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.22, 'num_input_tokens_seen': 45286, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3237, 'grad_norm': 1.3276675939559937, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.22, 'num_input_tokens_seen': 23545, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9403, 'grad_norm': 1.3276675939559937, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.22, 'num_input_tokens_seen': 30750, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.8225, 'grad_norm': 1.3276675939559937, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.22, 'num_input_tokens_seen': 27145, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 2.6848, 'grad_norm': 1.3276675939559937, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.22, 'num_input_tokens_seen': 19867, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1207, 'grad_norm': 1.3276675939559937, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.22, 'num_input_tokens_seen': 15553, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2776, 'grad_norm': 1.3276675939559937, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.22, 'num_input_tokens_seen': 37219, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2632, 'grad_norm': 1.1502567529678345, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.22, 'num_input_tokens_seen': 62214, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4809, 'grad_norm': 1.1502567529678345, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.22, 'num_input_tokens_seen': 24318, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1951, 'grad_norm': 1.1502567529678345, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.22, 'num_input_tokens_seen': 15816, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1209, 'grad_norm': 1.1502567529678345, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.22, 'num_input_tokens_seen': 31983, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.5557, 'grad_norm': 1.1502567529678345, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.22, 'num_input_tokens_seen': 28131, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8544, 'grad_norm': 1.1502567529678345, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.22, 'num_input_tokens_seen': 20498, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8875, 'grad_norm': 1.1502567529678345, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.22, 'num_input_tokens_seen': 38554, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6847, 'grad_norm': 1.1502567529678345, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.22, 'num_input_tokens_seen': 46684, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5824, 'grad_norm': 1.0262022018432617, 'learning_rate': 2.9e-05, 'epoch': 0.23, 'num_input_tokens_seen': 63822, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2725, 'grad_norm': 1.0262022018432617, 'learning_rate': 2.9e-05, 'epoch': 0.23, 'num_input_tokens_seen': 33048, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8519, 'grad_norm': 1.0262022018432617, 'learning_rate': 2.9e-05, 'epoch': 0.23, 'num_input_tokens_seen': 39634, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3949, 'grad_norm': 1.0262022018432617, 'learning_rate': 2.9e-05, 'epoch': 0.23, 'num_input_tokens_seen': 48124, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4006, 'grad_norm': 1.0262022018432617, 'learning_rate': 2.9e-05, 'epoch': 0.23, 'num_input_tokens_seen': 28983, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9593, 'grad_norm': 1.0262022018432617, 'learning_rate': 2.9e-05, 'epoch': 0.23, 'num_input_tokens_seen': 25065, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2328, 'grad_norm': 1.0262022018432617, 'learning_rate': 2.9e-05, 'epoch': 0.23, 'num_input_tokens_seen': 16158, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.9114, 'grad_norm': 1.0262022018432617, 'learning_rate': 2.9e-05, 'epoch': 0.23, 'num_input_tokens_seen': 21061, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2371, 'grad_norm': 0.7820402979850769, 'learning_rate': 3e-05, 'epoch': 0.24, 'num_input_tokens_seen': 40869, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2786, 'grad_norm': 0.7820402979850769, 'learning_rate': 3e-05, 'epoch': 0.24, 'num_input_tokens_seen': 65460, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.9078, 'grad_norm': 0.7820402979850769, 'learning_rate': 3e-05, 'epoch': 0.24, 'num_input_tokens_seen': 21823, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6657, 'grad_norm': 0.7820402979850769, 'learning_rate': 3e-05, 'epoch': 0.24, 'num_input_tokens_seen': 49407, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.8736, 'grad_norm': 0.7820402979850769, 'learning_rate': 3e-05, 'epoch': 0.24, 'num_input_tokens_seen': 29892, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.6449, 'grad_norm': 0.7820402979850769, 'learning_rate': 3e-05, 'epoch': 0.24, 'num_input_tokens_seen': 16790, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6524, 'grad_norm': 0.7820402979850769, 'learning_rate': 3e-05, 'epoch': 0.24, 'num_input_tokens_seen': 25870, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0584, 'grad_norm': 0.7820402979850769, 'learning_rate': 3e-05, 'epoch': 0.24, 'num_input_tokens_seen': 34070, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.989, 'grad_norm': 1.019315242767334, 'learning_rate': 3.1e-05, 'epoch': 0.25, 'num_input_tokens_seen': 22494, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4702, 'grad_norm': 1.019315242767334, 'learning_rate': 3.1e-05, 'epoch': 0.25, 'num_input_tokens_seen': 30648, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.7696, 'grad_norm': 1.019315242767334, 'learning_rate': 3.1e-05, 'epoch': 0.25, 'num_input_tokens_seen': 41839, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0266, 'grad_norm': 1.019315242767334, 'learning_rate': 3.1e-05, 'epoch': 0.25, 'num_input_tokens_seen': 50499, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.0973, 'grad_norm': 1.019315242767334, 'learning_rate': 3.1e-05, 'epoch': 0.25, 'num_input_tokens_seen': 66631, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.9494, 'grad_norm': 1.019315242767334, 'learning_rate': 3.1e-05, 'epoch': 0.25, 'num_input_tokens_seen': 34885, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9342, 'grad_norm': 1.019315242767334, 'learning_rate': 3.1e-05, 'epoch': 0.25, 'num_input_tokens_seen': 26623, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.1721, 'grad_norm': 1.019315242767334, 'learning_rate': 3.1e-05, 'epoch': 0.25, 'num_input_tokens_seen': 17409, 'trained_token_ratio': 1.0}\n",
      " 25%|██████████▍                               | 31/125 [00:48<00:55,  1.69it/s]Unsloth: Will smartly offload gradients to save VRAM!\n",
      "{'loss': 0.9046, 'grad_norm': 0.5344029068946838, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.26, 'num_input_tokens_seen': 68828, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1152, 'grad_norm': 0.5344029068946838, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.26, 'num_input_tokens_seen': 18363, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3838, 'grad_norm': 0.5344029068946838, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.26, 'num_input_tokens_seen': 36191, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2096, 'grad_norm': 0.5344029068946838, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.26, 'num_input_tokens_seen': 27757, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.5627, 'grad_norm': 0.5344029068946838, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.26, 'num_input_tokens_seen': 52462, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4716, 'grad_norm': 0.5344029068946838, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.26, 'num_input_tokens_seen': 31795, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6558, 'grad_norm': 0.5344029068946838, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.26, 'num_input_tokens_seen': 23516, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2456, 'grad_norm': 0.5344029068946838, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.26, 'num_input_tokens_seen': 43757, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.5866, 'grad_norm': 0.514685869216919, 'learning_rate': 3.3e-05, 'epoch': 0.26, 'num_input_tokens_seen': 70703, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3286, 'grad_norm': 0.514685869216919, 'learning_rate': 3.3e-05, 'epoch': 0.26, 'num_input_tokens_seen': 32720, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0959, 'grad_norm': 0.514685869216919, 'learning_rate': 3.3e-05, 'epoch': 0.26, 'num_input_tokens_seen': 24328, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5779, 'grad_norm': 0.514685869216919, 'learning_rate': 3.3e-05, 'epoch': 0.26, 'num_input_tokens_seen': 37242, 'trained_token_ratio': 1.0}\n",
      " 26%|███████████                               | 33/125 [00:51<00:56,  1.64it/s]{'loss': 0.9703, 'grad_norm': 0.514685869216919, 'learning_rate': 3.3e-05, 'epoch': 0.26, 'num_input_tokens_seen': 54019, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.8228, 'grad_norm': 0.514685869216919, 'learning_rate': 3.3e-05, 'epoch': 0.26, 'num_input_tokens_seen': 45038, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8049, 'grad_norm': 0.514685869216919, 'learning_rate': 3.3e-05, 'epoch': 0.26, 'num_input_tokens_seen': 18989, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3981, 'grad_norm': 0.514685869216919, 'learning_rate': 3.3e-05, 'epoch': 0.26, 'num_input_tokens_seen': 28582, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.8977, 'grad_norm': 0.5182068943977356, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.27, 'num_input_tokens_seen': 33872, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9229, 'grad_norm': 0.5182068943977356, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.27, 'num_input_tokens_seen': 38420, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.112, 'grad_norm': 0.5182068943977356, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.27, 'num_input_tokens_seen': 25305, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3075, 'grad_norm': 0.5182068943977356, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.27, 'num_input_tokens_seen': 55305, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7976, 'grad_norm': 0.5182068943977356, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.27, 'num_input_tokens_seen': 72768, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6952, 'grad_norm': 0.5182068943977356, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.27, 'num_input_tokens_seen': 19557, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.6955, 'grad_norm': 0.5182068943977356, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.27, 'num_input_tokens_seen': 46302, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0196, 'grad_norm': 0.5182068943977356, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.27, 'num_input_tokens_seen': 29719, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.6458, 'grad_norm': 0.5165284276008606, 'learning_rate': 3.5e-05, 'epoch': 0.28, 'num_input_tokens_seen': 26154, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9877, 'grad_norm': 0.5165284276008606, 'learning_rate': 3.5e-05, 'epoch': 0.28, 'num_input_tokens_seen': 76520, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.7598, 'grad_norm': 0.5165284276008606, 'learning_rate': 3.5e-05, 'epoch': 0.28, 'num_input_tokens_seen': 20181, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0706, 'grad_norm': 0.5165284276008606, 'learning_rate': 3.5e-05, 'epoch': 0.28, 'num_input_tokens_seen': 34942, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8523, 'grad_norm': 0.5165284276008606, 'learning_rate': 3.5e-05, 'epoch': 0.28, 'num_input_tokens_seen': 48197, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2216, 'grad_norm': 0.5165284276008606, 'learning_rate': 3.5e-05, 'epoch': 0.28, 'num_input_tokens_seen': 39569, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3912, 'grad_norm': 0.5165284276008606, 'learning_rate': 3.5e-05, 'epoch': 0.28, 'num_input_tokens_seen': 30642, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0859, 'grad_norm': 0.5165284276008606, 'learning_rate': 3.5e-05, 'epoch': 0.28, 'num_input_tokens_seen': 58309, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.0271, 'grad_norm': 0.6461941003799438, 'learning_rate': 3.6e-05, 'epoch': 0.29, 'num_input_tokens_seen': 49445, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2825, 'grad_norm': 0.6461941003799438, 'learning_rate': 3.6e-05, 'epoch': 0.29, 'num_input_tokens_seen': 40452, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.8319, 'grad_norm': 0.6461941003799438, 'learning_rate': 3.6e-05, 'epoch': 0.29, 'num_input_tokens_seen': 77919, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.9833, 'grad_norm': 0.6461941003799438, 'learning_rate': 3.6e-05, 'epoch': 0.29, 'num_input_tokens_seen': 59584, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1577, 'grad_norm': 0.6461941003799438, 'learning_rate': 3.6e-05, 'epoch': 0.29, 'num_input_tokens_seen': 26721, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9769, 'grad_norm': 0.6461941003799438, 'learning_rate': 3.6e-05, 'epoch': 0.29, 'num_input_tokens_seen': 31501, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4053, 'grad_norm': 0.6461941003799438, 'learning_rate': 3.6e-05, 'epoch': 0.29, 'num_input_tokens_seen': 20574, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.5678, 'grad_norm': 0.6461941003799438, 'learning_rate': 3.6e-05, 'epoch': 0.29, 'num_input_tokens_seen': 35813, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.405, 'grad_norm': 0.5656741857528687, 'learning_rate': 3.7e-05, 'epoch': 0.3, 'num_input_tokens_seen': 32622, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.989, 'grad_norm': 0.5656741857528687, 'learning_rate': 3.7e-05, 'epoch': 0.3, 'num_input_tokens_seen': 81678, 'trained_token_ratio': 1.0}\n",
      " 30%|████████████▍                             | 37/125 [00:48<01:14,  1.18it/s]\u001b[1mINFO    \u001b[0m | \u001b[36mtrainer.py:2250 (train)\u001b[0m - \u001b[1m\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| Task                 | File:Line       |   Depth | Time (s)   | Percentage (%)   |\n",
      "+======================+=================+=========+============+==================+\n",
      "| 1. get_batch_samples | patching.py:501 |       0 | 0.23 s     | \u001b[94m0.47 %\u001b[0m           |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 2. forward           | patching.py:597 |       0 | 30.75 s    | \u001b[93m63.18 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 3. sync gradients    | patching.py:601 |       0 | 12.24 s    | \u001b[92m25.15 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 4. optimizer step    | patching.py:666 |       0 | 5.45 s     | \u001b[94m11.20 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "\n",
      "Total time elapsed: 48.68 seconds.\u001b[0m\n",
      "{'loss': 0.8465, 'grad_norm': 0.5656741857528687, 'learning_rate': 3.7e-05, 'epoch': 0.3, 'num_input_tokens_seen': 62186, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4087, 'grad_norm': 0.5656741857528687, 'learning_rate': 3.7e-05, 'epoch': 0.3, 'num_input_tokens_seen': 51636, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.6346, 'grad_norm': 0.5656741857528687, 'learning_rate': 3.7e-05, 'epoch': 0.3, 'num_input_tokens_seen': 41945, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.8777, 'grad_norm': 0.5656741857528687, 'learning_rate': 3.7e-05, 'epoch': 0.3, 'num_input_tokens_seen': 27553, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2167, 'grad_norm': 0.5656741857528687, 'learning_rate': 3.7e-05, 'epoch': 0.3, 'num_input_tokens_seen': 21349, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.9311, 'grad_norm': 0.5656741857528687, 'learning_rate': 3.7e-05, 'epoch': 0.3, 'num_input_tokens_seen': 37044, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2314, 'grad_norm': 0.6363309025764465, 'learning_rate': 3.8e-05, 'epoch': 0.3, 'num_input_tokens_seen': 83231, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4863, 'grad_norm': 0.6363309025764465, 'learning_rate': 3.8e-05, 'epoch': 0.3, 'num_input_tokens_seen': 21574, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.916, 'grad_norm': 0.6363309025764465, 'learning_rate': 3.8e-05, 'epoch': 0.3, 'num_input_tokens_seen': 37933, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4588, 'grad_norm': 0.6363309025764465, 'learning_rate': 3.8e-05, 'epoch': 0.3, 'num_input_tokens_seen': 63487, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.0742, 'grad_norm': 0.6363309025764465, 'learning_rate': 3.8e-05, 'epoch': 0.3, 'num_input_tokens_seen': 28286, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8984, 'grad_norm': 0.6363309025764465, 'learning_rate': 3.8e-05, 'epoch': 0.3, 'num_input_tokens_seen': 33356, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.353, 'grad_norm': 0.6363309025764465, 'learning_rate': 3.8e-05, 'epoch': 0.3, 'num_input_tokens_seen': 43184, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.4203, 'grad_norm': 0.6363309025764465, 'learning_rate': 3.8e-05, 'epoch': 0.3, 'num_input_tokens_seen': 52919, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3647, 'grad_norm': 0.6504945755004883, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.31, 'num_input_tokens_seen': 34398, 'trained_token_ratio': 1.0}\n",
      " 31%|█████████████                             | 39/125 [00:54<01:00,  1.42it/s]{'loss': 1.7638, 'grad_norm': 0.6504945755004883, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.31, 'num_input_tokens_seen': 85058, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 2.1081, 'grad_norm': 0.6504945755004883, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.31, 'num_input_tokens_seen': 38986, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.7571, 'grad_norm': 0.6504945755004883, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.31, 'num_input_tokens_seen': 29230, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.9732, 'grad_norm': 0.6504945755004883, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.31, 'num_input_tokens_seen': 65103, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8479, 'grad_norm': 0.6504945755004883, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.31, 'num_input_tokens_seen': 54258, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5209, 'grad_norm': 0.6504945755004883, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.31, 'num_input_tokens_seen': 22205, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9158, 'grad_norm': 0.6504945755004883, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.31, 'num_input_tokens_seen': 44314, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9167, 'grad_norm': 0.6463387608528137, 'learning_rate': 4e-05, 'epoch': 0.32, 'num_input_tokens_seen': 35531, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4705, 'grad_norm': 0.6463387608528137, 'learning_rate': 4e-05, 'epoch': 0.32, 'num_input_tokens_seen': 22810, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0587, 'grad_norm': 0.6463387608528137, 'learning_rate': 4e-05, 'epoch': 0.32, 'num_input_tokens_seen': 67171, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2181, 'grad_norm': 0.6463387608528137, 'learning_rate': 4e-05, 'epoch': 0.32, 'num_input_tokens_seen': 40226, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2101, 'grad_norm': 0.6463387608528137, 'learning_rate': 4e-05, 'epoch': 0.32, 'num_input_tokens_seen': 87174, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8919, 'grad_norm': 0.6463387608528137, 'learning_rate': 4e-05, 'epoch': 0.32, 'num_input_tokens_seen': 56321, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9578, 'grad_norm': 0.6463387608528137, 'learning_rate': 4e-05, 'epoch': 0.32, 'num_input_tokens_seen': 46103, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6451, 'grad_norm': 0.6463387608528137, 'learning_rate': 4e-05, 'epoch': 0.32, 'num_input_tokens_seen': 29843, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8906, 'grad_norm': 0.7589970231056213, 'learning_rate': 4.1e-05, 'epoch': 0.33, 'num_input_tokens_seen': 88951, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.7258, 'grad_norm': 0.7589970231056213, 'learning_rate': 4.1e-05, 'epoch': 0.33, 'num_input_tokens_seen': 36620, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9372, 'grad_norm': 0.7589970231056213, 'learning_rate': 4.1e-05, 'epoch': 0.33, 'num_input_tokens_seen': 23165, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0232, 'grad_norm': 0.7589970231056213, 'learning_rate': 4.1e-05, 'epoch': 0.33, 'num_input_tokens_seen': 68500, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3145, 'grad_norm': 0.7589970231056213, 'learning_rate': 4.1e-05, 'epoch': 0.33, 'num_input_tokens_seen': 30883, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1452, 'grad_norm': 0.7589970231056213, 'learning_rate': 4.1e-05, 'epoch': 0.33, 'num_input_tokens_seen': 41334, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1343, 'grad_norm': 0.7589970231056213, 'learning_rate': 4.1e-05, 'epoch': 0.33, 'num_input_tokens_seen': 47237, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.1569, 'grad_norm': 0.7589970231056213, 'learning_rate': 4.1e-05, 'epoch': 0.33, 'num_input_tokens_seen': 57569, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8994, 'grad_norm': 0.5832715630531311, 'learning_rate': 4.2e-05, 'epoch': 0.34, 'num_input_tokens_seen': 23490, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4198, 'grad_norm': 0.5832715630531311, 'learning_rate': 4.2e-05, 'epoch': 0.34, 'num_input_tokens_seen': 31747, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7567, 'grad_norm': 0.5832715630531311, 'learning_rate': 4.2e-05, 'epoch': 0.34, 'num_input_tokens_seen': 91940, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.013, 'grad_norm': 0.5832715630531311, 'learning_rate': 4.2e-05, 'epoch': 0.34, 'num_input_tokens_seen': 37537, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0346, 'grad_norm': 0.5832715630531311, 'learning_rate': 4.2e-05, 'epoch': 0.34, 'num_input_tokens_seen': 69810, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.9435, 'grad_norm': 0.5832715630531311, 'learning_rate': 4.2e-05, 'epoch': 0.34, 'num_input_tokens_seen': 58858, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.7992, 'grad_norm': 0.5832715630531311, 'learning_rate': 4.2e-05, 'epoch': 0.34, 'num_input_tokens_seen': 48300, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8669, 'grad_norm': 0.5832715630531311, 'learning_rate': 4.2e-05, 'epoch': 0.34, 'num_input_tokens_seen': 42385, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6869, 'grad_norm': 0.6311100125312805, 'learning_rate': 4.3e-05, 'epoch': 0.34, 'num_input_tokens_seen': 38693, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7283, 'grad_norm': 0.6311100125312805, 'learning_rate': 4.3e-05, 'epoch': 0.34, 'num_input_tokens_seen': 94071, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6068, 'grad_norm': 0.6311100125312805, 'learning_rate': 4.3e-05, 'epoch': 0.34, 'num_input_tokens_seen': 23783, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.4889, 'grad_norm': 0.6311100125312805, 'learning_rate': 4.3e-05, 'epoch': 0.34, 'num_input_tokens_seen': 71363, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9786, 'grad_norm': 0.6311100125312805, 'learning_rate': 4.3e-05, 'epoch': 0.34, 'num_input_tokens_seen': 32831, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0464, 'grad_norm': 0.6311100125312805, 'learning_rate': 4.3e-05, 'epoch': 0.34, 'num_input_tokens_seen': 60221, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5138, 'grad_norm': 0.6311100125312805, 'learning_rate': 4.3e-05, 'epoch': 0.34, 'num_input_tokens_seen': 49515, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7699, 'grad_norm': 0.6311100125312805, 'learning_rate': 4.3e-05, 'epoch': 0.34, 'num_input_tokens_seen': 43542, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2791, 'grad_norm': 0.600837767124176, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.35, 'num_input_tokens_seen': 96968, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3943, 'grad_norm': 0.600837767124176, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.35, 'num_input_tokens_seen': 39809, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8689, 'grad_norm': 0.600837767124176, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.35, 'num_input_tokens_seen': 33896, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9395, 'grad_norm': 0.600837767124176, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.35, 'num_input_tokens_seen': 73937, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9742, 'grad_norm': 0.600837767124176, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.35, 'num_input_tokens_seen': 24831, 'trained_token_ratio': 1.0}\n",
      " 35%|██████████████▊                           | 44/125 [00:59<00:52,  1.54it/s]{'loss': 1.3297, 'grad_norm': 0.600837767124176, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.35, 'num_input_tokens_seen': 44835, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9257, 'grad_norm': 0.600837767124176, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.35, 'num_input_tokens_seen': 61696, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2166, 'grad_norm': 0.600837767124176, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.35, 'num_input_tokens_seen': 50858, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4546, 'grad_norm': 0.7166121602058411, 'learning_rate': 4.5e-05, 'epoch': 0.36, 'num_input_tokens_seen': 25509, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2137, 'grad_norm': 0.7166121602058411, 'learning_rate': 4.5e-05, 'epoch': 0.36, 'num_input_tokens_seen': 34817, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9158, 'grad_norm': 0.7166121602058411, 'learning_rate': 4.5e-05, 'epoch': 0.36, 'num_input_tokens_seen': 98388, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.0644, 'grad_norm': 0.7166121602058411, 'learning_rate': 4.5e-05, 'epoch': 0.36, 'num_input_tokens_seen': 75283, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8192, 'grad_norm': 0.7166121602058411, 'learning_rate': 4.5e-05, 'epoch': 0.36, 'num_input_tokens_seen': 45794, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7279, 'grad_norm': 0.7166121602058411, 'learning_rate': 4.5e-05, 'epoch': 0.36, 'num_input_tokens_seen': 40754, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2908, 'grad_norm': 0.7166121602058411, 'learning_rate': 4.5e-05, 'epoch': 0.36, 'num_input_tokens_seen': 62883, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3611, 'grad_norm': 0.7166121602058411, 'learning_rate': 4.5e-05, 'epoch': 0.36, 'num_input_tokens_seen': 52041, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7138, 'grad_norm': 0.5092946290969849, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.37, 'num_input_tokens_seen': 26211, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1503, 'grad_norm': 0.5092946290969849, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.37, 'num_input_tokens_seen': 41725, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6737, 'grad_norm': 0.5092946290969849, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.37, 'num_input_tokens_seen': 35599, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3077, 'grad_norm': 0.5092946290969849, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.37, 'num_input_tokens_seen': 101223, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2763, 'grad_norm': 0.5092946290969849, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.37, 'num_input_tokens_seen': 53486, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6876, 'grad_norm': 0.5092946290969849, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.37, 'num_input_tokens_seen': 64616, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7769, 'grad_norm': 0.5092946290969849, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.37, 'num_input_tokens_seen': 77798, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4863, 'grad_norm': 0.5092946290969849, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.37, 'num_input_tokens_seen': 46808, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.2816, 'grad_norm': 0.5456611514091492, 'learning_rate': 4.7e-05, 'epoch': 0.38, 'num_input_tokens_seen': 103845, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.8704, 'grad_norm': 0.5456611514091492, 'learning_rate': 4.7e-05, 'epoch': 0.38, 'num_input_tokens_seen': 42855, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3222, 'grad_norm': 0.5456611514091492, 'learning_rate': 4.7e-05, 'epoch': 0.38, 'num_input_tokens_seen': 79814, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9126, 'grad_norm': 0.5456611514091492, 'learning_rate': 4.7e-05, 'epoch': 0.38, 'num_input_tokens_seen': 36688, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0388, 'grad_norm': 0.5456611514091492, 'learning_rate': 4.7e-05, 'epoch': 0.38, 'num_input_tokens_seen': 66111, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4082, 'grad_norm': 0.5456611514091492, 'learning_rate': 4.7e-05, 'epoch': 0.38, 'num_input_tokens_seen': 48000, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9881, 'grad_norm': 0.5456611514091492, 'learning_rate': 4.7e-05, 'epoch': 0.38, 'num_input_tokens_seen': 54785, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6476, 'grad_norm': 0.5456611514091492, 'learning_rate': 4.7e-05, 'epoch': 0.38, 'num_input_tokens_seen': 27138, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.049, 'grad_norm': 0.5189039707183838, 'learning_rate': 4.8e-05, 'epoch': 0.38, 'num_input_tokens_seen': 107414, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0025, 'grad_norm': 0.5189039707183838, 'learning_rate': 4.8e-05, 'epoch': 0.38, 'num_input_tokens_seen': 37479, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4728, 'grad_norm': 0.5189039707183838, 'learning_rate': 4.8e-05, 'epoch': 0.38, 'num_input_tokens_seen': 48847, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.8649, 'grad_norm': 0.5189039707183838, 'learning_rate': 4.8e-05, 'epoch': 0.38, 'num_input_tokens_seen': 43658, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.311, 'grad_norm': 0.5189039707183838, 'learning_rate': 4.8e-05, 'epoch': 0.38, 'num_input_tokens_seen': 81838, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0748, 'grad_norm': 0.5189039707183838, 'learning_rate': 4.8e-05, 'epoch': 0.38, 'num_input_tokens_seen': 67678, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6732, 'grad_norm': 0.5189039707183838, 'learning_rate': 4.8e-05, 'epoch': 0.38, 'num_input_tokens_seen': 55834, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6664, 'grad_norm': 0.5189039707183838, 'learning_rate': 4.8e-05, 'epoch': 0.38, 'num_input_tokens_seen': 27828, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6416, 'grad_norm': 0.4740128815174103, 'learning_rate': 4.9e-05, 'epoch': 0.39, 'num_input_tokens_seen': 38509, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3426, 'grad_norm': 0.4740128815174103, 'learning_rate': 4.9e-05, 'epoch': 0.39, 'num_input_tokens_seen': 50578, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0319, 'grad_norm': 0.4740128815174103, 'learning_rate': 4.9e-05, 'epoch': 0.39, 'num_input_tokens_seen': 110589, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8345, 'grad_norm': 0.4740128815174103, 'learning_rate': 4.9e-05, 'epoch': 0.39, 'num_input_tokens_seen': 84444, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0993, 'grad_norm': 0.4740128815174103, 'learning_rate': 4.9e-05, 'epoch': 0.39, 'num_input_tokens_seen': 57673, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1014, 'grad_norm': 0.4740128815174103, 'learning_rate': 4.9e-05, 'epoch': 0.39, 'num_input_tokens_seen': 28619, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3015, 'grad_norm': 0.4740128815174103, 'learning_rate': 4.9e-05, 'epoch': 0.39, 'num_input_tokens_seen': 70053, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.443, 'grad_norm': 0.4740128815174103, 'learning_rate': 4.9e-05, 'epoch': 0.39, 'num_input_tokens_seen': 44822, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0796, 'grad_norm': 0.5422195196151733, 'learning_rate': 5e-05, 'epoch': 0.4, 'num_input_tokens_seen': 85828, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1907, 'grad_norm': 0.5422195196151733, 'learning_rate': 5e-05, 'epoch': 0.4, 'num_input_tokens_seen': 29016, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2942, 'grad_norm': 0.5422195196151733, 'learning_rate': 5e-05, 'epoch': 0.4, 'num_input_tokens_seen': 58463, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.22, 'grad_norm': 0.5422195196151733, 'learning_rate': 5e-05, 'epoch': 0.4, 'num_input_tokens_seen': 112485, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6979, 'grad_norm': 0.5422195196151733, 'learning_rate': 5e-05, 'epoch': 0.4, 'num_input_tokens_seen': 70912, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9951, 'grad_norm': 0.5422195196151733, 'learning_rate': 5e-05, 'epoch': 0.4, 'num_input_tokens_seen': 39153, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5723, 'grad_norm': 0.5422195196151733, 'learning_rate': 5e-05, 'epoch': 0.4, 'num_input_tokens_seen': 51345, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.7816, 'grad_norm': 0.5422195196151733, 'learning_rate': 5e-05, 'epoch': 0.4, 'num_input_tokens_seen': 45493, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.9837, 'grad_norm': 0.4019942283630371, 'learning_rate': 5.1000000000000006e-05, 'epoch': 0.41, 'num_input_tokens_seen': 114503, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8961, 'grad_norm': 0.4019942283630371, 'learning_rate': 5.1000000000000006e-05, 'epoch': 0.41, 'num_input_tokens_seen': 40064, 'trained_token_ratio': 1.0}\n",
      " 41%|█████████████████▏                        | 51/125 [01:09<00:56,  1.30it/s]\u001b[1mINFO    \u001b[0m | \u001b[36mtrainer.py:2250 (train)\u001b[0m - \u001b[1m\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| Task                 | File:Line       |   Depth | Time (s)   | Percentage (%)   |\n",
      "+======================+=================+=========+============+==================+\n",
      "| 1. get_batch_samples | patching.py:501 |       0 | 0.30 s     | \u001b[94m0.51 %\u001b[0m           |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 2. forward           | patching.py:597 |       0 | 35.17 s    | \u001b[93m59.83 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 3. sync gradients    | patching.py:601 |       0 | 16.21 s    | \u001b[92m27.57 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 4. optimizer step    | patching.py:666 |       0 | 7.11 s     | \u001b[94m12.09 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "\n",
      "Total time elapsed: 58.78 seconds.\u001b[0m\n",
      "{'loss': 0.599, 'grad_norm': 0.4019942283630371, 'learning_rate': 5.1000000000000006e-05, 'epoch': 0.41, 'num_input_tokens_seen': 52377, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7149, 'grad_norm': 0.4019942283630371, 'learning_rate': 5.1000000000000006e-05, 'epoch': 0.41, 'num_input_tokens_seen': 29472, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8798, 'grad_norm': 0.4019942283630371, 'learning_rate': 5.1000000000000006e-05, 'epoch': 0.41, 'num_input_tokens_seen': 46488, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.8868, 'grad_norm': 0.4019942283630371, 'learning_rate': 5.1000000000000006e-05, 'epoch': 0.41, 'num_input_tokens_seen': 59600, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9859, 'grad_norm': 0.4019942283630371, 'learning_rate': 5.1000000000000006e-05, 'epoch': 0.41, 'num_input_tokens_seen': 87709, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5838, 'grad_norm': 0.4019942283630371, 'learning_rate': 5.1000000000000006e-05, 'epoch': 0.41, 'num_input_tokens_seen': 72458, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1904, 'grad_norm': 0.48395100235939026, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.42, 'num_input_tokens_seen': 115901, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9453, 'grad_norm': 0.48395100235939026, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.42, 'num_input_tokens_seen': 30227, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9621, 'grad_norm': 0.48395100235939026, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.42, 'num_input_tokens_seen': 53440, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6212, 'grad_norm': 0.48395100235939026, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.42, 'num_input_tokens_seen': 40847, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2128, 'grad_norm': 0.48395100235939026, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.42, 'num_input_tokens_seen': 88969, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7455, 'grad_norm': 0.48395100235939026, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.42, 'num_input_tokens_seen': 60682, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8312, 'grad_norm': 0.48395100235939026, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.42, 'num_input_tokens_seen': 73546, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9703, 'grad_norm': 0.48395100235939026, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.42, 'num_input_tokens_seen': 47336, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8051, 'grad_norm': 0.4364422857761383, 'learning_rate': 5.300000000000001e-05, 'epoch': 0.42, 'num_input_tokens_seen': 54425, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.0718, 'grad_norm': 0.4364422857761383, 'learning_rate': 5.300000000000001e-05, 'epoch': 0.42, 'num_input_tokens_seen': 117719, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.1132, 'grad_norm': 0.4364422857761383, 'learning_rate': 5.300000000000001e-05, 'epoch': 0.42, 'num_input_tokens_seen': 90367, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.8561, 'grad_norm': 0.4364422857761383, 'learning_rate': 5.300000000000001e-05, 'epoch': 0.42, 'num_input_tokens_seen': 61675, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0979, 'grad_norm': 0.4364422857761383, 'learning_rate': 5.300000000000001e-05, 'epoch': 0.42, 'num_input_tokens_seen': 74568, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5414, 'grad_norm': 0.4364422857761383, 'learning_rate': 5.300000000000001e-05, 'epoch': 0.42, 'num_input_tokens_seen': 48172, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7051, 'grad_norm': 0.4364422857761383, 'learning_rate': 5.300000000000001e-05, 'epoch': 0.42, 'num_input_tokens_seen': 30800, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.861, 'grad_norm': 0.4364422857761383, 'learning_rate': 5.300000000000001e-05, 'epoch': 0.42, 'num_input_tokens_seen': 41474, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2909, 'grad_norm': 0.4068441689014435, 'learning_rate': 5.4000000000000005e-05, 'epoch': 0.43, 'num_input_tokens_seen': 49439, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.7913, 'grad_norm': 0.4068441689014435, 'learning_rate': 5.4000000000000005e-05, 'epoch': 0.43, 'num_input_tokens_seen': 119530, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2613, 'grad_norm': 0.4068441689014435, 'learning_rate': 5.4000000000000005e-05, 'epoch': 0.43, 'num_input_tokens_seen': 91899, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8076, 'grad_norm': 0.4068441689014435, 'learning_rate': 5.4000000000000005e-05, 'epoch': 0.43, 'num_input_tokens_seen': 76033, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1229, 'grad_norm': 0.4068441689014435, 'learning_rate': 5.4000000000000005e-05, 'epoch': 0.43, 'num_input_tokens_seen': 63069, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1544, 'grad_norm': 0.4068441689014435, 'learning_rate': 5.4000000000000005e-05, 'epoch': 0.43, 'num_input_tokens_seen': 31610, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.4356, 'grad_norm': 0.4068441689014435, 'learning_rate': 5.4000000000000005e-05, 'epoch': 0.43, 'num_input_tokens_seen': 42312, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.5103, 'grad_norm': 0.4068441689014435, 'learning_rate': 5.4000000000000005e-05, 'epoch': 0.43, 'num_input_tokens_seen': 55729, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3672, 'grad_norm': 0.4620124101638794, 'learning_rate': 5.500000000000001e-05, 'epoch': 0.44, 'num_input_tokens_seen': 77179, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6971, 'grad_norm': 0.4620124101638794, 'learning_rate': 5.500000000000001e-05, 'epoch': 0.44, 'num_input_tokens_seen': 50400, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.2536, 'grad_norm': 0.4620124101638794, 'learning_rate': 5.500000000000001e-05, 'epoch': 0.44, 'num_input_tokens_seen': 43237, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6959, 'grad_norm': 0.4620124101638794, 'learning_rate': 5.500000000000001e-05, 'epoch': 0.44, 'num_input_tokens_seen': 121773, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1018, 'grad_norm': 0.4620124101638794, 'learning_rate': 5.500000000000001e-05, 'epoch': 0.44, 'num_input_tokens_seen': 56772, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2768, 'grad_norm': 0.4620124101638794, 'learning_rate': 5.500000000000001e-05, 'epoch': 0.44, 'num_input_tokens_seen': 32376, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5722, 'grad_norm': 0.4620124101638794, 'learning_rate': 5.500000000000001e-05, 'epoch': 0.44, 'num_input_tokens_seen': 93098, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.5453, 'grad_norm': 0.4620124101638794, 'learning_rate': 5.500000000000001e-05, 'epoch': 0.44, 'num_input_tokens_seen': 64130, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7024, 'grad_norm': 0.46401306986808777, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.45, 'num_input_tokens_seen': 124669, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5393, 'grad_norm': 0.46401306986808777, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.45, 'num_input_tokens_seen': 51264, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6777, 'grad_norm': 0.46401306986808777, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.45, 'num_input_tokens_seen': 79312, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1912, 'grad_norm': 0.46401306986808777, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.45, 'num_input_tokens_seen': 57674, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8639, 'grad_norm': 0.46401306986808777, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.45, 'num_input_tokens_seen': 32970, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8124, 'grad_norm': 0.46401306986808777, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.45, 'num_input_tokens_seen': 43912, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0163, 'grad_norm': 0.46401306986808777, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.45, 'num_input_tokens_seen': 65666, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3288, 'grad_norm': 0.46401306986808777, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.45, 'num_input_tokens_seen': 95376, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9264, 'grad_norm': 0.5287972092628479, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.46, 'num_input_tokens_seen': 126347, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.4838, 'grad_norm': 0.5287972092628479, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.46, 'num_input_tokens_seen': 44493, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8796, 'grad_norm': 0.5287972092628479, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.46, 'num_input_tokens_seen': 58516, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7487, 'grad_norm': 0.5287972092628479, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.46, 'num_input_tokens_seen': 33464, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8334, 'grad_norm': 0.5287972092628479, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.46, 'num_input_tokens_seen': 80633, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 2.0945, 'grad_norm': 0.5287972092628479, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.46, 'num_input_tokens_seen': 52040, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0356, 'grad_norm': 0.5287972092628479, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.46, 'num_input_tokens_seen': 97009, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6664, 'grad_norm': 0.5287972092628479, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.46, 'num_input_tokens_seen': 66744, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6029, 'grad_norm': 0.6402620077133179, 'learning_rate': 5.8e-05, 'epoch': 0.46, 'num_input_tokens_seen': 127513, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.1734, 'grad_norm': 0.6402620077133179, 'learning_rate': 5.8e-05, 'epoch': 0.46, 'num_input_tokens_seen': 81727, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1836, 'grad_norm': 0.6402620077133179, 'learning_rate': 5.8e-05, 'epoch': 0.46, 'num_input_tokens_seen': 52856, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1139, 'grad_norm': 0.6402620077133179, 'learning_rate': 5.8e-05, 'epoch': 0.46, 'num_input_tokens_seen': 67835, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0777, 'grad_norm': 0.6402620077133179, 'learning_rate': 5.8e-05, 'epoch': 0.46, 'num_input_tokens_seen': 98146, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.6646, 'grad_norm': 0.6402620077133179, 'learning_rate': 5.8e-05, 'epoch': 0.46, 'num_input_tokens_seen': 45221, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3075, 'grad_norm': 0.6402620077133179, 'learning_rate': 5.8e-05, 'epoch': 0.46, 'num_input_tokens_seen': 34097, 'trained_token_ratio': 1.0}\n",
      "{'loss': 2.1468, 'grad_norm': 0.6402620077133179, 'learning_rate': 5.8e-05, 'epoch': 0.46, 'num_input_tokens_seen': 59380, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0536, 'grad_norm': 0.4441506564617157, 'learning_rate': 5.9e-05, 'epoch': 0.47, 'num_input_tokens_seen': 129245, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1024, 'grad_norm': 0.4441506564617157, 'learning_rate': 5.9e-05, 'epoch': 0.47, 'num_input_tokens_seen': 46117, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1322, 'grad_norm': 0.4441506564617157, 'learning_rate': 5.9e-05, 'epoch': 0.47, 'num_input_tokens_seen': 53766, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.3289, 'grad_norm': 0.4441506564617157, 'learning_rate': 5.9e-05, 'epoch': 0.47, 'num_input_tokens_seen': 99449, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2008, 'grad_norm': 0.4441506564617157, 'learning_rate': 5.9e-05, 'epoch': 0.47, 'num_input_tokens_seen': 68823, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7072, 'grad_norm': 0.4441506564617157, 'learning_rate': 5.9e-05, 'epoch': 0.47, 'num_input_tokens_seen': 34453, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6963, 'grad_norm': 0.4441506564617157, 'learning_rate': 5.9e-05, 'epoch': 0.47, 'num_input_tokens_seen': 60341, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6317, 'grad_norm': 0.4441506564617157, 'learning_rate': 5.9e-05, 'epoch': 0.47, 'num_input_tokens_seen': 82856, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8066, 'grad_norm': 0.4106844365596771, 'learning_rate': 6e-05, 'epoch': 0.48, 'num_input_tokens_seen': 133987, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.561, 'grad_norm': 0.4106844365596771, 'learning_rate': 6e-05, 'epoch': 0.48, 'num_input_tokens_seen': 101829, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.014, 'grad_norm': 0.4106844365596771, 'learning_rate': 6e-05, 'epoch': 0.48, 'num_input_tokens_seen': 70308, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5765, 'grad_norm': 0.4106844365596771, 'learning_rate': 6e-05, 'epoch': 0.48, 'num_input_tokens_seen': 61647, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7073, 'grad_norm': 0.4106844365596771, 'learning_rate': 6e-05, 'epoch': 0.48, 'num_input_tokens_seen': 54988, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0543, 'grad_norm': 0.4106844365596771, 'learning_rate': 6e-05, 'epoch': 0.48, 'num_input_tokens_seen': 84932, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1075, 'grad_norm': 0.4106844365596771, 'learning_rate': 6e-05, 'epoch': 0.48, 'num_input_tokens_seen': 47020, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4052, 'grad_norm': 0.4106844365596771, 'learning_rate': 6e-05, 'epoch': 0.48, 'num_input_tokens_seen': 35313, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8636, 'grad_norm': 0.605482816696167, 'learning_rate': 6.1e-05, 'epoch': 0.49, 'num_input_tokens_seen': 136391, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5271, 'grad_norm': 0.605482816696167, 'learning_rate': 6.1e-05, 'epoch': 0.49, 'num_input_tokens_seen': 36011, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.356, 'grad_norm': 0.605482816696167, 'learning_rate': 6.1e-05, 'epoch': 0.49, 'num_input_tokens_seen': 86052, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.471, 'grad_norm': 0.605482816696167, 'learning_rate': 6.1e-05, 'epoch': 0.49, 'num_input_tokens_seen': 47777, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.5616, 'grad_norm': 0.605482816696167, 'learning_rate': 6.1e-05, 'epoch': 0.49, 'num_input_tokens_seen': 62591, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8941, 'grad_norm': 0.605482816696167, 'learning_rate': 6.1e-05, 'epoch': 0.49, 'num_input_tokens_seen': 103074, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.3712, 'grad_norm': 0.605482816696167, 'learning_rate': 6.1e-05, 'epoch': 0.49, 'num_input_tokens_seen': 71268, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2568, 'grad_norm': 0.605482816696167, 'learning_rate': 6.1e-05, 'epoch': 0.49, 'num_input_tokens_seen': 55846, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.6695, 'grad_norm': 0.5120340585708618, 'learning_rate': 6.2e-05, 'epoch': 0.5, 'num_input_tokens_seen': 105344, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.5156, 'grad_norm': 0.5120340585708618, 'learning_rate': 6.2e-05, 'epoch': 0.5, 'num_input_tokens_seen': 63692, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8232, 'grad_norm': 0.5120340585708618, 'learning_rate': 6.2e-05, 'epoch': 0.5, 'num_input_tokens_seen': 73105, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7975, 'grad_norm': 0.5120340585708618, 'learning_rate': 6.2e-05, 'epoch': 0.5, 'num_input_tokens_seen': 138889, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.909, 'grad_norm': 0.5120340585708618, 'learning_rate': 6.2e-05, 'epoch': 0.5, 'num_input_tokens_seen': 36462, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6975, 'grad_norm': 0.5120340585708618, 'learning_rate': 6.2e-05, 'epoch': 0.5, 'num_input_tokens_seen': 56855, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.5114, 'grad_norm': 0.5120340585708618, 'learning_rate': 6.2e-05, 'epoch': 0.5, 'num_input_tokens_seen': 48237, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1624, 'grad_norm': 0.5120340585708618, 'learning_rate': 6.2e-05, 'epoch': 0.5, 'num_input_tokens_seen': 87913, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.5116, 'grad_norm': 0.4383357763290405, 'learning_rate': 6.3e-05, 'epoch': 0.5, 'num_input_tokens_seen': 142664, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3727, 'grad_norm': 0.4383357763290405, 'learning_rate': 6.3e-05, 'epoch': 0.5, 'num_input_tokens_seen': 89383, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.789, 'grad_norm': 0.4383357763290405, 'learning_rate': 6.3e-05, 'epoch': 0.5, 'num_input_tokens_seen': 107600, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7746, 'grad_norm': 0.4383357763290405, 'learning_rate': 6.3e-05, 'epoch': 0.5, 'num_input_tokens_seen': 57953, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8241, 'grad_norm': 0.4383357763290405, 'learning_rate': 6.3e-05, 'epoch': 0.5, 'num_input_tokens_seen': 64809, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9532, 'grad_norm': 0.4383357763290405, 'learning_rate': 6.3e-05, 'epoch': 0.5, 'num_input_tokens_seen': 74229, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7755, 'grad_norm': 0.4383357763290405, 'learning_rate': 6.3e-05, 'epoch': 0.5, 'num_input_tokens_seen': 36825, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8045, 'grad_norm': 0.4383357763290405, 'learning_rate': 6.3e-05, 'epoch': 0.5, 'num_input_tokens_seen': 48671, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9919, 'grad_norm': 0.42137420177459717, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.51, 'num_input_tokens_seen': 144431, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6027, 'grad_norm': 0.42137420177459717, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.51, 'num_input_tokens_seen': 49560, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9864, 'grad_norm': 0.42137420177459717, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.51, 'num_input_tokens_seen': 58877, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.507, 'grad_norm': 0.42137420177459717, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.51, 'num_input_tokens_seen': 66108, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.134, 'grad_norm': 0.42137420177459717, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.51, 'num_input_tokens_seen': 37616, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6032, 'grad_norm': 0.42137420177459717, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.51, 'num_input_tokens_seen': 90703, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1119, 'grad_norm': 0.42137420177459717, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.51, 'num_input_tokens_seen': 109046, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.8158, 'grad_norm': 0.42137420177459717, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.51, 'num_input_tokens_seen': 75530, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0333, 'grad_norm': 0.4269309341907501, 'learning_rate': 6.500000000000001e-05, 'epoch': 0.52, 'num_input_tokens_seen': 146277, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7741, 'grad_norm': 0.4269309341907501, 'learning_rate': 6.500000000000001e-05, 'epoch': 0.52, 'num_input_tokens_seen': 59715, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7048, 'grad_norm': 0.4269309341907501, 'learning_rate': 6.500000000000001e-05, 'epoch': 0.52, 'num_input_tokens_seen': 38210, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8241, 'grad_norm': 0.4269309341907501, 'learning_rate': 6.500000000000001e-05, 'epoch': 0.52, 'num_input_tokens_seen': 67138, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.5068, 'grad_norm': 0.4269309341907501, 'learning_rate': 6.500000000000001e-05, 'epoch': 0.52, 'num_input_tokens_seen': 50343, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1702, 'grad_norm': 0.4269309341907501, 'learning_rate': 6.500000000000001e-05, 'epoch': 0.52, 'num_input_tokens_seen': 110390, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0583, 'grad_norm': 0.4269309341907501, 'learning_rate': 6.500000000000001e-05, 'epoch': 0.52, 'num_input_tokens_seen': 92016, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.141, 'grad_norm': 0.4269309341907501, 'learning_rate': 6.500000000000001e-05, 'epoch': 0.52, 'num_input_tokens_seen': 76681, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1563, 'grad_norm': 0.4208557903766632, 'learning_rate': 6.6e-05, 'epoch': 0.53, 'num_input_tokens_seen': 148527, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.455, 'grad_norm': 0.4208557903766632, 'learning_rate': 6.6e-05, 'epoch': 0.53, 'num_input_tokens_seen': 38948, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0102, 'grad_norm': 0.4208557903766632, 'learning_rate': 6.6e-05, 'epoch': 0.53, 'num_input_tokens_seen': 51166, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1378, 'grad_norm': 0.4208557903766632, 'learning_rate': 6.6e-05, 'epoch': 0.53, 'num_input_tokens_seen': 68243, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0553, 'grad_norm': 0.4208557903766632, 'learning_rate': 6.6e-05, 'epoch': 0.53, 'num_input_tokens_seen': 60671, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9772, 'grad_norm': 0.4208557903766632, 'learning_rate': 6.6e-05, 'epoch': 0.53, 'num_input_tokens_seen': 93504, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9612, 'grad_norm': 0.4208557903766632, 'learning_rate': 6.6e-05, 'epoch': 0.53, 'num_input_tokens_seen': 112111, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.0669, 'grad_norm': 0.4208557903766632, 'learning_rate': 6.6e-05, 'epoch': 0.53, 'num_input_tokens_seen': 77957, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7849, 'grad_norm': 7.4257493019104, 'learning_rate': 6.7e-05, 'epoch': 0.54, 'num_input_tokens_seen': 61184, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6831, 'grad_norm': 7.4257493019104, 'learning_rate': 6.7e-05, 'epoch': 0.54, 'num_input_tokens_seen': 79115, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0133, 'grad_norm': 7.4257493019104, 'learning_rate': 6.7e-05, 'epoch': 0.54, 'num_input_tokens_seen': 113853, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8568, 'grad_norm': 7.4257493019104, 'learning_rate': 6.7e-05, 'epoch': 0.54, 'num_input_tokens_seen': 151830, 'trained_token_ratio': 1.0}\n",
      " 54%|██████████████████████▌                   | 67/125 [01:08<00:37,  1.55it/s]\u001b[1mINFO    \u001b[0m | \u001b[36mtrainer.py:2250 (train)\u001b[0m - \u001b[1m\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| Task                 | File:Line       |   Depth | Time (s)   | Percentage (%)   |\n",
      "+======================+=================+=========+============+==================+\n",
      "| 1. get_batch_samples | patching.py:501 |       0 | 0.38 s     | \u001b[94m0.55 %\u001b[0m           |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 2. forward           | patching.py:597 |       0 | 40.58 s    | \u001b[93m58.89 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 3. sync gradients    | patching.py:601 |       0 | 18.85 s    | \u001b[92m27.36 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 4. optimizer step    | patching.py:666 |       0 | 9.09 s     | \u001b[94m13.20 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "\n",
      "Total time elapsed: 68.91 seconds.\u001b[0m\n",
      "{'loss': 7.905, 'grad_norm': 7.4257493019104, 'learning_rate': 6.7e-05, 'epoch': 0.54, 'num_input_tokens_seen': 38992, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6806, 'grad_norm': 7.4257493019104, 'learning_rate': 6.7e-05, 'epoch': 0.54, 'num_input_tokens_seen': 51385, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2816, 'grad_norm': 7.4257493019104, 'learning_rate': 6.7e-05, 'epoch': 0.54, 'num_input_tokens_seen': 68914, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0463, 'grad_norm': 7.4257493019104, 'learning_rate': 6.7e-05, 'epoch': 0.54, 'num_input_tokens_seen': 95237, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9378, 'grad_norm': 0.5124738812446594, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.54, 'num_input_tokens_seen': 96708, 'trained_token_ratio': 1.0}\n",
      " 54%|██████████████████████▊                   | 68/125 [01:17<00:34,  1.66it/s]{'loss': 0.7464, 'grad_norm': 0.5124738812446594, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.54, 'num_input_tokens_seen': 39540, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.936, 'grad_norm': 0.5124738812446594, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.54, 'num_input_tokens_seen': 115434, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.058, 'grad_norm': 0.5124738812446594, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.54, 'num_input_tokens_seen': 80580, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8972, 'grad_norm': 0.5124738812446594, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.54, 'num_input_tokens_seen': 153455, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1767, 'grad_norm': 0.5124738812446594, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.54, 'num_input_tokens_seen': 52316, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8089, 'grad_norm': 0.5124738812446594, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.54, 'num_input_tokens_seen': 70332, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4885, 'grad_norm': 0.5124738812446594, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.54, 'num_input_tokens_seen': 62472, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.7952, 'grad_norm': 0.521369457244873, 'learning_rate': 6.9e-05, 'epoch': 0.55, 'num_input_tokens_seen': 154601, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0494, 'grad_norm': 0.521369457244873, 'learning_rate': 6.9e-05, 'epoch': 0.55, 'num_input_tokens_seen': 52862, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9349, 'grad_norm': 0.521369457244873, 'learning_rate': 6.9e-05, 'epoch': 0.55, 'num_input_tokens_seen': 39983, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7697, 'grad_norm': 0.521369457244873, 'learning_rate': 6.9e-05, 'epoch': 0.55, 'num_input_tokens_seen': 63182, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7836, 'grad_norm': 0.521369457244873, 'learning_rate': 6.9e-05, 'epoch': 0.55, 'num_input_tokens_seen': 81328, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6206, 'grad_norm': 0.521369457244873, 'learning_rate': 6.9e-05, 'epoch': 0.55, 'num_input_tokens_seen': 97493, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1071, 'grad_norm': 0.521369457244873, 'learning_rate': 6.9e-05, 'epoch': 0.55, 'num_input_tokens_seen': 116493, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7724, 'grad_norm': 0.521369457244873, 'learning_rate': 6.9e-05, 'epoch': 0.55, 'num_input_tokens_seen': 71067, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.8334, 'grad_norm': 0.4262486696243286, 'learning_rate': 7e-05, 'epoch': 0.56, 'num_input_tokens_seen': 64099, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9587, 'grad_norm': 0.4262486696243286, 'learning_rate': 7e-05, 'epoch': 0.56, 'num_input_tokens_seen': 82484, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7524, 'grad_norm': 0.4262486696243286, 'learning_rate': 7e-05, 'epoch': 0.56, 'num_input_tokens_seen': 99026, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4152, 'grad_norm': 0.4262486696243286, 'learning_rate': 7e-05, 'epoch': 0.56, 'num_input_tokens_seen': 72106, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6481, 'grad_norm': 0.4262486696243286, 'learning_rate': 7e-05, 'epoch': 0.56, 'num_input_tokens_seen': 157916, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.208, 'grad_norm': 0.4262486696243286, 'learning_rate': 7e-05, 'epoch': 0.56, 'num_input_tokens_seen': 118551, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0236, 'grad_norm': 0.4262486696243286, 'learning_rate': 7e-05, 'epoch': 0.56, 'num_input_tokens_seen': 40653, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5716, 'grad_norm': 0.4262486696243286, 'learning_rate': 7e-05, 'epoch': 0.56, 'num_input_tokens_seen': 53698, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6739, 'grad_norm': 0.43259039521217346, 'learning_rate': 7.1e-05, 'epoch': 0.57, 'num_input_tokens_seen': 73085, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1871, 'grad_norm': 0.43259039521217346, 'learning_rate': 7.1e-05, 'epoch': 0.57, 'num_input_tokens_seen': 100953, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9336, 'grad_norm': 0.43259039521217346, 'learning_rate': 7.1e-05, 'epoch': 0.57, 'num_input_tokens_seen': 160754, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0149, 'grad_norm': 0.43259039521217346, 'learning_rate': 7.1e-05, 'epoch': 0.57, 'num_input_tokens_seen': 65070, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.8459, 'grad_norm': 0.43259039521217346, 'learning_rate': 7.1e-05, 'epoch': 0.57, 'num_input_tokens_seen': 41429, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6493, 'grad_norm': 0.43259039521217346, 'learning_rate': 7.1e-05, 'epoch': 0.57, 'num_input_tokens_seen': 54591, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3672, 'grad_norm': 0.43259039521217346, 'learning_rate': 7.1e-05, 'epoch': 0.57, 'num_input_tokens_seen': 120878, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9821, 'grad_norm': 0.43259039521217346, 'learning_rate': 7.1e-05, 'epoch': 0.57, 'num_input_tokens_seen': 84326, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0143, 'grad_norm': 0.40028372406959534, 'learning_rate': 7.2e-05, 'epoch': 0.58, 'num_input_tokens_seen': 123226, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.8883, 'grad_norm': 0.40028372406959534, 'learning_rate': 7.2e-05, 'epoch': 0.58, 'num_input_tokens_seen': 165359, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.8315, 'grad_norm': 0.40028372406959534, 'learning_rate': 7.2e-05, 'epoch': 0.58, 'num_input_tokens_seen': 85644, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1597, 'grad_norm': 0.40028372406959534, 'learning_rate': 7.2e-05, 'epoch': 0.58, 'num_input_tokens_seen': 65894, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9039, 'grad_norm': 0.40028372406959534, 'learning_rate': 7.2e-05, 'epoch': 0.58, 'num_input_tokens_seen': 74395, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0538, 'grad_norm': 0.40028372406959534, 'learning_rate': 7.2e-05, 'epoch': 0.58, 'num_input_tokens_seen': 42058, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0159, 'grad_norm': 0.40028372406959534, 'learning_rate': 7.2e-05, 'epoch': 0.58, 'num_input_tokens_seen': 55365, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.9852, 'grad_norm': 0.40028372406959534, 'learning_rate': 7.2e-05, 'epoch': 0.58, 'num_input_tokens_seen': 102551, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.4313, 'grad_norm': 0.372734934091568, 'learning_rate': 7.3e-05, 'epoch': 0.58, 'num_input_tokens_seen': 125005, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4604, 'grad_norm': 0.372734934091568, 'learning_rate': 7.3e-05, 'epoch': 0.58, 'num_input_tokens_seen': 167636, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6647, 'grad_norm': 0.372734934091568, 'learning_rate': 7.3e-05, 'epoch': 0.58, 'num_input_tokens_seen': 56289, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.4816, 'grad_norm': 0.372734934091568, 'learning_rate': 7.3e-05, 'epoch': 0.58, 'num_input_tokens_seen': 75634, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0391, 'grad_norm': 0.372734934091568, 'learning_rate': 7.3e-05, 'epoch': 0.58, 'num_input_tokens_seen': 104047, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7215, 'grad_norm': 0.372734934091568, 'learning_rate': 7.3e-05, 'epoch': 0.58, 'num_input_tokens_seen': 67067, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1728, 'grad_norm': 0.372734934091568, 'learning_rate': 7.3e-05, 'epoch': 0.58, 'num_input_tokens_seen': 42886, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9509, 'grad_norm': 0.372734934091568, 'learning_rate': 7.3e-05, 'epoch': 0.58, 'num_input_tokens_seen': 86899, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2451, 'grad_norm': 0.44810134172439575, 'learning_rate': 7.4e-05, 'epoch': 0.59, 'num_input_tokens_seen': 169793, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0496, 'grad_norm': 0.44810134172439575, 'learning_rate': 7.4e-05, 'epoch': 0.59, 'num_input_tokens_seen': 67941, 'trained_token_ratio': 1.0}\n",
      " 59%|████████████████████████▊                 | 74/125 [01:25<00:36,  1.38it/s]{'loss': 1.3402, 'grad_norm': 0.44810134172439575, 'learning_rate': 7.4e-05, 'epoch': 0.59, 'num_input_tokens_seen': 76529, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9236, 'grad_norm': 0.44810134172439575, 'learning_rate': 7.4e-05, 'epoch': 0.59, 'num_input_tokens_seen': 126489, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.5106, 'grad_norm': 0.44810134172439575, 'learning_rate': 7.4e-05, 'epoch': 0.59, 'num_input_tokens_seen': 57093, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8514, 'grad_norm': 0.44810134172439575, 'learning_rate': 7.4e-05, 'epoch': 0.59, 'num_input_tokens_seen': 87992, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9372, 'grad_norm': 0.44810134172439575, 'learning_rate': 7.4e-05, 'epoch': 0.59, 'num_input_tokens_seen': 105162, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.5445, 'grad_norm': 0.44810134172439575, 'learning_rate': 7.4e-05, 'epoch': 0.59, 'num_input_tokens_seen': 43402, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7955, 'grad_norm': 0.40762531757354736, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.6, 'num_input_tokens_seen': 171757, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6949, 'grad_norm': 0.40762531757354736, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.6, 'num_input_tokens_seen': 89412, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2648, 'grad_norm': 0.40762531757354736, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.6, 'num_input_tokens_seen': 128029, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8519, 'grad_norm': 0.40762531757354736, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.6, 'num_input_tokens_seen': 69193, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6204, 'grad_norm': 0.40762531757354736, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.6, 'num_input_tokens_seen': 44028, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.986, 'grad_norm': 0.40762531757354736, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.6, 'num_input_tokens_seen': 106664, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6357, 'grad_norm': 0.40762531757354736, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.6, 'num_input_tokens_seen': 77906, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0264, 'grad_norm': 0.40762531757354736, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.6, 'num_input_tokens_seen': 58173, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.8968, 'grad_norm': 0.5430788397789001, 'learning_rate': 7.6e-05, 'epoch': 0.61, 'num_input_tokens_seen': 90661, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.9337, 'grad_norm': 0.5430788397789001, 'learning_rate': 7.6e-05, 'epoch': 0.61, 'num_input_tokens_seen': 129491, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.2663, 'grad_norm': 0.5430788397789001, 'learning_rate': 7.6e-05, 'epoch': 0.61, 'num_input_tokens_seen': 44243, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9825, 'grad_norm': 0.5430788397789001, 'learning_rate': 7.6e-05, 'epoch': 0.61, 'num_input_tokens_seen': 173687, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9008, 'grad_norm': 0.5430788397789001, 'learning_rate': 7.6e-05, 'epoch': 0.61, 'num_input_tokens_seen': 70346, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.5335, 'grad_norm': 0.5430788397789001, 'learning_rate': 7.6e-05, 'epoch': 0.61, 'num_input_tokens_seen': 58976, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2967, 'grad_norm': 0.5430788397789001, 'learning_rate': 7.6e-05, 'epoch': 0.61, 'num_input_tokens_seen': 79097, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7024, 'grad_norm': 0.5430788397789001, 'learning_rate': 7.6e-05, 'epoch': 0.61, 'num_input_tokens_seen': 108018, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3532, 'grad_norm': 0.3952338397502899, 'learning_rate': 7.7e-05, 'epoch': 0.62, 'num_input_tokens_seen': 59853, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.7444, 'grad_norm': 0.3952338397502899, 'learning_rate': 7.7e-05, 'epoch': 0.62, 'num_input_tokens_seen': 175601, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1912, 'grad_norm': 0.3952338397502899, 'learning_rate': 7.7e-05, 'epoch': 0.62, 'num_input_tokens_seen': 80195, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5667, 'grad_norm': 0.3952338397502899, 'learning_rate': 7.7e-05, 'epoch': 0.62, 'num_input_tokens_seen': 71242, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.861, 'grad_norm': 0.3952338397502899, 'learning_rate': 7.7e-05, 'epoch': 0.62, 'num_input_tokens_seen': 109803, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.4625, 'grad_norm': 0.3952338397502899, 'learning_rate': 7.7e-05, 'epoch': 0.62, 'num_input_tokens_seen': 44898, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8461, 'grad_norm': 0.3952338397502899, 'learning_rate': 7.7e-05, 'epoch': 0.62, 'num_input_tokens_seen': 131364, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1608, 'grad_norm': 0.3952338397502899, 'learning_rate': 7.7e-05, 'epoch': 0.62, 'num_input_tokens_seen': 91842, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2181, 'grad_norm': 0.43942657113075256, 'learning_rate': 7.800000000000001e-05, 'epoch': 0.62, 'num_input_tokens_seen': 81169, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7154, 'grad_norm': 0.43942657113075256, 'learning_rate': 7.800000000000001e-05, 'epoch': 0.62, 'num_input_tokens_seen': 177436, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.3913, 'grad_norm': 0.43942657113075256, 'learning_rate': 7.800000000000001e-05, 'epoch': 0.62, 'num_input_tokens_seen': 45371, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9199, 'grad_norm': 0.43942657113075256, 'learning_rate': 7.800000000000001e-05, 'epoch': 0.62, 'num_input_tokens_seen': 111048, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3277, 'grad_norm': 0.43942657113075256, 'learning_rate': 7.800000000000001e-05, 'epoch': 0.62, 'num_input_tokens_seen': 72103, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.535, 'grad_norm': 0.43942657113075256, 'learning_rate': 7.800000000000001e-05, 'epoch': 0.62, 'num_input_tokens_seen': 60645, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9706, 'grad_norm': 0.43942657113075256, 'learning_rate': 7.800000000000001e-05, 'epoch': 0.62, 'num_input_tokens_seen': 92904, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.0619, 'grad_norm': 0.43942657113075256, 'learning_rate': 7.800000000000001e-05, 'epoch': 0.62, 'num_input_tokens_seen': 132876, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4209, 'grad_norm': 0.4685569405555725, 'learning_rate': 7.900000000000001e-05, 'epoch': 0.63, 'num_input_tokens_seen': 72963, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8553, 'grad_norm': 0.4685569405555725, 'learning_rate': 7.900000000000001e-05, 'epoch': 0.63, 'num_input_tokens_seen': 181072, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1768, 'grad_norm': 0.4685569405555725, 'learning_rate': 7.900000000000001e-05, 'epoch': 0.63, 'num_input_tokens_seen': 61498, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.5485, 'grad_norm': 0.4685569405555725, 'learning_rate': 7.900000000000001e-05, 'epoch': 0.63, 'num_input_tokens_seen': 134177, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4911, 'grad_norm': 0.4685569405555725, 'learning_rate': 7.900000000000001e-05, 'epoch': 0.63, 'num_input_tokens_seen': 45985, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.523, 'grad_norm': 0.4685569405555725, 'learning_rate': 7.900000000000001e-05, 'epoch': 0.63, 'num_input_tokens_seen': 112287, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.5332, 'grad_norm': 0.4685569405555725, 'learning_rate': 7.900000000000001e-05, 'epoch': 0.63, 'num_input_tokens_seen': 82061, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2746, 'grad_norm': 0.4685569405555725, 'learning_rate': 7.900000000000001e-05, 'epoch': 0.63, 'num_input_tokens_seen': 94009, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.6979, 'grad_norm': 0.4866802394390106, 'learning_rate': 8e-05, 'epoch': 0.64, 'num_input_tokens_seen': 46414, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7611, 'grad_norm': 0.4866802394390106, 'learning_rate': 8e-05, 'epoch': 0.64, 'num_input_tokens_seen': 182892, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.69, 'grad_norm': 0.4866802394390106, 'learning_rate': 8e-05, 'epoch': 0.64, 'num_input_tokens_seen': 95148, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9104, 'grad_norm': 0.4866802394390106, 'learning_rate': 8e-05, 'epoch': 0.64, 'num_input_tokens_seen': 113442, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3879, 'grad_norm': 0.4866802394390106, 'learning_rate': 8e-05, 'epoch': 0.64, 'num_input_tokens_seen': 135569, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7275, 'grad_norm': 0.4866802394390106, 'learning_rate': 8e-05, 'epoch': 0.64, 'num_input_tokens_seen': 62129, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.9496, 'grad_norm': 0.4866802394390106, 'learning_rate': 8e-05, 'epoch': 0.64, 'num_input_tokens_seen': 73639, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.574, 'grad_norm': 0.4866802394390106, 'learning_rate': 8e-05, 'epoch': 0.64, 'num_input_tokens_seen': 83162, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6799, 'grad_norm': 0.4655865430831909, 'learning_rate': 8.1e-05, 'epoch': 0.65, 'num_input_tokens_seen': 185169, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7567, 'grad_norm': 0.4655865430831909, 'learning_rate': 8.1e-05, 'epoch': 0.65, 'num_input_tokens_seen': 62964, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6577, 'grad_norm': 0.4655865430831909, 'learning_rate': 8.1e-05, 'epoch': 0.65, 'num_input_tokens_seen': 47055, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6645, 'grad_norm': 0.4655865430831909, 'learning_rate': 8.1e-05, 'epoch': 0.65, 'num_input_tokens_seen': 84026, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.7941, 'grad_norm': 0.4655865430831909, 'learning_rate': 8.1e-05, 'epoch': 0.65, 'num_input_tokens_seen': 74475, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3721, 'grad_norm': 0.4655865430831909, 'learning_rate': 8.1e-05, 'epoch': 0.65, 'num_input_tokens_seen': 96016, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9949, 'grad_norm': 0.4655865430831909, 'learning_rate': 8.1e-05, 'epoch': 0.65, 'num_input_tokens_seen': 114530, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.0264, 'grad_norm': 0.4655865430831909, 'learning_rate': 8.1e-05, 'epoch': 0.65, 'num_input_tokens_seen': 136823, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8873, 'grad_norm': 0.38399842381477356, 'learning_rate': 8.2e-05, 'epoch': 0.66, 'num_input_tokens_seen': 138442, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0365, 'grad_norm': 0.38399842381477356, 'learning_rate': 8.2e-05, 'epoch': 0.66, 'num_input_tokens_seen': 64033, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4, 'grad_norm': 0.38399842381477356, 'learning_rate': 8.2e-05, 'epoch': 0.66, 'num_input_tokens_seen': 116005, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2336, 'grad_norm': 0.38399842381477356, 'learning_rate': 8.2e-05, 'epoch': 0.66, 'num_input_tokens_seen': 97389, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.9182, 'grad_norm': 0.38399842381477356, 'learning_rate': 8.2e-05, 'epoch': 0.66, 'num_input_tokens_seen': 85262, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0903, 'grad_norm': 0.38399842381477356, 'learning_rate': 8.2e-05, 'epoch': 0.66, 'num_input_tokens_seen': 186806, 'trained_token_ratio': 1.0}\n",
      " 66%|███████████████████████████▌              | 82/125 [01:18<00:25,  1.69it/s]\u001b[1mINFO    \u001b[0m | \u001b[36mtrainer.py:2250 (train)\u001b[0m - \u001b[1m\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| Task                 | File:Line       |   Depth | Time (s)   | Percentage (%)   |\n",
      "+======================+=================+=========+============+==================+\n",
      "| 1. get_batch_samples | patching.py:501 |       0 | 0.46 s     | \u001b[94m0.58 %\u001b[0m           |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 2. forward           | patching.py:597 |       0 | 45.58 s    | \u001b[93m57.70 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 3. sync gradients    | patching.py:601 |       0 | 22.27 s    | \u001b[92m28.19 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 4. optimizer step    | patching.py:666 |       0 | 10.68 s    | \u001b[94m13.52 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "\n",
      "Total time elapsed: 78.99 seconds.\u001b[0m\n",
      "{'loss': 0.4507, 'grad_norm': 0.38399842381477356, 'learning_rate': 8.2e-05, 'epoch': 0.66, 'num_input_tokens_seen': 47632, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6714, 'grad_norm': 0.38399842381477356, 'learning_rate': 8.2e-05, 'epoch': 0.66, 'num_input_tokens_seen': 75656, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8242, 'grad_norm': 0.3868148624897003, 'learning_rate': 8.3e-05, 'epoch': 0.66, 'num_input_tokens_seen': 76781, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2123, 'grad_norm': 0.3868148624897003, 'learning_rate': 8.3e-05, 'epoch': 0.66, 'num_input_tokens_seen': 189076, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.267, 'grad_norm': 0.3868148624897003, 'learning_rate': 8.3e-05, 'epoch': 0.66, 'num_input_tokens_seen': 48439, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6975, 'grad_norm': 0.3868148624897003, 'learning_rate': 8.3e-05, 'epoch': 0.66, 'num_input_tokens_seen': 86397, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0356, 'grad_norm': 0.3868148624897003, 'learning_rate': 8.3e-05, 'epoch': 0.66, 'num_input_tokens_seen': 139879, 'trained_token_ratio': 1.0}\n",
      "                                                                                {'loss': 1.7037, 'grad_norm': 0.3868148624897003, 'learning_rate': 8.3e-05, 'epoch': 0.66, 'num_input_tokens_seen': 98798, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.8944, 'grad_norm': 0.3868148624897003, 'learning_rate': 8.3e-05, 'epoch': 0.66, 'num_input_tokens_seen': 65065, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.8968, 'grad_norm': 0.3868148624897003, 'learning_rate': 8.3e-05, 'epoch': 0.66, 'num_input_tokens_seen': 117431, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0263, 'grad_norm': 0.40956854820251465, 'learning_rate': 8.4e-05, 'epoch': 0.67, 'num_input_tokens_seen': 87825, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.5569, 'grad_norm': 0.40956854820251465, 'learning_rate': 8.4e-05, 'epoch': 0.67, 'num_input_tokens_seen': 192132, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.0843, 'grad_norm': 0.40956854820251465, 'learning_rate': 8.4e-05, 'epoch': 0.67, 'num_input_tokens_seen': 100448, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3598, 'grad_norm': 0.40956854820251465, 'learning_rate': 8.4e-05, 'epoch': 0.67, 'num_input_tokens_seen': 119100, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6916, 'grad_norm': 0.40956854820251465, 'learning_rate': 8.4e-05, 'epoch': 0.67, 'num_input_tokens_seen': 142220, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0404, 'grad_norm': 0.40956854820251465, 'learning_rate': 8.4e-05, 'epoch': 0.67, 'num_input_tokens_seen': 49134, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7048, 'grad_norm': 0.40956854820251465, 'learning_rate': 8.4e-05, 'epoch': 0.67, 'num_input_tokens_seen': 77795, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6434, 'grad_norm': 0.40956854820251465, 'learning_rate': 8.4e-05, 'epoch': 0.67, 'num_input_tokens_seen': 65847, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7163, 'grad_norm': 0.48231860995292664, 'learning_rate': 8.5e-05, 'epoch': 0.68, 'num_input_tokens_seen': 66443, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7578, 'grad_norm': 0.48231860995292664, 'learning_rate': 8.5e-05, 'epoch': 0.68, 'num_input_tokens_seen': 194533, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9973, 'grad_norm': 0.48231860995292664, 'learning_rate': 8.5e-05, 'epoch': 0.68, 'num_input_tokens_seen': 78441, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1213, 'grad_norm': 0.48231860995292664, 'learning_rate': 8.5e-05, 'epoch': 0.68, 'num_input_tokens_seen': 88774, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0481, 'grad_norm': 0.48231860995292664, 'learning_rate': 8.5e-05, 'epoch': 0.68, 'num_input_tokens_seen': 49524, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4026, 'grad_norm': 0.48231860995292664, 'learning_rate': 8.5e-05, 'epoch': 0.68, 'num_input_tokens_seen': 143424, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6711, 'grad_norm': 0.48231860995292664, 'learning_rate': 8.5e-05, 'epoch': 0.68, 'num_input_tokens_seen': 120172, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.907, 'grad_norm': 0.48231860995292664, 'learning_rate': 8.5e-05, 'epoch': 0.68, 'num_input_tokens_seen': 101511, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3223, 'grad_norm': 56.14131164550781, 'learning_rate': 8.6e-05, 'epoch': 0.69, 'num_input_tokens_seen': 196893, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2313, 'grad_norm': 56.14131164550781, 'learning_rate': 8.6e-05, 'epoch': 0.69, 'num_input_tokens_seen': 79272, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6382, 'grad_norm': 56.14131164550781, 'learning_rate': 8.6e-05, 'epoch': 0.69, 'num_input_tokens_seen': 145667, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6765, 'grad_norm': 56.14131164550781, 'learning_rate': 8.6e-05, 'epoch': 0.69, 'num_input_tokens_seen': 102540, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6029, 'grad_norm': 56.14131164550781, 'learning_rate': 8.6e-05, 'epoch': 0.69, 'num_input_tokens_seen': 121221, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7237, 'grad_norm': 56.14131164550781, 'learning_rate': 8.6e-05, 'epoch': 0.69, 'num_input_tokens_seen': 67151, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7276, 'grad_norm': 56.14131164550781, 'learning_rate': 8.6e-05, 'epoch': 0.69, 'num_input_tokens_seen': 89801, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.5407, 'grad_norm': 56.14131164550781, 'learning_rate': 8.6e-05, 'epoch': 0.69, 'num_input_tokens_seen': 49998, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2556, 'grad_norm': 0.38574957847595215, 'learning_rate': 8.7e-05, 'epoch': 0.7, 'num_input_tokens_seen': 103815, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2411, 'grad_norm': 0.38574957847595215, 'learning_rate': 8.7e-05, 'epoch': 0.7, 'num_input_tokens_seen': 122551, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9766, 'grad_norm': 0.38574957847595215, 'learning_rate': 8.7e-05, 'epoch': 0.7, 'num_input_tokens_seen': 50773, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8719, 'grad_norm': 0.38574957847595215, 'learning_rate': 8.7e-05, 'epoch': 0.7, 'num_input_tokens_seen': 201075, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.2148, 'grad_norm': 0.38574957847595215, 'learning_rate': 8.7e-05, 'epoch': 0.7, 'num_input_tokens_seen': 91064, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8343, 'grad_norm': 0.38574957847595215, 'learning_rate': 8.7e-05, 'epoch': 0.7, 'num_input_tokens_seen': 148802, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.7861, 'grad_norm': 0.38574957847595215, 'learning_rate': 8.7e-05, 'epoch': 0.7, 'num_input_tokens_seen': 68128, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.81, 'grad_norm': 0.38574957847595215, 'learning_rate': 8.7e-05, 'epoch': 0.7, 'num_input_tokens_seen': 80299, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9471, 'grad_norm': 0.4655711054801941, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.7, 'num_input_tokens_seen': 202887, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0241, 'grad_norm': 0.4655711054801941, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.7, 'num_input_tokens_seen': 51023, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.4765, 'grad_norm': 0.4655711054801941, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.7, 'num_input_tokens_seen': 68762, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1188, 'grad_norm': 0.4655711054801941, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.7, 'num_input_tokens_seen': 81147, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8028, 'grad_norm': 0.4655711054801941, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.7, 'num_input_tokens_seen': 124190, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.1458, 'grad_norm': 0.4655711054801941, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.7, 'num_input_tokens_seen': 92205, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.7289, 'grad_norm': 0.4655711054801941, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.7, 'num_input_tokens_seen': 104971, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8481, 'grad_norm': 0.4655711054801941, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.7, 'num_input_tokens_seen': 150513, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.715, 'grad_norm': 0.47733643651008606, 'learning_rate': 8.900000000000001e-05, 'epoch': 0.71, 'num_input_tokens_seen': 82045, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0165, 'grad_norm': 0.47733643651008606, 'learning_rate': 8.900000000000001e-05, 'epoch': 0.71, 'num_input_tokens_seen': 204823, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6917, 'grad_norm': 0.47733643651008606, 'learning_rate': 8.900000000000001e-05, 'epoch': 0.71, 'num_input_tokens_seen': 51414, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.586, 'grad_norm': 0.47733643651008606, 'learning_rate': 8.900000000000001e-05, 'epoch': 0.71, 'num_input_tokens_seen': 69555, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9176, 'grad_norm': 0.47733643651008606, 'learning_rate': 8.900000000000001e-05, 'epoch': 0.71, 'num_input_tokens_seen': 93243, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.9785, 'grad_norm': 0.47733643651008606, 'learning_rate': 8.900000000000001e-05, 'epoch': 0.71, 'num_input_tokens_seen': 151961, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.423, 'grad_norm': 0.47733643651008606, 'learning_rate': 8.900000000000001e-05, 'epoch': 0.71, 'num_input_tokens_seen': 125597, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6234, 'grad_norm': 0.47733643651008606, 'learning_rate': 8.900000000000001e-05, 'epoch': 0.71, 'num_input_tokens_seen': 106074, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7535, 'grad_norm': 0.4578840136528015, 'learning_rate': 9e-05, 'epoch': 0.72, 'num_input_tokens_seen': 206164, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.5018, 'grad_norm': 0.4578840136528015, 'learning_rate': 9e-05, 'epoch': 0.72, 'num_input_tokens_seen': 93978, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.9371, 'grad_norm': 0.4578840136528015, 'learning_rate': 9e-05, 'epoch': 0.72, 'num_input_tokens_seen': 82708, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9435, 'grad_norm': 0.4578840136528015, 'learning_rate': 9e-05, 'epoch': 0.72, 'num_input_tokens_seen': 153019, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6235, 'grad_norm': 0.4578840136528015, 'learning_rate': 9e-05, 'epoch': 0.72, 'num_input_tokens_seen': 126514, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7505, 'grad_norm': 0.4578840136528015, 'learning_rate': 9e-05, 'epoch': 0.72, 'num_input_tokens_seen': 106886, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.0722, 'grad_norm': 0.4578840136528015, 'learning_rate': 9e-05, 'epoch': 0.72, 'num_input_tokens_seen': 70196, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7827, 'grad_norm': 0.4578840136528015, 'learning_rate': 9e-05, 'epoch': 0.72, 'num_input_tokens_seen': 51882, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9045, 'grad_norm': 0.35501018166542053, 'learning_rate': 9.1e-05, 'epoch': 0.73, 'num_input_tokens_seen': 155438, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3577, 'grad_norm': 0.35501018166542053, 'learning_rate': 9.1e-05, 'epoch': 0.73, 'num_input_tokens_seen': 95586, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2321, 'grad_norm': 0.35501018166542053, 'learning_rate': 9.1e-05, 'epoch': 0.73, 'num_input_tokens_seen': 209031, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9401, 'grad_norm': 0.35501018166542053, 'learning_rate': 9.1e-05, 'epoch': 0.73, 'num_input_tokens_seen': 84285, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.4849, 'grad_norm': 0.35501018166542053, 'learning_rate': 9.1e-05, 'epoch': 0.73, 'num_input_tokens_seen': 52581, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7573, 'grad_norm': 0.35501018166542053, 'learning_rate': 9.1e-05, 'epoch': 0.73, 'num_input_tokens_seen': 71044, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.897, 'grad_norm': 0.35501018166542053, 'learning_rate': 9.1e-05, 'epoch': 0.73, 'num_input_tokens_seen': 108667, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.4053, 'grad_norm': 0.35501018166542053, 'learning_rate': 9.1e-05, 'epoch': 0.73, 'num_input_tokens_seen': 128503, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9732, 'grad_norm': 0.4071270823478699, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.74, 'num_input_tokens_seen': 53395, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7372, 'grad_norm': 0.4071270823478699, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.74, 'num_input_tokens_seen': 210683, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1074, 'grad_norm': 0.4071270823478699, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.74, 'num_input_tokens_seen': 72103, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7362, 'grad_norm': 0.4071270823478699, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.74, 'num_input_tokens_seen': 129941, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5666, 'grad_norm': 0.4071270823478699, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.74, 'num_input_tokens_seen': 109972, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.624, 'grad_norm': 0.4071270823478699, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.74, 'num_input_tokens_seen': 85351, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.5152, 'grad_norm': 0.4071270823478699, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.74, 'num_input_tokens_seen': 157044, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8445, 'grad_norm': 0.4071270823478699, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.74, 'num_input_tokens_seen': 96693, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.4634, 'grad_norm': 0.4570745825767517, 'learning_rate': 9.300000000000001e-05, 'epoch': 0.74, 'num_input_tokens_seen': 86253, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3613, 'grad_norm': 0.4570745825767517, 'learning_rate': 9.300000000000001e-05, 'epoch': 0.74, 'num_input_tokens_seen': 212951, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.1114, 'grad_norm': 0.4570745825767517, 'learning_rate': 9.300000000000001e-05, 'epoch': 0.74, 'num_input_tokens_seen': 111608, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1745, 'grad_norm': 0.4570745825767517, 'learning_rate': 9.300000000000001e-05, 'epoch': 0.74, 'num_input_tokens_seen': 131601, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5537, 'grad_norm': 0.4570745825767517, 'learning_rate': 9.300000000000001e-05, 'epoch': 0.74, 'num_input_tokens_seen': 97953, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.833, 'grad_norm': 0.4570745825767517, 'learning_rate': 9.300000000000001e-05, 'epoch': 0.74, 'num_input_tokens_seen': 54123, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0512, 'grad_norm': 0.4570745825767517, 'learning_rate': 9.300000000000001e-05, 'epoch': 0.74, 'num_input_tokens_seen': 158850, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0121, 'grad_norm': 0.4570745825767517, 'learning_rate': 9.300000000000001e-05, 'epoch': 0.74, 'num_input_tokens_seen': 72899, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6913, 'grad_norm': 0.41706496477127075, 'learning_rate': 9.4e-05, 'epoch': 0.75, 'num_input_tokens_seen': 54710, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.7483, 'grad_norm': 0.41706496477127075, 'learning_rate': 9.4e-05, 'epoch': 0.75, 'num_input_tokens_seen': 215210, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8508, 'grad_norm': 0.41706496477127075, 'learning_rate': 9.4e-05, 'epoch': 0.75, 'num_input_tokens_seen': 112835, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0136, 'grad_norm': 0.41706496477127075, 'learning_rate': 9.4e-05, 'epoch': 0.75, 'num_input_tokens_seen': 132919, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8875, 'grad_norm': 0.41706496477127075, 'learning_rate': 9.4e-05, 'epoch': 0.75, 'num_input_tokens_seen': 99162, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8178, 'grad_norm': 0.41706496477127075, 'learning_rate': 9.4e-05, 'epoch': 0.75, 'num_input_tokens_seen': 87275, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.5531, 'grad_norm': 0.41706496477127075, 'learning_rate': 9.4e-05, 'epoch': 0.75, 'num_input_tokens_seen': 160791, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7524, 'grad_norm': 0.41706496477127075, 'learning_rate': 9.4e-05, 'epoch': 0.75, 'num_input_tokens_seen': 73842, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7258, 'grad_norm': 0.4924200475215912, 'learning_rate': 9.5e-05, 'epoch': 0.76, 'num_input_tokens_seen': 216981, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2783, 'grad_norm': 0.4924200475215912, 'learning_rate': 9.5e-05, 'epoch': 0.76, 'num_input_tokens_seen': 88177, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.537, 'grad_norm': 0.4924200475215912, 'learning_rate': 9.5e-05, 'epoch': 0.76, 'num_input_tokens_seen': 55144, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.7541, 'grad_norm': 0.4924200475215912, 'learning_rate': 9.5e-05, 'epoch': 0.76, 'num_input_tokens_seen': 74381, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.342, 'grad_norm': 0.4924200475215912, 'learning_rate': 9.5e-05, 'epoch': 0.76, 'num_input_tokens_seen': 100108, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.5028, 'grad_norm': 0.4924200475215912, 'learning_rate': 9.5e-05, 'epoch': 0.76, 'num_input_tokens_seen': 134013, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.688, 'grad_norm': 0.4924200475215912, 'learning_rate': 9.5e-05, 'epoch': 0.76, 'num_input_tokens_seen': 113903, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.4849, 'grad_norm': 0.4924200475215912, 'learning_rate': 9.5e-05, 'epoch': 0.76, 'num_input_tokens_seen': 162033, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.311, 'grad_norm': 0.38773515820503235, 'learning_rate': 9.6e-05, 'epoch': 0.77, 'num_input_tokens_seen': 219537, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2963, 'grad_norm': 0.38773515820503235, 'learning_rate': 9.6e-05, 'epoch': 0.77, 'num_input_tokens_seen': 89152, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9698, 'grad_norm': 0.38773515820503235, 'learning_rate': 9.6e-05, 'epoch': 0.77, 'num_input_tokens_seen': 135922, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7929, 'grad_norm': 0.38773515820503235, 'learning_rate': 9.6e-05, 'epoch': 0.77, 'num_input_tokens_seen': 163952, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3854, 'grad_norm': 0.38773515820503235, 'learning_rate': 9.6e-05, 'epoch': 0.77, 'num_input_tokens_seen': 115386, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.7361, 'grad_norm': 0.38773515820503235, 'learning_rate': 9.6e-05, 'epoch': 0.77, 'num_input_tokens_seen': 55983, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0204, 'grad_norm': 0.38773515820503235, 'learning_rate': 9.6e-05, 'epoch': 0.77, 'num_input_tokens_seen': 75290, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.9688, 'grad_norm': 0.38773515820503235, 'learning_rate': 9.6e-05, 'epoch': 0.77, 'num_input_tokens_seen': 101242, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6265, 'grad_norm': 0.5438706874847412, 'learning_rate': 9.7e-05, 'epoch': 0.78, 'num_input_tokens_seen': 75859, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.908, 'grad_norm': 0.5438706874847412, 'learning_rate': 9.7e-05, 'epoch': 0.78, 'num_input_tokens_seen': 221555, 'trained_token_ratio': 1.0}\n",
      " 78%|████████████████████████████████▌         | 97/125 [01:29<00:20,  1.37it/s]\u001b[1mINFO    \u001b[0m | \u001b[36mtrainer.py:2250 (train)\u001b[0m - \u001b[1m\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| Task                 | File:Line       |   Depth | Time (s)   | Percentage (%)   |\n",
      "+======================+=================+=========+============+==================+\n",
      "| 1. get_batch_samples | patching.py:501 |       0 | 0.54 s     | \u001b[94m0.60 %\u001b[0m           |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 2. forward           | patching.py:597 |       0 | 50.29 s    | \u001b[93m56.15 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 3. sync gradients    | patching.py:601 |       0 | 26.40 s    | \u001b[92m29.47 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 4. optimizer step    | patching.py:666 |       0 | 12.34 s    | \u001b[94m13.77 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "\n",
      "Total time elapsed: 89.56 seconds.\u001b[0m\n",
      "{'loss': 1.6224, 'grad_norm': 0.5438706874847412, 'learning_rate': 9.7e-05, 'epoch': 0.78, 'num_input_tokens_seen': 101910, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5628, 'grad_norm': 0.5438706874847412, 'learning_rate': 9.7e-05, 'epoch': 0.78, 'num_input_tokens_seen': 89723, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4857, 'grad_norm': 0.5438706874847412, 'learning_rate': 9.7e-05, 'epoch': 0.78, 'num_input_tokens_seen': 136808, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.3459, 'grad_norm': 0.5438706874847412, 'learning_rate': 9.7e-05, 'epoch': 0.78, 'num_input_tokens_seen': 56479, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7691, 'grad_norm': 0.5438706874847412, 'learning_rate': 9.7e-05, 'epoch': 0.78, 'num_input_tokens_seen': 116199, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1967, 'grad_norm': 0.5438706874847412, 'learning_rate': 9.7e-05, 'epoch': 0.78, 'num_input_tokens_seen': 165021, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7072, 'grad_norm': 0.4975195825099945, 'learning_rate': 9.8e-05, 'epoch': 0.78, 'num_input_tokens_seen': 138194, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.4331, 'grad_norm': 0.4975195825099945, 'learning_rate': 9.8e-05, 'epoch': 0.78, 'num_input_tokens_seen': 230857, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8876, 'grad_norm': 0.4975195825099945, 'learning_rate': 9.8e-05, 'epoch': 0.78, 'num_input_tokens_seen': 76636, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8151, 'grad_norm': 0.4975195825099945, 'learning_rate': 9.8e-05, 'epoch': 0.78, 'num_input_tokens_seen': 90644, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0333, 'grad_norm': 0.4975195825099945, 'learning_rate': 9.8e-05, 'epoch': 0.78, 'num_input_tokens_seen': 117478, 'trained_token_ratio': 1.0}\n",
      " 78%|████████████████████████████████▉         | 98/125 [01:40<00:47,  1.75s/it]{'loss': 1.4326, 'grad_norm': 0.4975195825099945, 'learning_rate': 9.8e-05, 'epoch': 0.78, 'num_input_tokens_seen': 166416, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.88, 'grad_norm': 0.4975195825099945, 'learning_rate': 9.8e-05, 'epoch': 0.78, 'num_input_tokens_seen': 103161, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.104, 'grad_norm': 0.4975195825099945, 'learning_rate': 9.8e-05, 'epoch': 0.78, 'num_input_tokens_seen': 57147, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.8424, 'grad_norm': 0.48924416303634644, 'learning_rate': 9.900000000000001e-05, 'epoch': 0.79, 'num_input_tokens_seen': 91473, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.6355, 'grad_norm': 0.48924416303634644, 'learning_rate': 9.900000000000001e-05, 'epoch': 0.79, 'num_input_tokens_seen': 233390, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8201, 'grad_norm': 0.48924416303634644, 'learning_rate': 9.900000000000001e-05, 'epoch': 0.79, 'num_input_tokens_seen': 77367, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1664, 'grad_norm': 0.48924416303634644, 'learning_rate': 9.900000000000001e-05, 'epoch': 0.79, 'num_input_tokens_seen': 139774, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0116, 'grad_norm': 0.48924416303634644, 'learning_rate': 9.900000000000001e-05, 'epoch': 0.79, 'num_input_tokens_seen': 104025, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.8718, 'grad_norm': 0.48924416303634644, 'learning_rate': 9.900000000000001e-05, 'epoch': 0.79, 'num_input_tokens_seen': 168204, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.4251, 'grad_norm': 0.48924416303634644, 'learning_rate': 9.900000000000001e-05, 'epoch': 0.79, 'num_input_tokens_seen': 118981, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1432, 'grad_norm': 0.48924416303634644, 'learning_rate': 9.900000000000001e-05, 'epoch': 0.79, 'num_input_tokens_seen': 57473, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8341, 'grad_norm': 0.45558589696884155, 'learning_rate': 0.0001, 'epoch': 0.8, 'num_input_tokens_seen': 92460, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.5375, 'grad_norm': 0.45558589696884155, 'learning_rate': 0.0001, 'epoch': 0.8, 'num_input_tokens_seen': 57866, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9649, 'grad_norm': 0.45558589696884155, 'learning_rate': 0.0001, 'epoch': 0.8, 'num_input_tokens_seen': 236392, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0745, 'grad_norm': 0.45558589696884155, 'learning_rate': 0.0001, 'epoch': 0.8, 'num_input_tokens_seen': 169783, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.884, 'grad_norm': 0.45558589696884155, 'learning_rate': 0.0001, 'epoch': 0.8, 'num_input_tokens_seen': 120040, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7617, 'grad_norm': 0.45558589696884155, 'learning_rate': 0.0001, 'epoch': 0.8, 'num_input_tokens_seen': 77783, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6857, 'grad_norm': 0.45558589696884155, 'learning_rate': 0.0001, 'epoch': 0.8, 'num_input_tokens_seen': 105082, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8266, 'grad_norm': 0.45558589696884155, 'learning_rate': 0.0001, 'epoch': 0.8, 'num_input_tokens_seen': 141009, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6381, 'grad_norm': 0.4401923716068268, 'learning_rate': 9.6e-05, 'epoch': 0.81, 'num_input_tokens_seen': 78647, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7365, 'grad_norm': 0.4401923716068268, 'learning_rate': 9.6e-05, 'epoch': 0.81, 'num_input_tokens_seen': 240229, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9744, 'grad_norm': 0.4401923716068268, 'learning_rate': 9.6e-05, 'epoch': 0.81, 'num_input_tokens_seen': 171549, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.856, 'grad_norm': 0.4401923716068268, 'learning_rate': 9.6e-05, 'epoch': 0.81, 'num_input_tokens_seen': 121278, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.8712, 'grad_norm': 0.4401923716068268, 'learning_rate': 9.6e-05, 'epoch': 0.81, 'num_input_tokens_seen': 93482, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.103, 'grad_norm': 0.4401923716068268, 'learning_rate': 9.6e-05, 'epoch': 0.81, 'num_input_tokens_seen': 106261, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.0951, 'grad_norm': 0.4401923716068268, 'learning_rate': 9.6e-05, 'epoch': 0.81, 'num_input_tokens_seen': 142378, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2251, 'grad_norm': 0.4401923716068268, 'learning_rate': 9.6e-05, 'epoch': 0.81, 'num_input_tokens_seen': 58589, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6785, 'grad_norm': 0.5386640429496765, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.82, 'num_input_tokens_seen': 122268, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3079, 'grad_norm': 0.5386640429496765, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.82, 'num_input_tokens_seen': 241515, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0268, 'grad_norm': 0.5386640429496765, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.82, 'num_input_tokens_seen': 107103, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.5792, 'grad_norm': 0.5386640429496765, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.82, 'num_input_tokens_seen': 59345, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1589, 'grad_norm': 0.5386640429496765, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.82, 'num_input_tokens_seen': 143434, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6116, 'grad_norm': 0.5386640429496765, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.82, 'num_input_tokens_seen': 94317, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3571, 'grad_norm': 0.5386640429496765, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.82, 'num_input_tokens_seen': 79472, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0986, 'grad_norm': 0.5386640429496765, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.82, 'num_input_tokens_seen': 172739, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6559, 'grad_norm': 0.39838850498199463, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.82, 'num_input_tokens_seen': 59809, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7544, 'grad_norm': 0.39838850498199463, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.82, 'num_input_tokens_seen': 123883, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1272, 'grad_norm': 0.39838850498199463, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.82, 'num_input_tokens_seen': 174634, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.4305, 'grad_norm': 0.39838850498199463, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.82, 'num_input_tokens_seen': 108359, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0306, 'grad_norm': 0.39838850498199463, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.82, 'num_input_tokens_seen': 243546, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0362, 'grad_norm': 0.39838850498199463, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.82, 'num_input_tokens_seen': 80279, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9902, 'grad_norm': 0.39838850498199463, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.82, 'num_input_tokens_seen': 145062, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0381, 'grad_norm': 0.39838850498199463, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.82, 'num_input_tokens_seen': 95219, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6985, 'grad_norm': 0.6239455938339233, 'learning_rate': 8.4e-05, 'epoch': 0.83, 'num_input_tokens_seen': 146511, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7706, 'grad_norm': 0.6239455938339233, 'learning_rate': 8.4e-05, 'epoch': 0.83, 'num_input_tokens_seen': 245636, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.6865, 'grad_norm': 0.6239455938339233, 'learning_rate': 8.4e-05, 'epoch': 0.83, 'num_input_tokens_seen': 60351, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.2766, 'grad_norm': 0.6239455938339233, 'learning_rate': 8.4e-05, 'epoch': 0.83, 'num_input_tokens_seen': 176169, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3517, 'grad_norm': 0.6239455938339233, 'learning_rate': 8.4e-05, 'epoch': 0.83, 'num_input_tokens_seen': 80950, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4421, 'grad_norm': 0.6239455938339233, 'learning_rate': 8.4e-05, 'epoch': 0.83, 'num_input_tokens_seen': 125239, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.103, 'grad_norm': 0.6239455938339233, 'learning_rate': 8.4e-05, 'epoch': 0.83, 'num_input_tokens_seen': 109488, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5932, 'grad_norm': 0.6239455938339233, 'learning_rate': 8.4e-05, 'epoch': 0.83, 'num_input_tokens_seen': 95964, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4209, 'grad_norm': 0.4303472340106964, 'learning_rate': 8e-05, 'epoch': 0.84, 'num_input_tokens_seen': 247589, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.609, 'grad_norm': 0.4303472340106964, 'learning_rate': 8e-05, 'epoch': 0.84, 'num_input_tokens_seen': 110445, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8972, 'grad_norm': 0.4303472340106964, 'learning_rate': 8e-05, 'epoch': 0.84, 'num_input_tokens_seen': 126474, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.0795, 'grad_norm': 0.4303472340106964, 'learning_rate': 8e-05, 'epoch': 0.84, 'num_input_tokens_seen': 177802, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.992, 'grad_norm': 0.4303472340106964, 'learning_rate': 8e-05, 'epoch': 0.84, 'num_input_tokens_seen': 147773, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.4963, 'grad_norm': 0.4303472340106964, 'learning_rate': 8e-05, 'epoch': 0.84, 'num_input_tokens_seen': 60916, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.745, 'grad_norm': 0.4303472340106964, 'learning_rate': 8e-05, 'epoch': 0.84, 'num_input_tokens_seen': 81672, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.3713, 'grad_norm': 0.4303472340106964, 'learning_rate': 8e-05, 'epoch': 0.84, 'num_input_tokens_seen': 96690, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.2851, 'grad_norm': 0.46802279353141785, 'learning_rate': 7.6e-05, 'epoch': 0.85, 'num_input_tokens_seen': 249287, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2328, 'grad_norm': 0.46802279353141785, 'learning_rate': 7.6e-05, 'epoch': 0.85, 'num_input_tokens_seen': 82305, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.6512, 'grad_norm': 0.46802279353141785, 'learning_rate': 7.6e-05, 'epoch': 0.85, 'num_input_tokens_seen': 61357, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7244, 'grad_norm': 0.46802279353141785, 'learning_rate': 7.6e-05, 'epoch': 0.85, 'num_input_tokens_seen': 148837, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9681, 'grad_norm': 0.46802279353141785, 'learning_rate': 7.6e-05, 'epoch': 0.85, 'num_input_tokens_seen': 111214, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.5776, 'grad_norm': 0.46802279353141785, 'learning_rate': 7.6e-05, 'epoch': 0.85, 'num_input_tokens_seen': 179051, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3907, 'grad_norm': 0.46802279353141785, 'learning_rate': 7.6e-05, 'epoch': 0.85, 'num_input_tokens_seen': 127279, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6458, 'grad_norm': 0.46802279353141785, 'learning_rate': 7.6e-05, 'epoch': 0.85, 'num_input_tokens_seen': 97362, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9641, 'grad_norm': 0.3868485689163208, 'learning_rate': 7.2e-05, 'epoch': 0.86, 'num_input_tokens_seen': 252376, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3772, 'grad_norm': 0.3868485689163208, 'learning_rate': 7.2e-05, 'epoch': 0.86, 'num_input_tokens_seen': 83183, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6531, 'grad_norm': 0.3868485689163208, 'learning_rate': 7.2e-05, 'epoch': 0.86, 'num_input_tokens_seen': 112975, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2442, 'grad_norm': 0.3868485689163208, 'learning_rate': 7.2e-05, 'epoch': 0.86, 'num_input_tokens_seen': 181708, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9051, 'grad_norm': 0.3868485689163208, 'learning_rate': 7.2e-05, 'epoch': 0.86, 'num_input_tokens_seen': 98669, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.5105, 'grad_norm': 0.3868485689163208, 'learning_rate': 7.2e-05, 'epoch': 0.86, 'num_input_tokens_seen': 129372, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1619, 'grad_norm': 0.3868485689163208, 'learning_rate': 7.2e-05, 'epoch': 0.86, 'num_input_tokens_seen': 62023, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3672, 'grad_norm': 0.3868485689163208, 'learning_rate': 7.2e-05, 'epoch': 0.86, 'num_input_tokens_seen': 150970, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.715, 'grad_norm': 0.4244995713233948, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.86, 'num_input_tokens_seen': 152282, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0705, 'grad_norm': 0.4244995713233948, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.86, 'num_input_tokens_seen': 254932, 'trained_token_ratio': 1.0}\n",
      " 86%|███████████████████████████████████▍     | 108/125 [01:39<00:10,  1.59it/s]\u001b[1mINFO    \u001b[0m | \u001b[36mtrainer.py:2250 (train)\u001b[0m - \u001b[1m\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| Task                 | File:Line       |   Depth | Time (s)   | Percentage (%)   |\n",
      "+======================+=================+=========+============+==================+\n",
      "| 1. get_batch_samples | patching.py:501 |       0 | 0.60 s     | \u001b[94m0.60 %\u001b[0m           |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 2. forward           | patching.py:597 |       0 | 57.12 s    | \u001b[93m57.23 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 3. sync gradients    | patching.py:601 |       0 | 28.41 s    | \u001b[92m28.47 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 4. optimizer step    | patching.py:666 |       0 | 13.68 s    | \u001b[94m13.70 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "\n",
      "Total time elapsed: 99.81 seconds.\u001b[0m\n",
      "{'loss': 1.2754, 'grad_norm': 0.4244995713233948, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.86, 'num_input_tokens_seen': 114081, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7827, 'grad_norm': 0.4244995713233948, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.86, 'num_input_tokens_seen': 99431, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9886, 'grad_norm': 0.4244995713233948, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.86, 'num_input_tokens_seen': 183814, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9728, 'grad_norm': 0.4244995713233948, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.86, 'num_input_tokens_seen': 62547, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.4493, 'grad_norm': 0.4244995713233948, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.86, 'num_input_tokens_seen': 83940, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.386, 'grad_norm': 0.4244995713233948, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.86, 'num_input_tokens_seen': 130659, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9729, 'grad_norm': 0.458375483751297, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.87, 'num_input_tokens_seen': 256343, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7588, 'grad_norm': 0.458375483751297, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.87, 'num_input_tokens_seen': 185190, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2647, 'grad_norm': 0.458375483751297, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.87, 'num_input_tokens_seen': 100311, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.2319, 'grad_norm': 0.458375483751297, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.87, 'num_input_tokens_seen': 84634, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9306, 'grad_norm': 0.458375483751297, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.87, 'num_input_tokens_seen': 62992, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.3786, 'grad_norm': 0.458375483751297, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.87, 'num_input_tokens_seen': 114969, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1431, 'grad_norm': 0.458375483751297, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.87, 'num_input_tokens_seen': 131744, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.5191, 'grad_norm': 0.458375483751297, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.87, 'num_input_tokens_seen': 153534, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8674, 'grad_norm': 0.5215111970901489, 'learning_rate': 6e-05, 'epoch': 0.88, 'num_input_tokens_seen': 257665, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.6608, 'grad_norm': 0.5215111970901489, 'learning_rate': 6e-05, 'epoch': 0.88, 'num_input_tokens_seen': 186396, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.585, 'grad_norm': 0.5215111970901489, 'learning_rate': 6e-05, 'epoch': 0.88, 'num_input_tokens_seen': 63587, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4368, 'grad_norm': 0.5215111970901489, 'learning_rate': 6e-05, 'epoch': 0.88, 'num_input_tokens_seen': 115960, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.5699, 'grad_norm': 0.5215111970901489, 'learning_rate': 6e-05, 'epoch': 0.88, 'num_input_tokens_seen': 101224, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.172, 'grad_norm': 0.5215111970901489, 'learning_rate': 6e-05, 'epoch': 0.88, 'num_input_tokens_seen': 154593, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.7431, 'grad_norm': 0.5215111970901489, 'learning_rate': 6e-05, 'epoch': 0.88, 'num_input_tokens_seen': 132791, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4027, 'grad_norm': 0.5215111970901489, 'learning_rate': 6e-05, 'epoch': 0.88, 'num_input_tokens_seen': 85468, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8382, 'grad_norm': 0.49101805686950684, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.89, 'num_input_tokens_seen': 188550, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8701, 'grad_norm': 0.49101805686950684, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.89, 'num_input_tokens_seen': 116926, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.5597, 'grad_norm': 0.49101805686950684, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.89, 'num_input_tokens_seen': 262430, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6681, 'grad_norm': 0.49101805686950684, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.89, 'num_input_tokens_seen': 156492, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0609, 'grad_norm': 0.49101805686950684, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.89, 'num_input_tokens_seen': 134464, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9811, 'grad_norm': 0.49101805686950684, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.89, 'num_input_tokens_seen': 63805, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0586, 'grad_norm': 0.49101805686950684, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.89, 'num_input_tokens_seen': 102092, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7301, 'grad_norm': 0.49101805686950684, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.89, 'num_input_tokens_seen': 86090, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9105, 'grad_norm': 0.3725126087665558, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.9, 'num_input_tokens_seen': 158100, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8379, 'grad_norm': 0.3725126087665558, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.9, 'num_input_tokens_seen': 135989, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8451, 'grad_norm': 0.3725126087665558, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.9, 'num_input_tokens_seen': 266071, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6814, 'grad_norm': 0.3725126087665558, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.9, 'num_input_tokens_seen': 87298, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.7946, 'grad_norm': 0.3725126087665558, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.9, 'num_input_tokens_seen': 103401, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0946, 'grad_norm': 0.3725126087665558, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.9, 'num_input_tokens_seen': 190324, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1802, 'grad_norm': 0.3725126087665558, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.9, 'num_input_tokens_seen': 118374, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.5964, 'grad_norm': 0.3725126087665558, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.9, 'num_input_tokens_seen': 64344, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4954, 'grad_norm': 0.551727294921875, 'learning_rate': 4.8e-05, 'epoch': 0.9, 'num_input_tokens_seen': 267358, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.7007, 'grad_norm': 0.551727294921875, 'learning_rate': 4.8e-05, 'epoch': 0.9, 'num_input_tokens_seen': 119171, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.7501, 'grad_norm': 0.551727294921875, 'learning_rate': 4.8e-05, 'epoch': 0.9, 'num_input_tokens_seen': 64676, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7302, 'grad_norm': 0.551727294921875, 'learning_rate': 4.8e-05, 'epoch': 0.9, 'num_input_tokens_seen': 158944, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8664, 'grad_norm': 0.551727294921875, 'learning_rate': 4.8e-05, 'epoch': 0.9, 'num_input_tokens_seen': 191556, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1607, 'grad_norm': 0.551727294921875, 'learning_rate': 4.8e-05, 'epoch': 0.9, 'num_input_tokens_seen': 136789, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.4976, 'grad_norm': 0.551727294921875, 'learning_rate': 4.8e-05, 'epoch': 0.9, 'num_input_tokens_seen': 103903, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.5887, 'grad_norm': 0.551727294921875, 'learning_rate': 4.8e-05, 'epoch': 0.9, 'num_input_tokens_seen': 87798, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.5718, 'grad_norm': 0.41026195883750916, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.91, 'num_input_tokens_seen': 65561, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8159, 'grad_norm': 0.41026195883750916, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.91, 'num_input_tokens_seen': 268819, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1087, 'grad_norm': 0.41026195883750916, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.91, 'num_input_tokens_seen': 120293, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.7922, 'grad_norm': 0.41026195883750916, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.91, 'num_input_tokens_seen': 192864, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.1914, 'grad_norm': 0.41026195883750916, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.91, 'num_input_tokens_seen': 138017, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.653, 'grad_norm': 0.41026195883750916, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.91, 'num_input_tokens_seen': 160209, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6115, 'grad_norm': 0.41026195883750916, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.91, 'num_input_tokens_seen': 104877, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.9658, 'grad_norm': 0.41026195883750916, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.91, 'num_input_tokens_seen': 88707, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8008, 'grad_norm': 0.5088876485824585, 'learning_rate': 4e-05, 'epoch': 0.92, 'num_input_tokens_seen': 193981, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.4889, 'grad_norm': 0.5088876485824585, 'learning_rate': 4e-05, 'epoch': 0.92, 'num_input_tokens_seen': 138966, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9126, 'grad_norm': 0.5088876485824585, 'learning_rate': 4e-05, 'epoch': 0.92, 'num_input_tokens_seen': 161246, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8702, 'grad_norm': 0.5088876485824585, 'learning_rate': 4e-05, 'epoch': 0.92, 'num_input_tokens_seen': 270937, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8425, 'grad_norm': 0.5088876485824585, 'learning_rate': 4e-05, 'epoch': 0.92, 'num_input_tokens_seen': 105537, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.8529, 'grad_norm': 0.5088876485824585, 'learning_rate': 4e-05, 'epoch': 0.92, 'num_input_tokens_seen': 66060, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8168, 'grad_norm': 0.5088876485824585, 'learning_rate': 4e-05, 'epoch': 0.92, 'num_input_tokens_seen': 121184, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.5932, 'grad_norm': 0.5088876485824585, 'learning_rate': 4e-05, 'epoch': 0.92, 'num_input_tokens_seen': 89214, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9743, 'grad_norm': 0.39601343870162964, 'learning_rate': 3.6e-05, 'epoch': 0.93, 'num_input_tokens_seen': 272809, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.926, 'grad_norm': 0.39601343870162964, 'learning_rate': 3.6e-05, 'epoch': 0.93, 'num_input_tokens_seen': 66736, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.643, 'grad_norm': 0.39601343870162964, 'learning_rate': 3.6e-05, 'epoch': 0.93, 'num_input_tokens_seen': 89914, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.5219, 'grad_norm': 0.39601343870162964, 'learning_rate': 3.6e-05, 'epoch': 0.93, 'num_input_tokens_seen': 122399, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9677, 'grad_norm': 0.39601343870162964, 'learning_rate': 3.6e-05, 'epoch': 0.93, 'num_input_tokens_seen': 106745, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7045, 'grad_norm': 0.39601343870162964, 'learning_rate': 3.6e-05, 'epoch': 0.93, 'num_input_tokens_seen': 195483, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8264, 'grad_norm': 0.39601343870162964, 'learning_rate': 3.6e-05, 'epoch': 0.93, 'num_input_tokens_seen': 140183, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7984, 'grad_norm': 0.39601343870162964, 'learning_rate': 3.6e-05, 'epoch': 0.93, 'num_input_tokens_seen': 162525, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.5966, 'grad_norm': 0.4753822982311249, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.94, 'num_input_tokens_seen': 107557, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7356, 'grad_norm': 0.4753822982311249, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.94, 'num_input_tokens_seen': 274269, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7947, 'grad_norm': 0.4753822982311249, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.94, 'num_input_tokens_seen': 123213, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.557, 'grad_norm': 0.4753822982311249, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.94, 'num_input_tokens_seen': 90565, 'trained_token_ratio': 1.0}\n",
      " 94%|██████████████████████████████████████▍  | 117/125 [01:56<00:04,  1.67it/s]{'loss': 0.7422, 'grad_norm': 0.4753822982311249, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.94, 'num_input_tokens_seen': 67049, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7435, 'grad_norm': 0.4753822982311249, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.94, 'num_input_tokens_seen': 141104, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2674, 'grad_norm': 0.4753822982311249, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.94, 'num_input_tokens_seen': 163482, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8542, 'grad_norm': 0.4753822982311249, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.94, 'num_input_tokens_seen': 196772, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9703, 'grad_norm': 0.44083133339881897, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.94, 'num_input_tokens_seen': 124426, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2124, 'grad_norm': 0.44083133339881897, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.94, 'num_input_tokens_seen': 276193, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6596, 'grad_norm': 0.44083133339881897, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.94, 'num_input_tokens_seen': 108612, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3463, 'grad_norm': 0.44083133339881897, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.94, 'num_input_tokens_seen': 67723, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.672, 'grad_norm': 0.44083133339881897, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.94, 'num_input_tokens_seen': 91587, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6871, 'grad_norm': 0.44083133339881897, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.94, 'num_input_tokens_seen': 198666, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7583, 'grad_norm': 0.44083133339881897, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.94, 'num_input_tokens_seen': 142332, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3865, 'grad_norm': 0.44083133339881897, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.94, 'num_input_tokens_seen': 164998, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8628, 'grad_norm': 0.43917641043663025, 'learning_rate': 2.4e-05, 'epoch': 0.95, 'num_input_tokens_seen': 277724, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7407, 'grad_norm': 0.43917641043663025, 'learning_rate': 2.4e-05, 'epoch': 0.95, 'num_input_tokens_seen': 92310, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3644, 'grad_norm': 0.43917641043663025, 'learning_rate': 2.4e-05, 'epoch': 0.95, 'num_input_tokens_seen': 68414, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.7047, 'grad_norm': 0.43917641043663025, 'learning_rate': 2.4e-05, 'epoch': 0.95, 'num_input_tokens_seen': 109425, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.6784, 'grad_norm': 0.43917641043663025, 'learning_rate': 2.4e-05, 'epoch': 0.95, 'num_input_tokens_seen': 125446, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1761, 'grad_norm': 0.43917641043663025, 'learning_rate': 2.4e-05, 'epoch': 0.95, 'num_input_tokens_seen': 200187, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.715, 'grad_norm': 0.43917641043663025, 'learning_rate': 2.4e-05, 'epoch': 0.95, 'num_input_tokens_seen': 166176, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6835, 'grad_norm': 0.43917641043663025, 'learning_rate': 2.4e-05, 'epoch': 0.95, 'num_input_tokens_seen': 143408, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9958, 'grad_norm': 0.5094389319419861, 'learning_rate': 2e-05, 'epoch': 0.96, 'num_input_tokens_seen': 69022, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.759, 'grad_norm': 0.5094389319419861, 'learning_rate': 2e-05, 'epoch': 0.96, 'num_input_tokens_seen': 282036, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0544, 'grad_norm': 0.5094389319419861, 'learning_rate': 2e-05, 'epoch': 0.96, 'num_input_tokens_seen': 110247, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1094, 'grad_norm': 0.5094389319419861, 'learning_rate': 2e-05, 'epoch': 0.96, 'num_input_tokens_seen': 126527, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5178, 'grad_norm': 0.5094389319419861, 'learning_rate': 2e-05, 'epoch': 0.96, 'num_input_tokens_seen': 92978, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.4288, 'grad_norm': 0.5094389319419861, 'learning_rate': 2e-05, 'epoch': 0.96, 'num_input_tokens_seen': 144538, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2813, 'grad_norm': 0.5094389319419861, 'learning_rate': 2e-05, 'epoch': 0.96, 'num_input_tokens_seen': 167396, 'trained_token_ratio': 1.0}\n",
      " 96%|███████████████████████████████████████▎ | 120/125 [01:56<00:03,  1.42it/s]{'loss': 1.1556, 'grad_norm': 0.5094389319419861, 'learning_rate': 2e-05, 'epoch': 0.96, 'num_input_tokens_seen': 201417, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7676, 'grad_norm': 0.428365021944046, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.97, 'num_input_tokens_seen': 93959, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.58, 'grad_norm': 0.428365021944046, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.97, 'num_input_tokens_seen': 127824, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.818, 'grad_norm': 0.428365021944046, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.97, 'num_input_tokens_seen': 146022, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6846, 'grad_norm': 0.428365021944046, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.97, 'num_input_tokens_seen': 69635, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.857, 'grad_norm': 0.428365021944046, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.97, 'num_input_tokens_seen': 284166, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6324, 'grad_norm': 0.428365021944046, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.97, 'num_input_tokens_seen': 168894, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.0552, 'grad_norm': 0.428365021944046, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.97, 'num_input_tokens_seen': 203503, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9426, 'grad_norm': 0.428365021944046, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.97, 'num_input_tokens_seen': 111239, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7974, 'grad_norm': 0.5285118222236633, 'learning_rate': 1.2e-05, 'epoch': 0.98, 'num_input_tokens_seen': 170295, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0933, 'grad_norm': 0.5285118222236633, 'learning_rate': 1.2e-05, 'epoch': 0.98, 'num_input_tokens_seen': 147097, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6151, 'grad_norm': 0.5285118222236633, 'learning_rate': 1.2e-05, 'epoch': 0.98, 'num_input_tokens_seen': 285645, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.5771, 'grad_norm': 0.5285118222236633, 'learning_rate': 1.2e-05, 'epoch': 0.98, 'num_input_tokens_seen': 204961, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2629, 'grad_norm': 0.5285118222236633, 'learning_rate': 1.2e-05, 'epoch': 0.98, 'num_input_tokens_seen': 94615, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4019, 'grad_norm': 0.5285118222236633, 'learning_rate': 1.2e-05, 'epoch': 0.98, 'num_input_tokens_seen': 112089, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7195, 'grad_norm': 0.5285118222236633, 'learning_rate': 1.2e-05, 'epoch': 0.98, 'num_input_tokens_seen': 69947, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.8453, 'grad_norm': 0.5285118222236633, 'learning_rate': 1.2e-05, 'epoch': 0.98, 'num_input_tokens_seen': 128745, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9639, 'grad_norm': 0.4254318177700043, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.98, 'num_input_tokens_seen': 171742, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7733, 'grad_norm': 0.4254318177700043, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.98, 'num_input_tokens_seen': 148031, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7185, 'grad_norm': 0.4254318177700043, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.98, 'num_input_tokens_seen': 206699, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0519, 'grad_norm': 0.4254318177700043, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.98, 'num_input_tokens_seen': 287654, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3316, 'grad_norm': 0.4254318177700043, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.98, 'num_input_tokens_seen': 95124, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.4328, 'grad_norm': 0.4254318177700043, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.98, 'num_input_tokens_seen': 70363, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.5436, 'grad_norm': 0.4254318177700043, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.98, 'num_input_tokens_seen': 129608, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 0.5892, 'grad_norm': 0.4254318177700043, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.98, 'num_input_tokens_seen': 112930, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7324, 'grad_norm': 0.4957119822502136, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.99, 'num_input_tokens_seen': 149120, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1892, 'grad_norm': 0.4957119822502136, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.99, 'num_input_tokens_seen': 289527, 'trained_token_ratio': 1.0}\n",
      " 99%|████████████████████████████████████████▋| 124/125 [01:50<00:00,  1.71it/s]\u001b[1mINFO    \u001b[0m | \u001b[36mtrainer.py:2250 (train)\u001b[0m - \u001b[1m\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| Task                 | File:Line       |   Depth | Time (s)   | Percentage (%)   |\n",
      "+======================+=================+=========+============+==================+\n",
      "| 1. get_batch_samples | patching.py:501 |       0 | 0.67 s     | \u001b[94m0.61 %\u001b[0m           |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 2. forward           | patching.py:597 |       0 | 62.51 s    | \u001b[93m56.78 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 3. sync gradients    | patching.py:601 |       0 | 31.39 s    | \u001b[92m28.51 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 4. optimizer step    | patching.py:666 |       0 | 15.51 s    | \u001b[94m14.09 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "\n",
      "Total time elapsed: 110.07 seconds.\u001b[0m\n",
      "{'loss': 0.5456, 'grad_norm': 0.4957119822502136, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.99, 'num_input_tokens_seen': 70811, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9809, 'grad_norm': 0.4957119822502136, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.99, 'num_input_tokens_seen': 130362, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.0252, 'grad_norm': 0.4957119822502136, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.99, 'num_input_tokens_seen': 95739, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0821, 'grad_norm': 0.4957119822502136, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.99, 'num_input_tokens_seen': 113626, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.6761, 'grad_norm': 0.4957119822502136, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.99, 'num_input_tokens_seen': 208419, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9363, 'grad_norm': 0.4957119822502136, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.99, 'num_input_tokens_seen': 172873, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.7591, 'grad_norm': 0.3528132438659668, 'learning_rate': 0.0, 'epoch': 1.0, 'num_input_tokens_seen': 115133, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.0843, 'grad_norm': 0.3528132438659668, 'learning_rate': 0.0, 'epoch': 1.0, 'num_input_tokens_seen': 292211, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.4926, 'grad_norm': 0.3528132438659668, 'learning_rate': 0.0, 'epoch': 1.0, 'num_input_tokens_seen': 71910, 'trained_token_ratio': 0.9999999403953552}\n",
      "{'loss': 1.4446, 'grad_norm': 0.3528132438659668, 'learning_rate': 0.0, 'epoch': 1.0, 'num_input_tokens_seen': 96986, 'trained_token_ratio': 1.0}\n",
      "100%|█████████████████████████████████████████| 125/125 [02:01<00:00,  1.70it/s]Repo card metadata block was not found. Setting CardData to empty.\n",
      "{'loss': 0.6274, 'grad_norm': 0.3528132438659668, 'learning_rate': 0.0, 'epoch': 1.0, 'num_input_tokens_seen': 210425, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.93, 'grad_norm': 0.3528132438659668, 'learning_rate': 0.0, 'epoch': 1.0, 'num_input_tokens_seen': 174838, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2945, 'grad_norm': 0.3528132438659668, 'learning_rate': 0.0, 'epoch': 1.0, 'num_input_tokens_seen': 150905, 'trained_token_ratio': 1.0}\n",
      "{'loss': 0.9074, 'grad_norm': 0.3528132438659668, 'learning_rate': 0.0, 'epoch': 1.0, 'num_input_tokens_seen': 131936, 'trained_token_ratio': 1.0}\n",
      "100%|█████████████████████████████████████████| 125/125 [01:59<00:00,  1.67it/s]\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:726 (_inner_training_loop)\u001b[0m - \u001b[1m\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\u001b[0m\n",
      "{'train_runtime': 117.0068, 'train_samples_per_second': 1.068, 'train_steps_per_second': 1.068, 'train_loss': 1.196331015586853, 'epoch': 1.0, 'num_input_tokens_seen': 115133, 'trained_token_ratio': 1.0}\n",
      "100%|█████████████████████████████████████████| 125/125 [01:57<00:00,  1.07it/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:726 (_inner_training_loop)\u001b[0m - \u001b[1m\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\u001b[0m\n",
      "{'train_runtime': 112.2323, 'train_samples_per_second': 1.114, 'train_steps_per_second': 1.114, 'train_loss': 1.0430723390579224, 'epoch': 1.0, 'num_input_tokens_seen': 292211, 'trained_token_ratio': 1.0}\n",
      "100%|█████████████████████████████████████████| 125/125 [01:52<00:00,  1.11it/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mhp_trainer.py:85 (_train)\u001b[0m - \u001b[1mSave model to /data-4090/anhvth5/hypersloth_output/loras/gemma-3-1b-it/openo1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:726 (_inner_training_loop)\u001b[0m - \u001b[1m\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\u001b[0m\n",
      "{'train_runtime': 117.1784, 'train_samples_per_second': 1.067, 'train_steps_per_second': 1.067, 'train_loss': 1.2822096076011658, 'epoch': 1.0, 'num_input_tokens_seen': 71910, 'trained_token_ratio': 0.9999999403953552}\n",
      "100%|█████████████████████████████████████████| 125/125 [01:57<00:00,  1.07it/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:726 (_inner_training_loop)\u001b[0m - \u001b[1m\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\u001b[0m\n",
      "{'train_runtime': 123.3928, 'train_samples_per_second': 1.013, 'train_steps_per_second': 1.013, 'train_loss': 1.1543395731449126, 'epoch': 1.0, 'num_input_tokens_seen': 96986, 'trained_token_ratio': 1.0}\n",
      "100%|█████████████████████████████████████████| 125/125 [02:03<00:00,  1.01it/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:726 (_inner_training_loop)\u001b[0m - \u001b[1m\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\u001b[0m\n",
      "{'train_runtime': 121.0186, 'train_samples_per_second': 1.033, 'train_steps_per_second': 1.033, 'train_loss': 1.0995243089199067, 'epoch': 1.0, 'num_input_tokens_seen': 174838, 'trained_token_ratio': 1.0}\n",
      "100%|█████████████████████████████████████████| 125/125 [02:01<00:00,  1.03it/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:726 (_inner_training_loop)\u001b[0m - \u001b[1m\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\u001b[0m\n",
      "{'train_runtime': 121.8201, 'train_samples_per_second': 1.026, 'train_steps_per_second': 1.026, 'train_loss': 1.2163277549743652, 'epoch': 1.0, 'num_input_tokens_seen': 131936, 'trained_token_ratio': 1.0}\n",
      "100%|█████████████████████████████████████████| 125/125 [02:01<00:00,  1.03it/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:726 (_inner_training_loop)\u001b[0m - \u001b[1m\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\u001b[0m\n",
      "{'train_runtime': 117.103, 'train_samples_per_second': 1.067, 'train_steps_per_second': 1.067, 'train_loss': 1.090648378610611, 'epoch': 1.0, 'num_input_tokens_seen': 210425, 'trained_token_ratio': 1.0}\n",
      "100%|█████████████████████████████████████████| 125/125 [01:57<00:00,  1.07it/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:726 (_inner_training_loop)\u001b[0m - \u001b[1m\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\u001b[0m\n",
      "{'train_runtime': 119.0602, 'train_samples_per_second': 1.05, 'train_steps_per_second': 1.05, 'train_loss': 1.1958242387771607, 'epoch': 1.0, 'num_input_tokens_seen': 150905, 'trained_token_ratio': 1.0}\n",
      "100%|█████████████████████████████████████████| 125/125 [01:59<00:00,  1.05it/s]\n",
      "\u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mhp_trainer.py:194 (train)\u001b[0m - \u001b[32m\u001b[1mAll processes finished\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python HyperSloth/scripts/hp_trainer.py examples/config-openo1-gemma3-1b.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61adcf5e-a12e-40f0-b0aa-b0ed9d1e9171",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
