{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "658abd4f-17e4-4fd3-8770-cfe9d60f34a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[36mhp_trainer.py:144 (train)\u001b[0m - \u001b[1m\n",
      "Key                            Value\n",
      "-----------------------------  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "grad_dir                       /dev/shm/hypersloth\n",
      "data                           {'dataset_name_or_path': 'data/OpenO1-SFT', 'test_ratio': 0.0, 'dataset_num_proc': 16, 'instruction_part': '<start_of_turn>user\\n', 'response_part': '<start_of_turn>model\\n', 'num_samples': 80, 'group_by_length': True}\n",
      "training                       {'gpus': [0], 'loss_type': 'response_only', 'packing': False}\n",
      "fast_model_args                {'model_name': 'unsloth/gemma-3-1b-it-bnb-4bit', 'max_seq_length': 16000, 'load_in_4bit': True, 'load_in_8bit': False, 'full_finetuning': False, 'token': None}\n",
      "lora_args                      {'finetune_vision_layers': False, 'finetune_language_layers': True, 'finetune_attention_modules': True, 'finetune_mlp_modules': True, 'r': 16, 'lora_alpha': 16, 'lora_dropout': 0.0, 'bias': 'none', 'random_state': 3407}\n",
      "hps_version                    2\n",
      "output_dir                     /data-4090/anhvth5/hypersloth_output/loras/gemma-3-1b-it/openo1\n",
      "per_device_train_batch_size    1\n",
      "learning_rate                  0.0001\n",
      "gradient_accumulation_steps    8\n",
      "per_device_eval_batch_size     4\n",
      "eval_steps                     100000\n",
      "logging_steps                  1\n",
      "report_to                      tensorboard\n",
      "num_train_epochs               1\n",
      "lr_scheduler_type              linear\n",
      "warmup_steps                   0\n",
      "seed                           42\n",
      "save_total_limit               2\n",
      "bf16                           True\n",
      "fp16                           False\n",
      "optim                          adamw_8bit\n",
      "weight_decay                   0.01\n",
      "packing                        False\n",
      "save_only_model                True\n",
      "save_steps                     200\n",
      "include_num_input_tokens_seen  True\u001b[0m\n",
      "Logging to .log/process_0.log\n",
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtransformer_trainer_setup.py:81 (_initialize_model_and_tokenizer)\u001b[0m - \u001b[1mUsing compiler location: .cache/unsloth_compiled_cache_0\u001b[0m\n",
      "==((====))==  Unsloth 2025.3.18: Fast Gemma3 patching. Transformers: 4.50.0.dev0.\n",
      "   \\\\   /|    NVIDIA L40S. Num GPUs = 1. Max memory: 44.403 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtransformer_trainer_setup.py:113 (_create_trainer)\u001b[0m - \u001b[1mGPU 0: Loading dataset from .cache/dataset_42e24240d589ee13_b357b2fa1bbd5cc1.cache\u001b[0m\n",
      "num_proc must be <= 80. Reducing num_proc to 80 for dataset of size 80.\n",
      "Map (num_proc=80): 100%|████████████████| 80/80 [00:00<00:00, 117.13 examples/s]\n",
      "num_proc must be <= 80. Reducing num_proc to 80 for dataset of size 80.\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mtransformer_trainer_setup.py:45 (setup_model_and_training)\u001b[0m - \u001b[1mModel setup complete for GPU 0\u001b[0m\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "=== EXAMPLE #1 ===\n",
      "\u001b[92m<bos><start_of_turn>user\n",
      "Anita, Indu and Geeta can do a piece of work in 18 days, 27 days and 36 days respectively. They start working together. After working for 4 days, Anita goes away and Indu leaves 7 days before the work is finished. Only Geeta remains at work from beginning till end. In how many days was the whole work done?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\u001b[0m\u001b[93m<Thought>\n",
      "Alright, I need to figure out how long it took for Anita, Indu, and Geeta to complete the work under the given conditions. Let's break down the information step by step. \n",
      "\n",
      "First, Anita can complete the work in 18 days, Indu in 27 days, and Geeta in 36 days. So their individual work rates are:\n",
      "- Anita: 1/18 work per day\n",
      "- Indu: 1/27 work per day\n",
      "- Geeta: 1/36 work per day\n",
      "\n",
      "They start working together, which means their combined work rate initially is 1/18 + 1/27 + 1/36. I need to calculate this sum. Let's find a common denominator for 18, 27, and 36. The least common multiple of these numbers is 108. \n",
      "\n",
      "So,\n",
      "1/18 = 6/108\n",
      "1/27 = 4/108\n",
      "1/36 = 3/108\n",
      "\n",
      "Adding them up: 6/108 + 4/108 + 3/108 = 13/108 work per day.\n",
      "\n",
      "They work together for 4 days. So in these 4 days, they complete 4 * 13/108 = 52/108 of the work. Simplifying, that's 13/27 of the work done.\n",
      "\n",
      "After 4 days, Anita goes away. So now, only Indu and Geeta are working. Let's find their combined work rate:\n",
      "1/27 + 1/36. Again, find a common denominator, which is 108.\n",
      "1/27 = 4/108\n",
      "1/36 = 3/108\n",
      "Combined work rate = 4/108 + 3/108 = 7/108 work per day.\n",
      "\n",
      "But Indu leaves 7 days before the work is finished. Hmm, this part is tricky. Let's assume the total time taken to complete the work is T days. Anita leaves after 4 days, and Indu leaves at T - 7 days.\n",
      "\n",
      "So Geeta works the entire T days. Indu works from day 1 to day T - 7, which is (T - 7) days. Anita works only the first 4 days.\n",
      "\n",
      "Let's express the total work done as the sum of work done by each individual:\n",
      "- Anita: 4 days * (1/18) = 4/18 = 2/9\n",
      "- Indu: (T - 7) days * (1/27)\n",
      "- Geeta: T days * (1/36)\n",
      "\n",
      "So total work: 2/9 + (T - 7)/27 + T/36 = 1 (complete work).\n",
      "\n",
      "Let's simplify this equation:\n",
      "First, find the common denominator, which is 108.\n",
      "2/9 = 24/108\n",
      "(T - 7)/27 = 4(T - 7)/108 = (4T - 28)/108\n",
      "T/36 = 3T/108\n",
      "\n",
      "Adding them up: 24/108 + (4T - 28)/108 + 3T/108 = 1\n",
      "Combine like terms: (4T + 3T + 24 - 28) / 108 = 1\n",
      "7T - 4 = 108\n",
      "7T = 112\n",
      "T = 16 days\n",
      "\n",
      "Wait, but the problem says Indu leaves 7 days before the work is finished. If the total time is 16 days, Indu works for 16 - 7 = 9 days. Let's check if this makes sense.\n",
      "\n",
      "Work done by Anita: 4/18 = 2/9\n",
      "Work done by Indu: 9/27 = 1/3\n",
      "Work done by Geeta: 16/36 = 4/9\n",
      "\n",
      "Adding them up: 2/9 + 1/3 + 4/9 = 2/9 + 3/9 + 4/9 = 9/9 = 1. It checks out.\n",
      "\n",
      "So the whole work was done in 16 days.\n",
      "\n",
      "But wait, could I have made any mistakes here? Let's see. The initial calculations seem correct. I accounted for when each person was working and their respective contributions. The final check adds up to the whole work, so it seems accurate.\n",
      "\n",
      "Let me think differently: suppose I consider Geeta's continuous work and the varying contributions of Anita and Indu. Maybe setting up the equation differently could provide the same result, but given that the first method worked, it's probably correct.\n",
      "\n",
      "Another thought: could I have used ratios or proportions differently? Maybe, but the algebraic method seems straightforward for this problem.\n",
      "\n",
      "In conclusion, after breaking down each person's contribution and setting up the equation correctly, the total time taken to complete the work is 16 days.\n",
      "\n",
      "</Thought>\n",
      "<Output>\n",
      "The whole work was done in 16 days.\n",
      "</Output><end_of_turn>\n",
      "\u001b[0m\n",
      "\n",
      "More training debug examples written to .log/dataloader_examples.html\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mhp_trainer.py:70 (_train)\u001b[0m - \u001b[1mUsing gradient sync callback v2\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mmmap_gradient_sync_v2.py:88 (__init__)\u001b[0m - \u001b[1m[Init GPU=0] Memmap opened: total_size=13045764\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mmmap_gradient_sync_v2.py:97 (__init__)\u001b[0m - \u001b[1m[Init GPU=0] local_rank=0, gpus=[0], is_main=True, starting global_step=0\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mhp_trainer.py:80 (_train)\u001b[0m - \u001b[1mUsing gradient sync callback for GPU 0\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:365 (_inner_training_loop)\u001b[0m - \u001b[1m***** Running training *****\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:366 (_inner_training_loop)\u001b[0m - \u001b[1m  Num examples = 80\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:367 (_inner_training_loop)\u001b[0m - \u001b[1m  Num Epochs = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:368 (_inner_training_loop)\u001b[0m - \u001b[1m  Instantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:375 (_inner_training_loop)\u001b[0m - \u001b[1m  Total train batch size (w. parallel, distributed & accumulation) = 8\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:378 (_inner_training_loop)\u001b[0m - \u001b[1m  Gradient Accumulation steps = 8\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:381 (_inner_training_loop)\u001b[0m - \u001b[1m  Total optimization steps = 10\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:382 (_inner_training_loop)\u001b[0m - \u001b[1m  Number of trainable parameters = 13,045,760\u001b[0m\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]Unsloth: Will smartly offload gradients to save VRAM!\n",
      "{'loss': 1.386, 'grad_norm': 1.672317624092102, 'learning_rate': 0.0001, 'epoch': 0.1, 'num_input_tokens_seen': 11060, 'trained_token_ratio': 0.9999999403953552}\n",
      " 10%|████▍                                       | 1/10 [00:27<04:10, 27.83s/it]\u001b[1mINFO    \u001b[0m | \u001b[36mtrainer.py:2250 (train)\u001b[0m - \u001b[1m\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| Task                 | File:Line       |   Depth | Time (s)   | Percentage (%)   |\n",
      "+======================+=================+=========+============+==================+\n",
      "| 1. get_batch_samples | patching.py:502 |       0 | 0.06 s     | \u001b[94m0.20 %\u001b[0m           |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 2. forward           | patching.py:598 |       0 | 26.61 s    | \u001b[91m95.54 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 3. sync gradients    | patching.py:602 |       0 | 0.05 s     | \u001b[94m0.18 %\u001b[0m           |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 4. optimizer step    | patching.py:667 |       0 | 1.14 s     | \u001b[94m4.08 %\u001b[0m           |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "\n",
      "Total time elapsed: 27.85 seconds.\u001b[0m\n",
      "{'loss': 1.2685, 'grad_norm': 1.0606039762496948, 'learning_rate': 8.888888888888889e-05, 'epoch': 0.2, 'num_input_tokens_seen': 22961, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.2144, 'grad_norm': 1.2167237997055054, 'learning_rate': 7.777777777777778e-05, 'epoch': 0.3, 'num_input_tokens_seen': 33134, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3546, 'grad_norm': 1.3005342483520508, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.4, 'num_input_tokens_seen': 41428, 'trained_token_ratio': 1.0}\n",
      " 40%|█████████████████▌                          | 4/10 [00:37<00:39,  6.65s/it]\u001b[1mINFO    \u001b[0m | \u001b[36mtrainer.py:2250 (train)\u001b[0m - \u001b[1m\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| Task                 | File:Line       |   Depth | Time (s)   | Percentage (%)   |\n",
      "+======================+=================+=========+============+==================+\n",
      "| 1. get_batch_samples | patching.py:502 |       0 | 0.11 s     | \u001b[94m0.28 %\u001b[0m           |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 2. forward           | patching.py:598 |       0 | 36.37 s    | \u001b[91m95.85 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 3. sync gradients    | patching.py:602 |       0 | 0.21 s     | \u001b[94m0.56 %\u001b[0m           |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 4. optimizer step    | patching.py:667 |       0 | 1.26 s     | \u001b[94m3.31 %\u001b[0m           |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "\n",
      "Total time elapsed: 37.95 seconds.\u001b[0m\n",
      "{'loss': 1.0222, 'grad_norm': 0.8115654587745667, 'learning_rate': 5.555555555555556e-05, 'epoch': 0.5, 'num_input_tokens_seen': 50081, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.3967, 'grad_norm': 0.9273071885108948, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.6, 'num_input_tokens_seen': 57743, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1429, 'grad_norm': 0.8204396367073059, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.7, 'num_input_tokens_seen': 65759, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.4855, 'grad_norm': 0.918896496295929, 'learning_rate': 2.2222222222222223e-05, 'epoch': 0.8, 'num_input_tokens_seen': 75099, 'trained_token_ratio': 1.0}\n",
      " 80%|███████████████████████████████████▏        | 8/10 [00:50<00:07,  3.60s/it]\u001b[1mINFO    \u001b[0m | \u001b[36mtrainer.py:2250 (train)\u001b[0m - \u001b[1m\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| Task                 | File:Line       |   Depth | Time (s)   | Percentage (%)   |\n",
      "+======================+=================+=========+============+==================+\n",
      "| 1. get_batch_samples | patching.py:502 |       0 | 0.16 s     | \u001b[94m0.33 %\u001b[0m           |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 2. forward           | patching.py:598 |       0 | 48.46 s    | \u001b[91m96.04 %\u001b[0m          |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 3. sync gradients    | patching.py:602 |       0 | 0.41 s     | \u001b[94m0.81 %\u001b[0m           |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "| 4. optimizer step    | patching.py:667 |       0 | 1.43 s     | \u001b[94m2.83 %\u001b[0m           |\n",
      "+----------------------+-----------------+---------+------------+------------------+\n",
      "\n",
      "Total time elapsed: 50.46 seconds.\u001b[0m\n",
      "{'loss': 1.0745, 'grad_norm': 0.4851621985435486, 'learning_rate': 1.1111111111111112e-05, 'epoch': 0.9, 'num_input_tokens_seen': 86238, 'trained_token_ratio': 1.0}\n",
      "{'loss': 1.1982, 'grad_norm': 0.48760178685188293, 'learning_rate': 0.0, 'epoch': 1.0, 'num_input_tokens_seen': 96233, 'trained_token_ratio': 0.9999999403953552}\n",
      "100%|███████████████████████████████████████████| 10/10 [00:56<00:00,  3.18s/it]\u001b[1mINFO    \u001b[0m | \u001b[36mpatching.py:727 (_inner_training_loop)\u001b[0m - \u001b[1m\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\u001b[0m\n",
      "{'train_runtime': 57.4652, 'train_samples_per_second': 1.392, 'train_steps_per_second': 0.174, 'train_loss': 1.254346776008606, 'epoch': 1.0, 'num_input_tokens_seen': 96233, 'trained_token_ratio': 0.9999999403953552}\n",
      "100%|███████████████████████████████████████████| 10/10 [00:57<00:00,  5.75s/it]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[36mhp_trainer.py:85 (_train)\u001b[0m - \u001b[1mSave model to /data-4090/anhvth5/hypersloth_output/loras/gemma-3-1b-it/openo1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python HyperSloth/scripts/hp_trainer.py examples/config-openo1-gemma3-1b.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61adcf5e-a12e-40f0-b0aa-b0ed9d1e9171",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python HyperSloth/scripts/hp_trainer.py examples/config-openo1-gemma3-1b.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86070a1-f630-4844-81af-8e18a3dae773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af996594-9f1b-46b2-8419-45fb64d10462",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
