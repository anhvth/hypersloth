{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1555d26d",
   "metadata": {},
   "source": [
    "# üöÄ HyperSloth Demo Training Notebook\n",
    "\n",
    "This notebook demonstrates how to fine-tune large language models using HyperSloth's multi-GPU capabilities. It's equivalent to running:\n",
    "\n",
    "```bash\n",
    "hypersloth-train examples/example_sharegpt_lora_2gpus.py\n",
    "```\n",
    "\n",
    "## What This Demo Does\n",
    "\n",
    "- **Multi-GPU Training**: Uses 2 GPUs with NCCL synchronization\n",
    "- **Adaptive Batching**: Optimizes sequence sorting and padding\n",
    "- **LoRA Fine-tuning**: Efficient parameter updates with Low-Rank Adaptation\n",
    "- **Response-only Loss**: Calculates loss only on assistant responses\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. HyperSloth installed: `pip install git+https://github.com/anhvth/HyperSloth.git`\n",
    "2. At least 2 GPUs available (adjust `gpus=[0, 1]` if needed)\n",
    "3. Sufficient VRAM (reduce batch size if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed68946",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72a817bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• CUDA Available: True\n",
      "üî• GPU Count: 4\n",
      "   GPU 0: NVIDIA H100 80GB HBM3\n",
      "   GPU 1: NVIDIA H100 80GB HBM3\n",
      "   GPU 2: NVIDIA H100 80GB HBM3\n",
      "   GPU 3: NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "# Import HyperSloth configuration classes\n",
    "from HyperSloth.hypersloth_config import *\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f'üî• CUDA Available: {torch.cuda.is_available()}')\n",
    "print(f'üî• GPU Count: {torch.cuda.device_count()}')\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f'   GPU {i}: {torch.cuda.get_device_name(i)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b606c272",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration Setup\n",
    "\n",
    "HyperSloth uses Pydantic models for type-safe configuration. We'll set up:\n",
    "\n",
    "1. **Data Configuration**: Dataset and tokenization settings\n",
    "2. **Training Configuration**: GPU allocation and loss calculation\n",
    "3. **Model Configuration**: Base model and LoRA parameters\n",
    "4. **Training Arguments**: Learning rate, batch size, and optimization settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f33c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global batch size: 16\n",
      "[MP] Running on 2 GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m06:33:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mhp_trainer.py:42\u001b[0m | \u001b[1mTraining on GPU 1 with output_dir outputs/qwen3-8b-openthought-2gpus/\u001b[0m\n",
      "\u001b[32m06:33:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mhp_trainer.py:45\u001b[0m | \u001b[1müöÄ Starting total training timer\u001b[0m\n",
      "\u001b[32m06:33:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mhp_trainer.py:42\u001b[0m | \u001b[1mTraining on GPU 0 with output_dir outputs/qwen3-8b-openthought-2gpus/\u001b[0m\n",
      "\u001b[32m06:33:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mhp_trainer.py:45\u001b[0m | \u001b[1müöÄ Starting total training timer\u001b[0m\n",
      "\u001b[32m06:33:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_loading: 15.82s\u001b[0m\n",
      "\u001b[32m06:33:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mnccl_grad_sync.py:128\u001b[0m | \u001b[1m[GPU=1] NCCL env: RANK=1, WORLD_SIZE=2, MASTER_ADDR=127.0.0.1, MASTER_PORT=29501\u001b[0m\n",
      "\u001b[32m06:33:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mnccl_grad_sync.py:132\u001b[0m | \u001b[1m[GPU=1] Setting current CUDA device to:0, os.environ['CUDA_VISIBLE_DEVICES']='1'\u001b[0m\n",
      "\u001b[32m06:33:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_loading: 17.02s\u001b[0m\n",
      "\u001b[32m06:33:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mnccl_grad_sync.py:128\u001b[0m | \u001b[1m[GPU=0] NCCL env: RANK=0, WORLD_SIZE=2, MASTER_ADDR=127.0.0.1, MASTER_PORT=29501\u001b[0m\n",
      "\u001b[32m06:33:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mnccl_grad_sync.py:132\u001b[0m | \u001b[1m[GPU=0] Setting current CUDA device to:0, os.environ['CUDA_VISIBLE_DEVICES']='0'\u001b[0m\n",
      "\u001b[32m06:33:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:49\u001b[0m | \u001b[1mModel loaded on device cuda:0, tokenizer: Qwen2TokenizerFast\u001b[0m\n",
      "\u001b[32m06:33:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36minit_modules.py:49\u001b[0m | \u001b[1mModel loaded on device cuda:0, tokenizer: Qwen2TokenizerFast\u001b[0m\n",
      "\u001b[32m06:33:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  lora_setup: 3.96s\u001b[0m\n",
      "\u001b[32m06:33:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_init: 21.88s\u001b[0m\n",
      "\u001b[32m06:33:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mdataset_utils.py:222\u001b[0m | \u001b[1mPreparing dataset fe8a427db40dbf27...\u001b[0m\n",
      "\u001b[32m06:33:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  lora_setup: 3.98s\u001b[0m\n",
      "\u001b[32m06:33:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_init: 21.42s\u001b[0m\n",
      "\u001b[32m06:33:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mdataset_utils.py:232\u001b[0m | \u001b[1mDataset fe8a427db40dbf27 being prepared by another process, waiting...\u001b[0m\n",
      "Unsloth: Will map <|im_end|> to EOS = <|im_end|>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "Using compiler location: .cache/unsloth_compiled_cache_1\n",
      "==((====))==  Unsloth 2025.5.9: Fast Qwen3 patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.189 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying chat templates (num_proc=52): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:12<00:00, 78.74 examples/s]\n",
      "Tokenizing dataset (num_proc=52): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:13<00:00, 75.75 examples/s]\n",
      "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]\u001b[32m06:34:31\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mdataset_utils.py:242\u001b[0m | \u001b[33m\u001b[1mFailed to load dataset fe8a427db40dbf27: Directory /home/anhvth5/.cache/hypersloth/datasets/fe8a427db40dbf27 is neither a `Dataset` directory. Retrying in 3s... (attempt 1/30)\u001b[0m\n",
      "Saving the dataset (1/1 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 4276.21 examples/s]\n",
      "\u001b[32m06:34:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36minit_modules.py:142\u001b[0m | \u001b[1mCreating final SFTTrainer with prepared dataset...\u001b[0m\n",
      "\u001b[32m06:34:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:142\u001b[0m | \u001b[1mCreating final SFTTrainer with prepared dataset...\u001b[0m\n",
      "\u001b[32m06:34:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  final_trainer_creation: 3.52s\u001b[0m\n",
      "\u001b[32m06:34:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36minit_modules.py:155\u001b[0m | \u001b[1mReplacing DataCollatorForLanguageModeling with DataCollatorForSeq2Seq for better sequence handling\u001b[0m\n",
      "\u001b[32m06:34:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36minit_modules.py:163\u001b[0m | \u001b[1mTrainer setup completed successfully\u001b[0m\n",
      "\u001b[32m06:34:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  trainer_setup: 45.70s\u001b[0m\n",
      "\u001b[32m06:34:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  final_trainer_creation: 3.88s\u001b[0m\n",
      "\u001b[32m06:34:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:155\u001b[0m | \u001b[1mReplacing DataCollatorForLanguageModeling with DataCollatorForSeq2Seq for better sequence handling\u001b[0m\n",
      "\u001b[32m06:34:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:163\u001b[0m | \u001b[1mTrainer setup completed successfully\u001b[0m\n",
      "\u001b[32m06:34:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  trainer_setup: 48.92s\u001b[0m\n",
      "\u001b[32m06:34:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:119\u001b[0m | \u001b[1mAdd callback ShuffleData to Trainer UnslothSFTTrainer\u001b[0m\n",
      "\u001b[32m06:34:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  trainer_creation: 48.97s\u001b[0m\n",
      "\u001b[32m06:34:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  total_setup: 1.3m\u001b[0m\n",
      "\u001b[32m06:34:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_and_training_setup: 1.3m\u001b[0m\n",
      "\u001b[32m06:34:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mnccl_grad_sync.py:48\u001b[0m | \u001b[1m[GPU=0] NCCLGradSyncCallback initialized for rank 0/2\u001b[0m\n",
      "\u001b[32m06:34:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mhp_trainer.py:64\u001b[0m | \u001b[1mUsing gradient sync callback for GPU 0\u001b[0m\n",
      "\u001b[32m06:34:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  training_loop_patch: 3.29s\u001b[0m\n",
      "\u001b[32m06:34:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36minit_modules.py:119\u001b[0m | \u001b[1mAdd callback ShuffleData to Trainer UnslothSFTTrainer\u001b[0m\n",
      "\u001b[32m06:34:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  trainer_creation: 49.00s\u001b[0m\n",
      "\u001b[32m06:34:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  total_setup: 1.3m\u001b[0m\n",
      "\u001b[32m06:34:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_and_training_setup: 1.3m\u001b[0m\n",
      "\u001b[32m06:34:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mnccl_grad_sync.py:48\u001b[0m | \u001b[1m[GPU=1] NCCLGradSyncCallback initialized for rank 1/2\u001b[0m\n",
      "\u001b[32m06:34:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mhp_trainer.py:64\u001b[0m | \u001b[1mUsing gradient sync callback for GPU 1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "Using compiler location: .cache/unsloth_compiled_cache_0\n",
      "==((====))==  Unsloth 2025.5.9: Fast Qwen3 patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.189 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "[LOCAL_RANK=0] Patching log. Dir: outputs/qwen3-8b-openthought-2gpus/, GPUs: 2\n",
      "[LOCAL_RANK=0] Log patch initialization complete.\n",
      "üîß Patching Trainer to use RandomSamplerSeededByEpoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/189 [00:00<?, ?it/s]\u001b[32m06:34:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:21\u001b[0m | \u001b[1müîÑ Starting epoch 1\u001b[0m\n",
      "\u001b[32m06:34:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1müé≤ Sampler epoch 0: emitting 1000 indices\n",
      "First ids: [776, 507, 895, 922, 33, 483, 85, 750, 354, 523]\n",
      "...Last ids: [104, 754, 142, 228, 250, 281, 759, 25, 114, 654]\u001b[0m\n",
      "  0%|          | 0/189 [00:00<?, ?it/s]\u001b[32m06:34:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1müé≤ Sampler epoch 0: emitting 1000 indices\n",
      "First ids: [776, 507, 895, 922, 33, 483, 85, 750, 354, 523]\n",
      "...Last ids: [104, 754, 142, 228, 250, 281, 759, 25, 114, 654]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOCAL_RANK=1] Patching log. Dir: outputs/qwen3-8b-openthought-2gpus/, GPUs: 2\n",
      "[LOCAL_RANK=1] Log patch initialization complete.\n",
      "üîß Patching Trainer to use RandomSamplerSeededByEpoch\n",
      "\n",
      "=== EXAMPLE #1 ===\n",
      "\u001b[92m<|im_start|>system\n",
      "You are an assistant that thoroughly explores questions through a systematic long thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarization, exploration, reassessment, reflection, backtracing, and iteration to develop a well-considered thinking process. Detail your reasoning process using the specified format: <think>thought with steps separated by '\n",
      "\n",
      "'</think> Each step should include detailed considerations such as analyzing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. Based on various attempts, explorations, and reflections from the thoughts, you should systematically present the final solution that you deem correct. The solution should remain a logical, accurate, concise expression style and detail necessary steps needed to reach the conclusion. Now, try to solve the following question through the above guidelines.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the critical temperature at which water undergoes a phase transition from liquid to solid when subjected to external pressure of 100 atm? Use the theory of condensed matter physics to solve this problem.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "... (truncated)\u001b[0m\u001b[93m<think>\n",
      "Okay, so I need to figure out the critical temperature at which water turns into ice under 100 atm of pressure. Hmm, I remember that under normal atmospheric pressure (1 atm), water freezes at 0¬∞C. But when pressure increases, the freezing point might change. I think this has something to do with the phase diagram of water. \n",
      "\n",
      "First, I should recall what a phase diagram looks like for water. The phase diagram shows the states of matter (solid, liquid, gas) depending on temperature and pressure. For most substances, increasing pressure lowers the melting point, but water is a bit weird because it expands when it freezes. So, for water, increasing pressure should actually lower the melting point. Wait, no, wait. Since ice is less dense than liquid water, applying pressure makes it harder for water to freeze, right? So higher pressure would require lower temperatures to freeze. That would mean the melting point decreases with increasing pressure. So, at higher pressures, water remains liquid at lower temperatures than normal. But I need to confirm this.\n",
      "\n",
      "The phase diagram of water has a negative slope for the solid-liquid boundary. That means as pressure increases, the melting temperature decreases. For example, at 1 atm, it's 0¬∞C, but if pressure goes up, the temperature needed to freeze water would be lower. So at 100 atm, the freezing point should be below 0¬∞C. But by how much exactly?\n",
      "\n",
      "I think the Clausius-Clapeyron equation might be useful here. That equation relates the change in pressure and temperature along the phase boundary. The equation is dP/dT = (L)/(TŒîv), where L is the latent heat of fusion, and Œîv is the change in volume during the phase transition. If I can integrate this equation from the known point (1 atm, 273.15 K) to 100 atm, I can find the corresponding temperature.\n",
      "\n",
      "But wait, the Clausius-Clapeyron equation is derived for first-order phase transitions, which the solid-liquid transition is. So this should be applicable. However, the equation might require that L and Œîv are constant over the pressure and temperature range considered. I need to check if that's a valid assumption here. Since 100 atm isn't extremely high, maybe these values don't change too much, and we can approximate.\n",
      "\n",
      "First, let's note the parameters. For water, the latent heat of fusion L is about 333.55 kJ/kg. The change in volume Œîv when freezing is negative because the volume increases. The specific volume of liquid water at 0¬∞C is approximately 1.0002 √ó 10^-3 m¬≥/kg, and for ice, it's about 1.0907 √ó 10^-3 m¬≥/kg. So Œîv = v_ice - v_water = (1.0907 - 1.0002) √ó 10^-3 = 0.0905 √ó 10^-3 m¬≥/kg. But since this is per kg, and the equation uses molar quantities? Wait, no, the equation can be in terms of specific volume (per unit mass) as long as L is also per unit mass. Let me confirm the units.\n",
      "\n",
      "The Clausius-Clapeyron equation in terms of specific quantities would be dP/dT = L / (T (v_liquid - v_solid)). Wait, but since when freezing, the solid has higher volume, so v_solid > v_liquid, so Œîv = v_solid - v_liquid = positive, and L is the latent heat released when freezing (exothermic), so L is positive. Therefore, dP/dT = L / (T Œîv). Since Œîv is positive and L is positive, dP/dT is positive. But in the case of water, since Œîv is positive (ice expands), the slope dP/dT is positive. Wait, but earlier I thought the slope was negative. Wait, there's a confusion here. Let me get this straight.\n",
      "\n",
      "The slope of the solid-liquid boundary in the phase diagram is given by dP/dT = (L)/(T Œîv). For water, when going from liquid to solid, Œîv is positive (v increases), so Œîv = v_solid - v_liquid > 0. L is the latent heat of fusion, which is positive (energy released when freezing). Therefore, dP/dT = L/(T Œîv) is positive. However, in reality, the slope of the solid-liquid line for water is negative. That seems contradictory. Wait, maybe I messed up the sign convention. Let me check again.\n",
      "\n",
      "Wait, maybe the equation is written as dP/dT = (L)/(T (v_liquid - v_solid)). Because when you go from liquid to solid, the volume change is v_solid - v_liquid, but if the denominator is v_liquid - v_solid, that would make Œîv negative. Let me confirm the exact form of the Clausius-Clapeyron equation. \n",
      "\n",
      "The general Clausius-Clapeyron equation is (dP/dT) = (ŒîS)/(ŒîV), where ŒîS is the entropy change and ŒîV is the volume change for the phase transition. For the transition from liquid to solid, ŒîS is negative because the system becomes more ordered (entropy decreases), and ŒîV is negative because the volume decreases (but wait, in water, volume increases when freezing, so ŒîV is positive). Wait, now I'm getting confused again.\n",
      "\n",
      "Hold on. Let's define the transition as solid to liquid. Then, when melting, the system absorbs heat, so the entropy change ŒîS = S_liquid - S_solid > 0. The volume change ŒîV = V_liquid - V_solid. For water, V_liquid < V_solid, so ŒîV is negative. Therefore, (dP/dT) = (ŒîS)/(ŒîV). Since ŒîS is positive and ŒîV is negative, dP/dT is negative. That matches the negative slope on the phase diagram. So, in this case, using the solid to liquid transition, but we need to be careful with the direction.\n",
      "\n",
      "Alternatively, if we consider the liquid to solid transition (freezing), then ŒîS is negative and ŒîV is negative (since V decreases when freezing for most substances, but increases for water). Wait, no. For water, freezing (liquid to solid) causes an increase in volume, so ŒîV = V_solid - V_liquid > 0. The entropy change ŒîS = S_solid - S_liquid < 0 (since freezing releases entropy). Therefore, (dP/dT) = (ŒîS)/(ŒîV) = negative / positive = negative. Therefore, the slope dP/dT is negative, which aligns with the phase diagram of water. So, the slope is negative. Therefore, as pressure increases, the melting temperature decreases.\n",
      "\n",
      "Therefore, integrating the Clausius-Clapeyron equation will give us the relation between pressure and temperature along the phase boundary. So, to find the freezing temperature at 100 atm, we can start from the known point (T_0 = 273.15 K, P_0 = 1 atm) and integrate to P = 100 atm, finding the corresponding T.\n",
      "\n",
      "But to perform the integration, we need to know how L, ŒîV, and T vary with pressure and temperature. If we assume that L and ŒîV are constant over the temperature and pressure range considered, we can approximate the integral. Given that 100 atm is not extremely high (compared to, say, thousands of atm), this approximation might be reasonable, but there could be some error.\n",
      "\n",
      "Let me write the Clausius-Clapeyron equation again. For the solid-liquid transition (freezing), we have:\n",
      "\n",
      "dP/dT = (L) / (T ŒîV)\n",
      "\n",
      "But since the slope is negative, we have to be careful with signs. Wait, if we take L as the latent heat of fusion (which is energy released when freezing), then L is positive. ŒîV is the change in volume upon freezing, which for water is positive (since volume increases). Therefore, dP/dT = L/(T ŒîV) would be positive. But this contradicts the known negative slope. So where is the mistake here?\n",
      "\n",
      "Ah, perhaps the confusion is in the definition of L. When freezing, the process is exothermic, so the system releases heat. The latent heat L is typically defined as the heat required to melt the solid. So, L is positive for melting. Therefore, when considering freezing, the latent heat would be -L. Alternatively, the Clausius-Clapeyron equation should use the entropy change, which is ŒîS = L / T, where L is the latent heat of fusion (melting). For freezing, ŒîS would be negative.\n",
      "\n",
      "Let me approach this using entropy. The equation is:\n",
      "\n",
      "dP/dT = (ŒîS) / (ŒîV)\n",
      "\n",
      "For the transition from liquid to solid (freezing), ŒîS = S_solid - S_liquid = -L_fusion / T, since L_fusion is the heat absorbed when melting (opposite process). Therefore, ŒîS is negative. ŒîV is V_solid - V_liquid, which for water is positive. So, dP/dT = (-L / T) / (ŒîV) = - (L) / (T ŒîV). Therefore, the slope is negative, as expected.\n",
      "\n",
      "So the correct expression for dP/dT when considering the liquid-solid transition is:\n",
      "\n",
      "dP/dT = - (L) / (T ŒîV)\n",
      "\n",
      "But since we are moving along the phase boundary, we can treat this as a differential equation. Let's rearrange variables:\n",
      "\n",
      "dP = - (L / (T ŒîV)) dT\n",
      "\n",
      "But we need to integrate from P0 = 1 atm to P = 100 atm, and correspondingly from T0 = 273.15 K down to T. Since increasing pressure leads to decreasing melting temperature for water.\n",
      "\n",
      "Alternatively, if we let T decrease as P increases, the integral becomes:\n",
      "\n",
      "‚à´_{P0}^{P} dP' = - ‚à´_{T0}^{T} (L / (T' ŒîV)) dT'\n",
      "\n",
      "Assuming L and ŒîV are constant, which is an approximation, then:\n",
      "\n",
      "ŒîP = - (L / ŒîV) ‚à´_{T0}^{T} (1 / T') dT'\n",
      "\n",
      "ŒîP = - (L / ŒîV) ln(T / T0)\n",
      "\n",
      "But ŒîP here is P - P0 = 100 atm - 1 atm = 99 atm. Wait, but the left side is ŒîP = 99 atm, and the right side is - (L / ŒîV) ln(T / T0). Therefore:\n",
      "\n",
      "99 atm = - (L / ŒîV) ln(T / T0)\n",
      "\n",
      "But let's check the units. L is in J/kg, ŒîV is in m¬≥/kg, so L / ŒîV has units (J/kg) / (m¬≥/kg) = J/m¬≥ = Pa. So the right side needs to be in Pascals. Therefore, we need to convert the pressure difference from atm to Pascals.\n",
      "\n",
      "First, convert 99 atm to Pascals. 1 atm ‚âà 101325 Pa, so 99 atm ‚âà 99 * 101325 ‚âà 10,021,175 Pa ‚âà 1.002 √ó 10^7 Pa.\n",
      "\n",
      "Now, L = 333.55 kJ/kg = 333,550 J/kg.\n",
      "\n",
      "ŒîV = v_solid - v_liquid = 1.0907 √ó 10^-3 m¬≥/kg - 1.0002 √ó 10^-3 m¬≥/kg = 0.0905 √ó 10^-3 m¬≥/kg = 9.05 √ó 10^-5 m¬≥/kg.\n",
      "\n",
      "So, L / ŒîV = 333,550 / 9.05 √ó 10^-5 ‚âà 333,550 / 0.0000905 ‚âà 3.685 √ó 10^9 J/m¬≥.\n",
      "\n",
      "But wait, J/m¬≥ is equivalent to Pascals (since 1 J = 1 N¬∑m, so J/m¬≥ = N/m¬≤ = Pa). Therefore, L / ŒîV ‚âà 3.685 √ó 10^9 Pa.\n",
      "\n",
      "Now, plug into the equation:\n",
      "\n",
      "ŒîP = - (L / ŒîV) ln(T / T0)\n",
      "\n",
      "1.002 √ó 10^7 Pa = - (3.685 √ó 10^9 Pa) ln(T / 273.15)\n",
      "\n",
      "Divide both sides by -3.685 √ó 10^9 Pa:\n",
      "\n",
      "ln(T / 273.15) = - (1.002 √ó 10^7) / (3.685 √ó 10^9) ‚âà -0.002718\n",
      "\n",
      "Therefore:\n",
      "\n",
      "T / 273.15 = e^{-0.002718} ‚âà 0.99729\n",
      "\n",
      "Thus:\n",
      "\n",
      "T ‚âà 273.15 * 0.99729 ‚âà 272.36 K\n",
      "\n",
      "Convert that to Celsius: 272.36 K - 273.15 = -0.79¬∞C\n",
      "\n",
      "Wait, but this seems like a very small decrease in freezing temperature for 99 atm of pressure. From 0¬∞C to roughly -0.8¬∞C. But intuitively, I thought the effect would be more significant. Let me check if this makes sense.\n",
      "\n",
      "Looking up some data, for example, the phase diagram of water. At 100 atm, the melting point is indeed lower than 0¬∞C, but by how much? Some sources indicate that at around 100 MPa (which is about 1000 atm), the melting point is around -20¬∞C. But we're dealing with 100 atm, which is 10 MPa. So maybe a smaller depression. However, according to this calculation, it's only about -0.8¬∞C, but some references might show a larger depression. There might be an error in the calculation.\n",
      "\n",
      "Wait, perhaps the assumption that L and ŒîV are constant is not valid over this pressure range. Because even 100 atm might cause significant changes in the properties of water and ice. Alternatively, maybe the linear approximation isn't sufficient, and higher-order terms are needed.\n",
      "\n",
      "Alternatively, maybe the values used for ŒîV are not accurate under pressure. Let me check the specific volumes again. The specific volume of ice at 0¬∞C and 1 atm is about 1.0907 √ó 10^-3 m¬≥/kg, and liquid water is 1.0002 √ó 10^-3 m¬≥/kg. But under higher pressure, say 100 atm, the specific volume of ice and water would decrease. However, since we don't have exact data, perhaps the approximation is too rough.\n",
      "\n",
      "Alternatively, maybe integrating the Clausius-Clapeyron equation requires a different approach. Let's consider the equation again:\n",
      "\n",
      "dP/dT = - (L) / (T ŒîV)\n",
      "\n",
      "If we assume that ŒîV and L are constant, then we can separate variables:\n",
      "\n",
      "dP = - (L / ŒîV) (dT / T)\n",
      "\n",
      "Integrating both sides:\n",
      "\n",
      "‚à´_{P0}^{P} dP' = - (L / ŒîV) ‚à´_{T0}^{T} (1 / T') dT'\n",
      "\n",
      "Resulting in:\n",
      "\n",
      "P - P0 = - (L / ŒîV) ln(T / T0)\n",
      "\n",
      "Which is the same as before. So the calculation seems correct under the assumption that L and ŒîV are constants.\n",
      "\n",
      "But given that the result seems small, maybe the problem is in the units. Let me check the calculation again.\n",
      "\n",
      "ŒîP = 99 atm ‚âà 10,021,175 Pa\n",
      "\n",
      "L = 333,550 J/kg\n",
      "\n",
      "ŒîV = 9.05 √ó 10^-5 m¬≥/kg\n",
      "\n",
      "So L / ŒîV = 333,550 / 9.05e-5 ‚âà 333550 / 0.0000905 ‚âà Let's compute that:\n",
      "\n",
      "0.0000905 = 9.05e-5\n",
      "\n",
      "So 333550 / 9.05e-5\n",
      "\n",
      "Calculate denominator: 9.05e-5 = 0.0000905\n",
      "\n",
      "Divide 333550 by 0.0000905:\n",
      "\n",
      "First, 333550 / 0.0000905 ‚âà 333550 / 0.00009 ‚âà 3,705,555,555.56 ‚âà 3.7055e9 Pa\n",
      "\n",
      "But 0.0000905 is slightly more than 0.00009, so the result is slightly less: 3.7055e9 / (9.05/9) ‚âà 3.7055e9 * 9/9.05 ‚âà 3.7055e9 * 0.9945 ‚âà 3.685e9 Pa, which matches previous calculation.\n",
      "\n",
      "So 10,021,175 Pa = - (3.685e9 Pa) * ln(T/273.15)\n",
      "\n",
      "Thus, ln(T/273.15) = -10,021,175 / 3.685e9 ‚âà -0.002718\n",
      "\n",
      "Exponential of that is approximately 0.99729, so T ‚âà 273.15 * 0.99729 ‚âà 272.36 K, which is -0.79¬∞C.\n",
      "\n",
      "But according to some phase diagrams of water, at 100 atm, the melting point is lower than that. For example, one source I recall shows that at about 100 MPa (which is ~987 atm), the melting point is around -20¬∞C. So at 100 atm (10.1 MPa), the melting point depression should be smaller, but perhaps more than 1¬∞C. Wait, this calculation gives only -0.79¬∞C for 100 atm. Maybe the assumption of constant ŒîV and L is not valid here. Because under higher pressure, the specific volume of ice and water would decrease, and the latent heat might also change.\n",
      "\n",
      "Alternatively, maybe the linear approximation using the Clausius-Clapeyron equation is insufficient for such a large pressure change. The equation is more accurate for small changes in pressure and temperature. When dealing with a pressure increase of 100 atm, the changes might be too large for the approximation to hold.\n",
      "\n",
      "Alternatively, perhaps I should use a different approach. For example, using the empirical equation for the melting curve of ice. The Simon equation is an empirical relation that describes the melting curve of a solid: \n",
      "\n",
      "P = P0 + a[(T/T0)^c - 1]\n",
      "\n",
      "Where P0, a, c are constants. For water, I need to find the parameters for the Simon equation.\n",
      "\n",
      "Looking up, for ice I (the common form of ice), the Simon equation parameters might be available. For example, one source gives the melting curve of ice I as:\n",
      "\n",
      "P = 1 + (T/251.165)^5.511 - 1) * 395.2 MPa\n",
      "\n",
      "Wait, that might not be correct. Alternatively, the Simon equation is often written as:\n",
      "\n",
      "P = P0 + a*( (T / T0)^c - 1 )\n",
      "\n",
      "Where P0 is the reference pressure (1 atm), T0 is the reference temperature (273.15 K), and a and c are constants. However, finding the exact parameters for ice I might require looking up literature values.\n",
      "\n",
      "Alternatively, another empirical equation is the following for the melting temperature of ice under pressure:\n",
      "\n",
      "T_m = T0 - (K * P)\n",
      "\n",
      "Where K is a constant. But this would be a linear approximation, which might be valid for small pressures, but not necessarily up to 100 atm.\n",
      "\n",
      "Alternatively, I found a reference that states the melting temperature of ice decreases by approximately 0.0072¬∞C per atmosphere increase. So for 100 atm, that would be a decrease of 0.72¬∞C, leading to a melting point of -0.72¬∞C. This is close to our previous calculation of -0.79¬∞C. So maybe the approximation is reasonable.\n",
      "\n",
      "Another source mentions that the melting point of ice decreases by about 0.007 K/atm. So for 100 atm, that's 0.7 K, so from 273.15 K to 272.45 K, which is -0.7¬∞C. This aligns with the previous results. So perhaps the answer is approximately -0.7¬∞C.\n",
      "\n",
      "But wait, some sources might give different coefficients. For instance, another reference suggests that the depression is about 0.0098¬∞C per atm, which would lead to a depression of 0.98¬∞C for 100 atm. This discrepancy might be due to different experimental conditions or approximations.\n",
      "\n",
      "Alternatively, let's check the CRC Handbook or a similar resource. According to the CRC Handbook of Chemistry and Physics, the melting point of ice under pressure can be calculated using the following data. However, I don't have immediate access to it, but based on general knowledge, the approximate value seems to be around -0.7 to -1¬∞C for 100 atm.\n",
      "\n",
      "Alternatively, using the Clausius-Clapeyron equation with more accurate values. Let me double-check the latent heat and the specific volumes. The latent heat of fusion for water is indeed approximately 333.55 kJ/kg at 0¬∞C and 1 atm. The specific volume of ice at 0¬∞C is 1.0907 √ó 10^-3 m¬≥/kg, and liquid water is 1.0002 √ó 10^-3 m¬≥/kg, giving ŒîV = 0.0905 √ó 10^-3 m¬≥/kg. However, under pressure, the specific volumes would change. For example, under 100 atm, water is more compressed, so the specific volume of liquid water would decrease. Similarly, ice's specific volume would also decrease, but perhaps not as much as water. This would affect ŒîV.\n",
      "\n",
      "Assuming that under pressure, the compressibility of water is higher than that of ice, the ŒîV (v_ice - v_water) might decrease with pressure. If ŒîV decreases, then the denominator in the Clausius-Clapeyron equation becomes smaller, leading to a larger magnitude of dP/dT (more negative), which would mean a steeper slope. Therefore, the previous calculation assuming constant ŒîV might overestimate T (i.e., the melting point would be lower than calculated). But without exact data on how ŒîV changes with pressure, it's hard to adjust.\n",
      "\n",
      "Alternatively, considering the bulk modulus. The bulk modulus K is defined as K = -V (dP/dV). For water, the bulk modulus is about 2.15 √ó 10^9 Pa at 1 atm. For ice, it's about 8.8 √ó 10^9 Pa. This means that water is more compressible than ice. Therefore, under pressure, the specific volume of water decreases more than that of ice, leading to a decrease in ŒîV = v_ice - v_water. Let's see.\n",
      "\n",
      "If at 1 atm, ŒîV = 0.0905 √ó 10^-3 m¬≥/kg. Under pressure P, the specific volume of water would decrease by Œîv_water ‚âà -v_water0 * (P - P0)/K_water, and similarly for ice, Œîv_ice ‚âà -v_ice0 * (P - P0)/K_ice. \n",
      "\n",
      "Given that K_water ‚âà 2.15e9 Pa, K_ice ‚âà 8.8e9 Pa. The pressure change is ŒîP = 99 atm ‚âà 1e7 Pa.\n",
      "\n",
      "So, Œîv_water ‚âà -1.0002e-3 m¬≥/kg * (1e7 Pa) / (2.15e9 Pa) ‚âà -1.0002e-3 * 0.00465 ‚âà -4.65e-6 m¬≥/kg\n",
      "\n",
      "Œîv_ice ‚âà -1.0907e-3 m¬≥/kg * (1e7 Pa) / (8.8e9 Pa) ‚âà -1.0907e-3 * 0.001136 ‚âà -1.24e-6 m¬≥/kg\n",
      "\n",
      "Therefore, the new specific volumes at 100 atm would be:\n",
      "\n",
      "v_water ‚âà 1.0002e-3 - 4.65e-6 ‚âà 0.99555e-3 m¬≥/kg\n",
      "\n",
      "v_ice ‚âà 1.0907e-3 - 1.24e-6 ‚âà 1.08946e-3 m¬≥/kg\n",
      "\n",
      "Therefore, ŒîV_new = 1.08946e-3 - 0.99555e-3 ‚âà 0.09391e-3 m¬≥/kg\n",
      "\n",
      "Wait, this is actually a slight increase in ŒîV compared to the original 0.0905e-3 m¬≥/kg. Hmm, that seems counterintuitive. Wait, under pressure, water compresses more than ice, so the specific volume of water decreases more, so v_ice - v_water increases? Let me check the numbers again.\n",
      "\n",
      "Original ŒîV = 0.0905e-3 m¬≥/kg.\n",
      "\n",
      "After compression:\n",
      "\n",
      "v_water ‚âà 1.0002e-3 - 4.65e-6 = 0.99555e-3 m¬≥/kg\n",
      "\n",
      "v_ice ‚âà 1.0907e-3 - 1.24e-6 = 1.08946e-3 m¬≥/kg\n",
      "\n",
      "ŒîV_new = 1.08946e-3 - 0.99555e-3 = 0.09391e-3 m¬≥/kg\n",
      "\n",
      "So ŒîV increases by about 3.7%. Therefore, using this adjusted ŒîV in the Clausius-Clapeyron equation would lead to a smaller magnitude of dP/dT (since ŒîV is larger), which would mean a less steep slope. Therefore, the previous calculation assuming constant ŒîV would overestimate the depression (i.e., predict a lower temperature than actual). Wait, but this is a small effect (3.7% change in ŒîV), so the correction would be minor.\n",
      "\n",
      "Let me recalculate with ŒîV_new = 0.09391e-3 m¬≥/kg.\n",
      "\n",
      "Then, L / ŒîV_new = 333,550 / 0.09391e-3 ‚âà 333,550 / 0.00009391 ‚âà 3.55e9 Pa\n",
      "\n",
      "Then, ŒîP = 1.002e7 Pa = - (3.55e9 Pa) ln(T / 273.15)\n",
      "\n",
      "So, ln(T / 273.15) = -1.002e7 / 3.55e9 ‚âà -0.002823\n",
      "\n",
      "Thus, T / 273.15 ‚âà e^{-0.002823} ‚âà 0.99719\n",
      "\n",
      "So, T ‚âà 273.15 * 0.99719 ‚âà 272.38 K, which is -0.77¬∞C\n",
      "\n",
      "This is only a slight change from the previous -0.79¬∞C. So even accounting for the compressibility, the temperature depression is around -0.77¬∞C, which is still approximately -0.8¬∞C.\n",
      "\n",
      "Therefore, based on the Clausius-Clapeyron equation with adjusted ŒîV, the freezing point at 100 atm is approximately -0.8¬∞C. However, empirical data might slightly differ. For example, if the depression is about 0.0072¬∞C per atm, then 100 atm would give -0.72¬∞C. Both results are in the same ballpark.\n",
      "\n",
      "Alternatively, some sources provide a table or equation for the melting curve. For instance, the International Association for the Properties of Water and Steam (IAPWS) might have a more accurate formulation. However, without accessing those specialized resources, we rely on the Clausius-Clapeyron approximation.\n",
      "\n",
      "Another factor to consider is the variation of latent heat with pressure. The latent heat of fusion might decrease with increasing pressure. If L decreases, then the numerator in the Clausius-Clapeyron equation decreases, leading to a less negative dP/dT, meaning the slope is less steep, so the temperature depression would be smaller. However, data on how L changes with pressure is needed. If we assume L remains roughly constant, the approximation holds.\n",
      "\n",
      "In summary, using the Clausius-Clapeyron equation with reasonable assumptions about constant L and ŒîV, we estimate the freezing temperature at 100 atm to be approximately -0.8¬∞C. Empirical coefficients from literature suggesting about -0.7¬∞C to -0.9¬∞C support this result. Therefore, the critical temperature is around -0.8¬∞C.\n",
      "</think>\n",
      "\n",
      "To determine the critical temperature at which water freezes under 100 atm pressure, we analyze the phase transition using the Clausius-Clapeyron equation from condensed matter physics. Here's the systematic solution:\n",
      "\n",
      "### Key Analysis:\n",
      "1. **Phase Diagram Insight**:  \n",
      "   Water's solid-liquid phase boundary has a negative slope, meaning increasing pressure lowers the freezing temperature. This arises because ice (solid) is less dense than liquid water, causing volume expansion upon freezing.\n",
      "\n",
      "2. **Clausius-Clapeyron Equation**:  \n",
      "   The equation governing the phase boundary is:  \n",
      "   \\[\n",
      "   \\frac{dP}{dT} = -\\frac{L}{T \\Delta V}\n",
      "   \\]  \n",
      "   where:\n",
      "   - \\(L = 333.55 \\, \\text{kJ/kg}\\) (latent heat of fusion),\n",
      "   - \\(\\Delta V = v_{\\text{ice}} - v_{\\text{water}} = 9.05 \\times 10^{-5} \\, \\text{m}^3/\\text{kg}\\) (volume change),\n",
      "   - \\(T_0 = 273.15 \\, \\text{K}\\) (freezing point at 1 atm).\n",
      "\n",
      "3. **Integration Under Assumptions**:  \n",
      "   Assuming \\(L\\) and \\(\\Delta V\\) are constant over the pressure range, integrate from \\(P_0 = 1 \\, \\text{atm}\\) to \\(P = 100 \\, \\text{atm}\\):  \n",
      "   \\[\n",
      "   \\Delta P = -\\frac{L}{\\Delta V} \\ln\\left(\\frac{T}{T_0}\\right).\n",
      "   \\]  \n",
      "   Substituting \\(\\Delta P = 99 \\, \\text{atm} \\approx 1.002 \\times 10^7 \\, \\text{Pa}\\), solve for \\(T\\):  \n",
      "   \\[\n",
      "   \\ln\\left(\\frac{T}{273.15}\\right) = -\\frac{\\Delta P \\Delta V}{L} \\approx -0.002718 \\implies T \\approx 272.36 \\, \\text{K}.\n",
      "   \\]\n",
      "\n",
      "4. **Result**:  \n",
      "   Converting to Celsius:  \n",
      "   \\[\n",
      "   T \\approx 272.36 \\, \\text{K} - 273.15 = -0.79^\\circ \\text{C}.\n",
      "   \\]\n",
      "\n",
      "5. **Validation and Adjustments**:  \n",
      "   - Compressibility corrections slightly reduce \\(\\Delta V\\), leading to \\(T \\approx -0.77^\\circ \\text{C}\\).  \n",
      "   - Empirical coefficients (e.g., \\(0.0072^\\circ \\text{C/atm}\\)) align closely, yielding \\(T \\approx -0.72^\\circ \\text{C}\\).  \n",
      "\n",
      "### Conclusion:  \n",
      "The critical temperature at which water freezes under 100 atm pressure is **approximately \\(-0.8^\\circ \\text{C}\\)**, accounting for theoretical and empirical consistency.\n",
      "\n",
      "**Final Answer:**  \n",
      "The freezing temperature of water at 100 atm is \\(\\boxed{-0.8^\\circ \\text{C}}\\).<|im_end|>\n",
      "... (truncated)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "\u001b[32m06:34:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:28\u001b[0m | \u001b[1müìã Dataloader examples logged to .log/dataloader_examples.html\u001b[0m\n",
      "\u001b[32m06:34:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1müé≤ Sampler epoch 0: emitting 1000 indices\n",
      "First ids: [776, 507, 895, 922, 33, 483, 85, 750, 354, 523]\n",
      "...Last ids: [104, 754, 142, 228, 250, 281, 759, 25, 114, 654]\u001b[0m\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "  2%|‚ñè         | 3/189 [00:30<25:41,  8.29s/it]Process Process-2:\n",
      "Process Process-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anhvth5/miniconda3/envs/hypersloth_env/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/anhvth5/miniconda3/envs/hypersloth_env/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/anhvth5/projects/hypersloth/HyperSloth/scripts/hp_trainer.py\", line 69, in train_on_single_gpu\n",
      "    trainer.train()\n",
      "  File \"/home/anhvth5/miniconda3/envs/hypersloth_env/lib/python3.11/site-packages/transformers/trainer.py\", line 2240, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/projects/hypersloth/HyperSloth/patching/inner_training_loop.py\", line 516, in _inner_training_loop\n",
      "    self._maybe_log_save_evaluate(\n",
      "  File \"/home/anhvth5/miniconda3/envs/hypersloth_env/lib/python3.11/site-packages/transformers/trainer.py\", line 3091, in _maybe_log_save_evaluate\n",
      "    self.log(logs, start_time)\n",
      "  File \"/home/anhvth5/projects/hypersloth/HyperSloth/patching/patch_log.py\", line 181, in log\n",
      "    _write_logs_to_mmap(logs, support_keys, LOG_MMAP, LOG_LOCKS, LOCAL_RANK)\n",
      "  File \"/home/anhvth5/projects/hypersloth/HyperSloth/patching/patch_log.py\", line 283, in _write_logs_to_mmap\n",
      "    with log_locks[key]:\n",
      "  File \"/home/anhvth5/miniconda3/envs/hypersloth_env/lib/python3.11/site-packages/filelock/_api.py\", line 297, in __enter__\n",
      "    self.acquire()\n",
      "  File \"/home/anhvth5/miniconda3/envs/hypersloth_env/lib/python3.11/site-packages/filelock/_api.py\", line 267, in acquire\n",
      "    time.sleep(poll_interval)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  2%|‚ñè         | 3/189 [00:30<31:51, 10.28s/it]\n",
      "  File \"/home/anhvth5/miniconda3/envs/hypersloth_env/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/anhvth5/miniconda3/envs/hypersloth_env/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/anhvth5/projects/hypersloth/HyperSloth/scripts/hp_trainer.py\", line 69, in train_on_single_gpu\n",
      "    trainer.train()\n",
      "  File \"/home/anhvth5/miniconda3/envs/hypersloth_env/lib/python3.11/site-packages/transformers/trainer.py\", line 2240, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/projects/hypersloth/HyperSloth/patching/inner_training_loop.py\", line 516, in _inner_training_loop\n",
      "    self._maybe_log_save_evaluate(\n",
      "  File \"/home/anhvth5/miniconda3/envs/hypersloth_env/lib/python3.11/site-packages/transformers/trainer.py\", line 3091, in _maybe_log_save_evaluate\n",
      "    self.log(logs, start_time)\n",
      "  File \"/home/anhvth5/projects/hypersloth/HyperSloth/patching/patch_log.py\", line 181, in log\n",
      "    _write_logs_to_mmap(logs, support_keys, LOG_MMAP, LOG_LOCKS, LOCAL_RANK)\n",
      "  File \"/home/anhvth5/projects/hypersloth/HyperSloth/patching/patch_log.py\", line 285, in _write_logs_to_mmap\n",
      "    log_mmap[key].flush()\n",
      "  File \"/home/anhvth5/miniconda3/envs/hypersloth_env/lib/python3.11/site-packages/numpy/_core/memmap.py\", line 329, in flush\n",
      "    self.base.flush()\n",
      "KeyboardInterrupt\n",
      "  2%|‚ñè         | 3/189 [00:30<31:39, 10.21s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     43\u001b[39m training_config_model = TrainingArgsConfig(\n\u001b[32m     44\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33moutputs/qwen3-8b-openthought-2gpus/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     45\u001b[39m     per_device_train_batch_size=\u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     56\u001b[39m     report_to=\u001b[33m\"\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# tensorboard or wawndb\u001b[39;00m\n\u001b[32m     57\u001b[39m )\n\u001b[32m     59\u001b[39m setup_envs(hyper_config_model, training_config_model)\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[43mrun_mp_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhyper_config_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyper_config_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_config_model\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/hypersloth/HyperSloth/scripts/hp_trainer.py:241\u001b[39m, in \u001b[36mrun_mp_training\u001b[39m\u001b[34m(gpus, hyper_config, training_config)\u001b[39m\n\u001b[32m    239\u001b[39m                 processes.remove(proc)\n\u001b[32m    240\u001b[39m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m     time.sleep(\u001b[32m1\u001b[39m)\n\u001b[32m    242\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAll processes finished\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "More training debug examples written to .log/dataloader_examples.html\n",
      "Unsloth: Will smartly offload gradients to save VRAM!\n",
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W609 06:35:12.430246547 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "[rank1]:[W609 06:35:12.463359715 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "[rank1]:[W609 06:35:13.023497534 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=65, addr=[localhost]:56552, remote=[localhost]:29501): failed to recv, got 0 bytes\n",
      "Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7eba767785e8 in /home/anhvth5/miniconda3/envs/hypersloth_env/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x5ba8afe (0x7eba5f3a8afe in /home/anhvth5/miniconda3/envs/hypersloth_env/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x5baae40 (0x7eba5f3aae40 in /home/anhvth5/miniconda3/envs/hypersloth_env/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x5bab74a (0x7eba5f3ab74a in /home/anhvth5/miniconda3/envs/hypersloth_env/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7eba5f3a51a9 in /home/anhvth5/miniconda3/envs/hypersloth_env/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7eba1c3e09a9 in /home/anhvth5/miniconda3/envs/hypersloth_env/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0xdbbf4 (0x7eba0bedbbf4 in /home/anhvth5/miniconda3/envs/hypersloth_env/bin/../lib/libstdc++.so.6)\n",
      "frame #7: <unknown function> + 0x94ac3 (0x7eba78d09ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #8: clone + 0x44 (0x7eba78d9aa04 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "[rank1]:[W609 06:35:13.038883652 ProcessGroupNCCL.cpp:1659] [PG ID 0 PG GUID 0(default_pg) Rank 1] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes\n"
     ]
    }
   ],
   "source": [
    "from HyperSloth.hypersloth_config import *\n",
    "from HyperSloth.scripts.hp_trainer import run_mp_training, setup_envs\n",
    "\n",
    "# Main configuration using Pydantic models\n",
    "hyper_config_model = HyperConfig(\n",
    "    data=HFDatasetConfig(\n",
    "        dataset_name=\"llamafactory/OpenThoughts-114k\",\n",
    "        split=\"train\",\n",
    "        tokenizer_name=\"Qwen/Qwen3-8B\",  # does not matter same family qwen3\n",
    "        num_samples=1000,\n",
    "        instruction_part=\"<|im_start|>user\\n\",\n",
    "        response_part=\"<|im_start|>assistant\\n\",\n",
    "        chat_template=\"chatml\",\n",
    "    ),\n",
    "    training=TrainingConfig(\n",
    "        gpus=[0, 1],\n",
    "        loss_type=\"response_only\",\n",
    "    ),\n",
    "    fast_model_args=FastModelArgs(\n",
    "        model_name=\"unsloth/Qwen3-0.6b-bnb-4bit\",\n",
    "        max_seq_length=32_000,\n",
    "        load_in_4bit=True,\n",
    "    ),\n",
    "    lora_args=LoraArgs(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "        use_rslora=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Training arguments using Pydantic model\n",
    "training_config_model = TrainingArgsConfig(\n",
    "    output_dir=\"outputs/qwen3-8b-openthought-2gpus/\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-5,\n",
    "    logging_steps=3,\n",
    "    num_train_epochs=3,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=5,\n",
    "    save_total_limit=2,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_8bit\",\n",
    "    seed=3407,\n",
    "    report_to=\"none\",  # tensorboard or wawndb\n",
    ")\n",
    "\n",
    "setup_envs(hyper_config_model, training_config_model)\n",
    "\n",
    "run_mp_training(\n",
    "    hyper_config_model.training.gpus, hyper_config_model, training_config_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1d9548",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Training Arguments\n",
    "\n",
    "Configure the training hyperparameters for optimal performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b45e70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ee347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
