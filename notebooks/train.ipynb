{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1555d26d",
   "metadata": {},
   "source": [
    "# üöÄ OpenSloth Demo Training Notebook\n",
    "\n",
    "This notebook demonstrates how to fine-tune large language models using opensloth's multi-GPU capabilities. It's equivalent to running:\n",
    "\n",
    "```bash\n",
    "opensloth-train examples/example_sharegpt_lora_2gpus.py\n",
    "```\n",
    "\n",
    "## What This Demo Does\n",
    "\n",
    "- **Multi-GPU Training**: Uses 2 GPUs with NCCL synchronization\n",
    "- **Adaptive Batching**: Optimizes sequence sorting and padding\n",
    "- **LoRA Fine-tuning**: Efficient parameter updates with Low-Rank Adaptation\n",
    "- **Response-only Loss**: Calculates loss only on assistant responses\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. opensloth installed: `pip install git+https://github.com/anhvth/opensloth.git`\n",
    "2. At least 2 GPUs available (adjust `gpus=[0, 1]` if needed)\n",
    "3. Sufficient VRAM (reduce batch size if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b606c272",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration Setup\n",
    "\n",
    "HyperSloth uses Pydantic models for type-safe configuration. We'll set up:\n",
    "\n",
    "1. **Data Configuration**: Dataset and tokenization settings\n",
    "2. **Training Configuration**: GPU allocation and loss calculation\n",
    "3. **Model Configuration**: Base model and LoRA parameters\n",
    "4. **Training Arguments**: Learning rate, batch size, and optimization settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6008af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global batch size: 64\n",
      "[MP] Running on 2 GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m16:18:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mopensloth_sft_trainer.py:41\u001b[0m | \u001b[1mTraining on GPU 0 with output_dir outputs/exps/qwen3-14b-FineTome-2gpus-seql-packing\u001b[0m\n",
      "\u001b[32m16:18:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mopensloth_sft_trainer.py:44\u001b[0m | \u001b[1müöÄ Starting total training timer\u001b[0m\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "Using compiler location: .cache/unsloth_compiled_cache_0\n",
      "==((====))==  Unsloth 2025.5.9: Fast Qwen3 patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.189 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "Using compiler location: .cache/unsloth_compiled_cache_1\n",
      "==((====))==  Unsloth 2025.5.9: Fast Qwen3 patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.189 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.27s/it]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.25s/it]\n",
      "\u001b[32m16:19:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:161\u001b[0m | \u001b[1m‚è±Ô∏è  model_loading: 11.96s\u001b[0m\n",
      "\u001b[32m16:19:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mnccl_grad_sync.py:124\u001b[0m | \u001b[1m[GPU=0] NCCL env: RANK=0, WORLD_SIZE=2, MASTER_ADDR=127.0.0.1, MASTER_PORT=29501\u001b[0m\n",
      "\u001b[32m16:19:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mnccl_grad_sync.py:128\u001b[0m | \u001b[1m[GPU=0] Setting current CUDA device to:0, os.environ['CUDA_VISIBLE_DEVICES']='0'\u001b[0m\n",
      "\u001b[32m16:19:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:50\u001b[0m | \u001b[1mModel loaded on device cuda:0, tokenizer: Qwen2TokenizerFast\u001b[0m\n",
      "\u001b[32m16:19:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:161\u001b[0m | \u001b[1m‚è±Ô∏è  lora_setup: 5.67s\u001b[0m\n",
      "\u001b[32m16:19:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:74\u001b[0m | \u001b[1mApplied chat template: qwen3\u001b[0m\n",
      "\u001b[32m16:19:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:161\u001b[0m | \u001b[1m‚è±Ô∏è  model_init: 18.70s\u001b[0m\n",
      "\u001b[32m16:19:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mdataset_utils.py:232\u001b[0m | \u001b[1mDataset 4769b8ce4d4eb55c being prepared by another process, waiting...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset (num_proc=52): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:11<00:00, 861.23 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:00<00:00, 81555.71 examples/s]\n",
      "\u001b[32m16:19:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:148\u001b[0m | \u001b[1mCreating final SFTTrainer with prepared dataset...\u001b[0m\n",
      "\u001b[32m16:19:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:161\u001b[0m | \u001b[1mReplacing DataCollatorForLanguageModeling with DataCollatorForSeq2Seq for better sequence handling\u001b[0m\n",
      "\u001b[32m16:19:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:169\u001b[0m | \u001b[1mTrainer setup completed successfully\u001b[0m\n",
      "\u001b[32m16:19:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:161\u001b[0m | \u001b[1m‚è±Ô∏è  trainer_setup: 23.11s\u001b[0m\n",
      "\u001b[32m16:19:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:122\u001b[0m | \u001b[1mAdd callback ShuffleData to Trainer UnslothSFTTrainer\u001b[0m\n",
      "\u001b[32m16:19:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:161\u001b[0m | \u001b[1m‚è±Ô∏è  trainer_creation: 23.15s\u001b[0m\n",
      "\u001b[32m16:19:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:161\u001b[0m | \u001b[1m‚è±Ô∏è  total_setup: 54.43s\u001b[0m\n",
      "\u001b[32m16:19:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:161\u001b[0m | \u001b[1m‚è±Ô∏è  model_and_training_setup: 54.45s\u001b[0m\n",
      "\u001b[32m16:19:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mnccl_grad_sync.py:48\u001b[0m | \u001b[1m[GPU=0] NCCLGradSyncCallback initialized for rank 0/2\u001b[0m\n",
      "\u001b[32m16:19:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mopensloth_sft_trainer.py:67\u001b[0m | \u001b[1mUsing gradient sync callback for GPU 0\u001b[0m\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "  0%|          | 0/157 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOCAL_RANK=1] Patching log. Dir: outputs/exps/qwen3-14b-FineTome-2gpus-seql-packing, GPUs: 2\n",
      "[LOCAL_RANK=1] Log patch initialization complete.\n",
      "üîß Patching Trainer to use RandomSamplerSeededByEpoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "wandb: Currently logged in as: anhvth to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.11\n",
      "wandb: Run data is saved locally in /home/anhvth5/projects/opensloth/wandb/run-20250609_161932-fu95llxn\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run outputs/exps/qwen3-14b-FineTome-2gpus-seql-packing\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/anhvth/huggingface\n",
      "wandb: üöÄ View run at https://wandb.ai/anhvth/huggingface/runs/fu95llxn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "[LOCAL_RANK=0] Patching log. Dir: outputs/exps/qwen3-14b-FineTome-2gpus-seql-packing, GPUs: 2\n",
      "[LOCAL_RANK=0] Log patch initialization complete.\n",
      "üîß Patching Trainer to use RandomSamplerSeededByEpoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/157 [00:00<?, ?it/s]\u001b[32m16:19:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:21\u001b[0m | \u001b[1müîÑ Starting epoch 1\u001b[0m\n",
      "\u001b[32m16:19:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1müé≤ Sampler epoch 0: emitting 10000 indices\n",
      "First ids dataset samples: [3771, 6672, 7261, 760, 3779, 1772, 7509, 2679, 2305, 9215]\n",
      "...Last ids: [9674, 1424, 8935, 1679, 2286, 3657, 4012, 4506, 409, 1824]\u001b[0m\n",
      "\u001b[32m16:19:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:28\u001b[0m | \u001b[1müìã Dataloader examples logged to .log/dataloader_examples.html\u001b[0m\n",
      "\u001b[32m16:19:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1müé≤ Sampler epoch 0: emitting 10000 indices\n",
      "First ids dataset samples: [3771, 6672, 7261, 760, 3779, 1772, 7509, 2679, 2305, 9215]\n",
      "...Last ids: [9674, 1424, 8935, 1679, 2286, 3657, 4012, 4506, 409, 1824]\u001b[0m\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "  6%|‚ñã         | 10/157 [02:35<30:39, 12.51s/it] "
     ]
    }
   ],
   "source": [
    "from opensloth.scripts.opensloth_sft_trainer import run_mp_training, setup_envs\n",
    "from opensloth.opensloth_config import (\n",
    "    OpenSlothConfig,\n",
    "    HFDatasetConfig,\n",
    "    FastModelArgs,\n",
    "    LoraArgs,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from loguru import logger\n",
    "\n",
    "# from transformers.training_args import TrainingArguments\n",
    "\n",
    "\n",
    "# # Main configuration using Pydantic models\n",
    "def get_configs(devices) -> tuple[OpenSlothConfig, TrainingArguments]:\n",
    "    num_gpu = len(devices)\n",
    "    opensloth_config = OpenSlothConfig(\n",
    "        data=HFDatasetConfig(\n",
    "            tokenizer_name=\"Qwen/Qwen3-8B\",\n",
    "            chat_template=\"qwen3\",\n",
    "            instruction_part=\"<|im_start|>user\\n\",\n",
    "            response_part=\"<|im_start|>assistant\\n\",\n",
    "            num_samples=10000,\n",
    "            nproc=52,\n",
    "            max_seq_length=4096,\n",
    "            source_type=\"hf\",\n",
    "            dataset_name=\"mlabonne/FineTome-100k\",\n",
    "            split=\"train\",\n",
    "        ),\n",
    "        devices=devices,  # list of int representing GPU ids\n",
    "        fast_model_args=FastModelArgs(\n",
    "            model_name=\"model_store/unsloth/Qwen3-14B-bnb-4bit\",\n",
    "            max_seq_length=4096,\n",
    "            load_in_4bit=True,\n",
    "        ),\n",
    "        lora_args=LoraArgs(\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            target_modules=[\n",
    "                \"q_proj\",\n",
    "                \"k_proj\",\n",
    "                \"v_proj\",\n",
    "                \"o_proj\",\n",
    "                \"gate_proj\",\n",
    "                \"up_proj\",\n",
    "                \"down_proj\",\n",
    "            ],\n",
    "            lora_dropout=0,\n",
    "            bias=\"none\",\n",
    "            use_rslora=False,\n",
    "        ),\n",
    "        sequence_packing=True,\n",
    "    )\n",
    "\n",
    "    # # Training arguments using Pydantic model\n",
    "    training_config = TrainingArguments(\n",
    "        output_dir=f\"outputs/exps/qwen3-14b-FineTome-{num_gpu}gpus-seql-packing\",\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=8,  # Adjust based on n_gpu\n",
    "        learning_rate=1e-5,\n",
    "        logging_steps=1,\n",
    "        num_train_epochs=1,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        warmup_steps=5,\n",
    "        save_total_limit=1,\n",
    "        weight_decay=0.01,\n",
    "        optim=\"adamw_8bit\",\n",
    "        seed=3407,\n",
    "        report_to=\"wandb\",  # tensorboard or wawndb\n",
    "    )\n",
    "    setup_envs(opensloth_config, training_config)\n",
    "    return opensloth_config, training_config\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    opensloth_config, training_config = get_configs(devices=[0,2])\n",
    "    run_mp_training(opensloth_config.devices, opensloth_config, training_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1985db",
   "metadata": {},
   "source": [
    "## Unsloth default "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f33c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from opensloth.patching.patch_sampler import patch_sampler\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"HYPERSLOTH_LOCAL_RANK\"] = \"0\"\n",
    "\n",
    "\n",
    "def train_qwen3_model():\n",
    "    \"\"\"Train Qwen3 model with minimal setup.\"\"\"\n",
    "    from opensloth.dataset_utils import get_tokenized_dataset, HFDatasetConfig\n",
    "\n",
    "    text_dataset = get_tokenized_dataset(\n",
    "        HFDatasetConfig(\n",
    "            tokenizer_name=\"Qwen/Qwen3-8B\",\n",
    "            chat_template=\"qwen3\",\n",
    "            instruction_part=\"<|im_start|>user\\n\",\n",
    "            response_part=\"<|im_start|>assistant\\n\",\n",
    "            num_samples=10000,\n",
    "            nproc=52,\n",
    "            max_seq_length=4096,\n",
    "            source_type=\"hf\",\n",
    "            dataset_name=\"mlabonne/FineTome-100k\",\n",
    "            split=\"train\",\n",
    "        ),\n",
    "        do_tokenize=False,\n",
    "    )\n",
    "    from unsloth import FastLanguageModel\n",
    "    import torch\n",
    "    from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "    # Load model\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"unsloth/Qwen3-0.6B-bnb-4bit\",\n",
    "        max_seq_length=4096,\n",
    "        load_in_4bit=True,\n",
    "        load_in_8bit=False,\n",
    "        full_finetuning=False,\n",
    "    )\n",
    "\n",
    "    # Add LoRA adapters\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=True,\n",
    "        random_state=3407,\n",
    "        use_rslora=False,\n",
    "        loftq_config=None,\n",
    "    )\n",
    "    args = SFTConfig(\n",
    "        output_dir=\"outputs/exps/qwen3-14b-FineTome-unsloth\",\n",
    "        dataset_text_field=\"text\",\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=8*2, # Adjust based on n_gpu\n",
    "        warmup_steps=5,\n",
    "        learning_rate=1e-5,\n",
    "        num_train_epochs=1,\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        report_to=\"wandb\",  \n",
    "    )\n",
    "\n",
    "    # args.skip_prepare_dataset = True\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=text_dataset,\n",
    "        eval_dataset=None,\n",
    "        args=args,\n",
    "    )\n",
    "    from unsloth_zoo.dataset_utils import train_on_responses_only\n",
    "\n",
    "    trainer = train_on_responses_only(\n",
    "        trainer,\n",
    "        tokenizer=tokenizer,\n",
    "        instruction_part=\"<|im_start|>user\\n\",\n",
    "        response_part=\"<|im_start|>assistant\\n\",\n",
    "    )\n",
    "\n",
    "    # Show memory stats\n",
    "    gpu_stats = torch.cuda.get_device_properties(0)\n",
    "    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "    print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "    print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "    # Train the model\n",
    "\n",
    "    # from ._patch_sampler import patch_sampler\n",
    "\n",
    "    trainer = patch_sampler(trainer)\n",
    "    trainer_stats = trainer.train()\n",
    "\n",
    "    # Show final memory and time stats\n",
    "    used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "    used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "    lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "    print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "    print(\n",
    "        f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    "    )\n",
    "    print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "    print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "    print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "    print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, tokenizer = train_qwen3_model()\n",
    "    print(\"Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626094db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
