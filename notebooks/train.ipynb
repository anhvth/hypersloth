{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1555d26d",
   "metadata": {},
   "source": [
    "# üöÄ HyperSloth Demo Training Notebook\n",
    "\n",
    "This notebook demonstrates how to fine-tune large language models using HyperSloth's multi-GPU capabilities. It's equivalent to running:\n",
    "\n",
    "```bash\n",
    "hypersloth-train examples/example_sharegpt_lora_2gpus.py\n",
    "```\n",
    "\n",
    "## What This Demo Does\n",
    "\n",
    "- **Multi-GPU Training**: Uses 2 GPUs with NCCL synchronization\n",
    "- **Adaptive Batching**: Optimizes sequence sorting and padding\n",
    "- **LoRA Fine-tuning**: Efficient parameter updates with Low-Rank Adaptation\n",
    "- **Response-only Loss**: Calculates loss only on assistant responses\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. HyperSloth installed: `pip install git+https://github.com/anhvth/HyperSloth.git`\n",
    "2. At least 2 GPUs available (adjust `gpus=[0, 1]` if needed)\n",
    "3. Sufficient VRAM (reduce batch size if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed68946",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72a817bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• CUDA Available: True\n",
      "üî• GPU Count: 4\n",
      "   GPU 0: NVIDIA H100 80GB HBM3\n",
      "   GPU 1: NVIDIA H100 80GB HBM3\n",
      "   GPU 2: NVIDIA H100 80GB HBM3\n",
      "   GPU 3: NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "# Import HyperSloth configuration classes\n",
    "from HyperSloth.hypersloth_config import *\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f'üî• CUDA Available: {torch.cuda.is_available()}')\n",
    "print(f'üî• GPU Count: {torch.cuda.device_count()}')\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f'   GPU {i}: {torch.cuda.get_device_name(i)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b606c272",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration Setup\n",
    "\n",
    "HyperSloth uses Pydantic models for type-safe configuration. We'll set up:\n",
    "\n",
    "1. **Data Configuration**: Dataset and tokenization settings\n",
    "2. **Training Configuration**: GPU allocation and loss calculation\n",
    "3. **Model Configuration**: Base model and LoRA parameters\n",
    "4. **Training Arguments**: Learning rate, batch size, and optimization settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f33c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global batch size: 32\n",
      "[MP] Running on 4 GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m06:53:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mhp_trainer.py:42\u001b[0m | \u001b[1mTraining on GPU 0 with output_dir outputs/qwen3-8b-openthought-2gpus/\u001b[0m\n",
      "\u001b[32m06:53:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mhp_trainer.py:45\u001b[0m | \u001b[1müöÄ Starting total training timer\u001b[0m\n",
      "\u001b[32m06:53:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mhp_trainer.py:42\u001b[0m | \u001b[1mTraining on GPU 2 with output_dir outputs/qwen3-8b-openthought-2gpus/\u001b[0m\n",
      "\u001b[32m06:53:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mhp_trainer.py:45\u001b[0m | \u001b[1müöÄ Starting total training timer\u001b[0m\n",
      "\u001b[32m06:53:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mhp_trainer.py:42\u001b[0m | \u001b[1mTraining on GPU 3 with output_dir outputs/qwen3-8b-openthought-2gpus/\u001b[0m\n",
      "\u001b[32m06:53:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mhp_trainer.py:45\u001b[0m | \u001b[1müöÄ Starting total training timer\u001b[0m\n",
      "\u001b[32m06:53:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mhp_trainer.py:42\u001b[0m | \u001b[1mTraining on GPU 1 with output_dir outputs/qwen3-8b-openthought-2gpus/\u001b[0m\n",
      "\u001b[32m06:53:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mhp_trainer.py:45\u001b[0m | \u001b[1müöÄ Starting total training timer\u001b[0m\n",
      "\u001b[32m06:53:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_loading: 17.36s\u001b[0m\n",
      "\u001b[32m06:53:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mnccl_grad_sync.py:128\u001b[0m | \u001b[1m[GPU=0] NCCL env: RANK=0, WORLD_SIZE=4, MASTER_ADDR=127.0.0.1, MASTER_PORT=29501\u001b[0m\n",
      "\u001b[32m06:53:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mnccl_grad_sync.py:132\u001b[0m | \u001b[1m[GPU=0] Setting current CUDA device to:0, os.environ['CUDA_VISIBLE_DEVICES']='0'\u001b[0m\n",
      "\u001b[32m06:53:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_loading: 17.40s\u001b[0m\n",
      "\u001b[32m06:53:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mnccl_grad_sync.py:128\u001b[0m | \u001b[1m[GPU=1] NCCL env: RANK=1, WORLD_SIZE=4, MASTER_ADDR=127.0.0.1, MASTER_PORT=29501\u001b[0m\n",
      "\u001b[32m06:53:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mnccl_grad_sync.py:132\u001b[0m | \u001b[1m[GPU=1] Setting current CUDA device to:0, os.environ['CUDA_VISIBLE_DEVICES']='1'\u001b[0m\n",
      "\u001b[32m06:53:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36minit_modules.py:49\u001b[0m | \u001b[1mModel loaded on device cuda:0, tokenizer: Qwen2TokenizerFast\u001b[0m\n",
      "\u001b[32m06:53:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_loading: 17.69s\u001b[0m\n",
      "\u001b[32m06:53:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mnccl_grad_sync.py:128\u001b[0m | \u001b[1m[GPU=2] NCCL env: RANK=2, WORLD_SIZE=4, MASTER_ADDR=127.0.0.1, MASTER_PORT=29501\u001b[0m\n",
      "\u001b[32m06:53:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mnccl_grad_sync.py:132\u001b[0m | \u001b[1m[GPU=2] Setting current CUDA device to:0, os.environ['CUDA_VISIBLE_DEVICES']='2'\u001b[0m\n",
      "\u001b[32m06:53:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_loading: 17.99s\u001b[0m\n",
      "\u001b[32m06:53:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mnccl_grad_sync.py:128\u001b[0m | \u001b[1m[GPU=3] NCCL env: RANK=3, WORLD_SIZE=4, MASTER_ADDR=127.0.0.1, MASTER_PORT=29501\u001b[0m\n",
      "\u001b[32m06:53:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mnccl_grad_sync.py:132\u001b[0m | \u001b[1m[GPU=3] Setting current CUDA device to:0, os.environ['CUDA_VISIBLE_DEVICES']='3'\u001b[0m\n",
      "\u001b[32m06:53:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36minit_modules.py:49\u001b[0m | \u001b[1mModel loaded on device cuda:0, tokenizer: Qwen2TokenizerFast\u001b[0m\n",
      "\u001b[32m06:53:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:49\u001b[0m | \u001b[1mModel loaded on device cuda:0, tokenizer: Qwen2TokenizerFast\u001b[0m\n",
      "\u001b[32m06:53:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36minit_modules.py:49\u001b[0m | \u001b[1mModel loaded on device cuda:0, tokenizer: Qwen2TokenizerFast\u001b[0m\n",
      "\u001b[32m06:53:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  lora_setup: 4.42s\u001b[0m\n",
      "\u001b[32m06:53:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_init: 21.82s\u001b[0m\n",
      "\u001b[32m06:53:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36minit_modules.py:142\u001b[0m | \u001b[1mCreating final SFTTrainer with prepared dataset...\u001b[0m\n",
      "\u001b[32m06:53:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  lora_setup: 4.48s\u001b[0m\n",
      "\u001b[32m06:53:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_init: 22.17s\u001b[0m\n",
      "\u001b[32m06:53:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36minit_modules.py:142\u001b[0m | \u001b[1mCreating final SFTTrainer with prepared dataset...\u001b[0m\n",
      "\u001b[32m06:53:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  lora_setup: 4.41s\u001b[0m\n",
      "\u001b[32m06:53:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_init: 22.41s\u001b[0m\n",
      "\u001b[32m06:53:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36minit_modules.py:142\u001b[0m | \u001b[1mCreating final SFTTrainer with prepared dataset...\u001b[0m\n",
      "\u001b[32m06:53:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  lora_setup: 4.42s\u001b[0m\n",
      "\u001b[32m06:53:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_init: 23.04s\u001b[0m\n",
      "\u001b[32m06:53:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:142\u001b[0m | \u001b[1mCreating final SFTTrainer with prepared dataset...\u001b[0m\n",
      "\u001b[32m06:54:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  final_trainer_creation: 3.44s\u001b[0m\n",
      "\u001b[32m06:54:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36minit_modules.py:155\u001b[0m | \u001b[1mReplacing DataCollatorForLanguageModeling with DataCollatorForSeq2Seq for better sequence handling\u001b[0m\n",
      "\u001b[32m06:54:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36minit_modules.py:163\u001b[0m | \u001b[1mTrainer setup completed successfully\u001b[0m\n",
      "\u001b[32m06:54:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  trainer_setup: 3.45s\u001b[0m\n",
      "\u001b[32m06:54:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36minit_modules.py:119\u001b[0m | \u001b[1mAdd callback ShuffleData to Trainer UnslothSFTTrainer\u001b[0m\n",
      "\u001b[32m06:54:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  trainer_creation: 3.46s\u001b[0m\n",
      "\u001b[32m06:54:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  total_setup: 35.48s\u001b[0m\n",
      "\u001b[32m06:54:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_and_training_setup: 35.49s\u001b[0m\n",
      "\u001b[32m06:54:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mnccl_grad_sync.py:48\u001b[0m | \u001b[1m[GPU=1] NCCLGradSyncCallback initialized for rank 1/4\u001b[0m\n",
      "\u001b[32m06:54:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mhp_trainer.py:64\u001b[0m | \u001b[1mUsing gradient sync callback for GPU 1\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  final_trainer_creation: 3.43s\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:155\u001b[0m | \u001b[1mReplacing DataCollatorForLanguageModeling with DataCollatorForSeq2Seq for better sequence handling\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:163\u001b[0m | \u001b[1mTrainer setup completed successfully\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  trainer_setup: 3.44s\u001b[0m\n",
      "  0%|          | 0/96 [00:00<?, ?it/s]\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  final_trainer_creation: 3.49s\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36minit_modules.py:155\u001b[0m | \u001b[1mReplacing DataCollatorForLanguageModeling with DataCollatorForSeq2Seq for better sequence handling\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36minit_modules.py:163\u001b[0m | \u001b[1mTrainer setup completed successfully\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  trainer_setup: 3.50s\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36minit_modules.py:119\u001b[0m | \u001b[1mAdd callback ShuffleData to Trainer UnslothSFTTrainer\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  trainer_creation: 3.51s\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  total_setup: 36.11s\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_and_training_setup: 36.12s\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mnccl_grad_sync.py:48\u001b[0m | \u001b[1m[GPU=3] NCCLGradSyncCallback initialized for rank 3/4\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mhp_trainer.py:64\u001b[0m | \u001b[1mUsing gradient sync callback for GPU 3\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:119\u001b[0m | \u001b[1mAdd callback ShuffleData to Trainer UnslothSFTTrainer\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1müé≤ Sampler epoch 0: emitting 1000 indices\n",
      "First ids: [776, 507, 895, 922, 33, 483, 85, 750, 354, 523]\n",
      "...Last ids: [104, 754, 142, 228, 250, 281, 759, 25, 114, 654]\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  trainer_creation: 3.48s\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  total_setup: 36.19s\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_and_training_setup: 36.21s\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mnccl_grad_sync.py:48\u001b[0m | \u001b[1m[GPU=0] NCCLGradSyncCallback initialized for rank 0/4\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mhp_trainer.py:64\u001b[0m | \u001b[1mUsing gradient sync callback for GPU 0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "Using compiler location: .cache/unsloth_compiled_cache_1\n",
      "==((====))==  Unsloth 2025.5.9: Fast Qwen3 patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.189 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "[LOCAL_RANK=1] Patching log. Dir: outputs/qwen3-8b-openthought-2gpus/, GPUs: 4\n",
      "[LOCAL_RANK=1] Log patch initialization complete.\n",
      "üîß Patching Trainer to use RandomSamplerSeededByEpoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  final_trainer_creation: 3.92s\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36minit_modules.py:155\u001b[0m | \u001b[1mReplacing DataCollatorForLanguageModeling with DataCollatorForSeq2Seq for better sequence handling\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36minit_modules.py:163\u001b[0m | \u001b[1mTrainer setup completed successfully\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  trainer_setup: 3.94s\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36minit_modules.py:119\u001b[0m | \u001b[1mAdd callback ShuffleData to Trainer UnslothSFTTrainer\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  trainer_creation: 3.95s\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  total_setup: 36.37s\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_and_training_setup: 36.39s\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mnccl_grad_sync.py:48\u001b[0m | \u001b[1m[GPU=2] NCCLGradSyncCallback initialized for rank 2/4\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mhp_trainer.py:64\u001b[0m | \u001b[1mUsing gradient sync callback for GPU 2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "Using compiler location: .cache/unsloth_compiled_cache_3\n",
      "==((====))==  Unsloth 2025.5.9: Fast Qwen3 patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.189 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "[LOCAL_RANK=3] Patching log. Dir: outputs/qwen3-8b-openthought-2gpus/, GPUs: 4\n",
      "[LOCAL_RANK=3] Log patch initialization complete.\n",
      "üîß Patching Trainer to use RandomSamplerSeededByEpoch\n",
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "Using compiler location: .cache/unsloth_compiled_cache_0\n",
      "==((====))==  Unsloth 2025.5.9: Fast Qwen3 patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.189 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "[LOCAL_RANK=0] Patching log. Dir: outputs/qwen3-8b-openthought-2gpus/, GPUs: 4\n",
      "[LOCAL_RANK=0] Log patch initialization complete.\n",
      "üîß Patching Trainer to use RandomSamplerSeededByEpoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/96 [00:00<?, ?it/s]\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1müé≤ Sampler epoch 0: emitting 1000 indices\n",
      "First ids: [776, 507, 895, 922, 33, 483, 85, 750, 354, 523]\n",
      "...Last ids: [104, 754, 142, 228, 250, 281, 759, 25, 114, 654]\u001b[0m\n",
      "  0%|          | 0/96 [00:00<?, ?it/s]\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:21\u001b[0m | \u001b[1müîÑ Starting epoch 1\u001b[0m\n",
      "\u001b[32m06:54:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1müé≤ Sampler epoch 0: emitting 1000 indices\n",
      "First ids: [776, 507, 895, 922, 33, 483, 85, 750, 354, 523]\n",
      "...Last ids: [104, 754, 142, 228, 250, 281, 759, 25, 114, 654]\u001b[0m\n",
      "  0%|          | 0/96 [00:00<?, ?it/s]\u001b[32m06:54:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1müé≤ Sampler epoch 0: emitting 1000 indices\n",
      "First ids: [776, 507, 895, 922, 33, 483, 85, 750, 354, 523]\n",
      "...Last ids: [104, 754, 142, 228, 250, 281, 759, 25, 114, 654]\u001b[0m\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "Using compiler location: .cache/unsloth_compiled_cache_2\n",
      "==((====))==  Unsloth 2025.5.9: Fast Qwen3 patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.189 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "[LOCAL_RANK=2] Patching log. Dir: outputs/qwen3-8b-openthought-2gpus/, GPUs: 4\n",
      "[LOCAL_RANK=2] Log patch initialization complete.\n",
      "üîß Patching Trainer to use RandomSamplerSeededByEpoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "\u001b[32m06:54:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:28\u001b[0m | \u001b[1müìã Dataloader examples logged to .log/dataloader_examples.html\u001b[0m\n",
      "\u001b[32m06:54:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1müé≤ Sampler epoch 0: emitting 1000 indices\n",
      "First ids: [776, 507, 895, 922, 33, 483, 85, 750, 354, 523]\n",
      "...Last ids: [104, 754, 142, 228, 250, 281, 759, 25, 114, 654]\u001b[0m\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    }
   ],
   "source": [
    "from HyperSloth.hypersloth_config import *\n",
    "from HyperSloth.scripts.hp_trainer import run_mp_training, setup_envs\n",
    "\n",
    "# Main configuration using Pydantic models\n",
    "hyper_config_model = HyperConfig(\n",
    "    data=HFDatasetConfig(\n",
    "        dataset_name=\"llamafactory/OpenThoughts-114k\",\n",
    "        split=\"train\",\n",
    "        tokenizer_name=\"Qwen/Qwen3-8B\",  # does not matter same family qwen3\n",
    "        num_samples=1000,\n",
    "        instruction_part=\"<|im_start|>user\\n\",\n",
    "        response_part=\"<|im_start|>assistant\\n\",\n",
    "        chat_template=\"chatml\",\n",
    "    ),\n",
    "    training=TrainingConfig(\n",
    "        gpus=[0, 1,2,3],\n",
    "        loss_type=\"response_only\",\n",
    "    ),\n",
    "    fast_model_args=FastModelArgs(\n",
    "        model_name=\"unsloth/Qwen3-0.6b-bnb-4bit\",\n",
    "        max_seq_length=32_000,\n",
    "        load_in_4bit=True,\n",
    "    ),\n",
    "    lora_args=LoraArgs(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "        use_rslora=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Training arguments using Pydantic model\n",
    "training_config_model = TrainingArgsConfig(\n",
    "    output_dir=\"outputs/qwen3-8b-openthought-2gpus/\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-5,\n",
    "    logging_steps=3,\n",
    "    num_train_epochs=3,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=5,\n",
    "    save_total_limit=2,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_8bit\",\n",
    "    seed=3407,\n",
    "    report_to=\"none\",  # tensorboard or wawndb\n",
    ")\n",
    "\n",
    "setup_envs(hyper_config_model, training_config_model)\n",
    "\n",
    "run_mp_training(\n",
    "    hyper_config_model.training.gpus, hyper_config_model, training_config_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b45e70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ee347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
