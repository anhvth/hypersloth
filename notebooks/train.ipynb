{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f33c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from HyperSloth.hypersloth_config import *\n",
    "from HyperSloth.scripts.hp_trainer import run_multiprocess_training, setup_envs\n",
    "\n",
    "# Main configuration using Pydantic models\n",
    "hyper_config_model = HyperConfig(\n",
    "    data=HFDatasetConfig(\n",
    "        dataset_name=\"llamafactory/OpenThoughts-114k\",\n",
    "        split=\"train\",\n",
    "        tokenizer_name=\"Qwen/Qwen3-8B\",  # does not matter same family qwen3\n",
    "        num_samples=1000,\n",
    "        instruction_part=\"<|im_start|>user\\n\",\n",
    "        response_part=\"<|im_start|>assistant\\n\",\n",
    "        chat_template=\"chatml\",\n",
    "    ),\n",
    "    training=TrainingConfig(\n",
    "        gpus=[0, 1],\n",
    "        loss_type=\"response_only\",\n",
    "    ),\n",
    "    fast_model_args=FastModelArgs(\n",
    "        model_name=\"unsloth/Qwen3-0.6b-bnb-4bit\",\n",
    "        max_seq_length=32_000,\n",
    "        load_in_4bit=True,\n",
    "    ),\n",
    "    lora_args=LoraArgs(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "        use_rslora=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Training arguments using Pydantic model\n",
    "training_config_model = TrainingArgsConfig(\n",
    "    output_dir=\"outputs/qwen3-8b-openthought-2gpus/\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-5,\n",
    "    logging_steps=3,\n",
    "    num_train_epochs=3,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=5,\n",
    "    save_total_limit=2,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_8bit\",\n",
    "    seed=3407,\n",
    "    report_to=\"none\",  # tensorboard or wawndb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b45e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global batch size: 16\n",
      "[MP] Running on 2 GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m03:05:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mhp_trainer.py:44\u001b[0m | \u001b[1müîß GPU 1 (Rank 1/1) | Model: unsloth/Qwen3-0.6b-bnb-4bit\u001b[0m\n",
      "\u001b[32m03:05:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mhp_trainer.py:50\u001b[0m | \u001b[1mTraining on GPU 1 with output_dir outputs/qwen3-8b-openthought-2gpus/\u001b[0m\n",
      "\u001b[32m03:05:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mhp_trainer.py:53\u001b[0m | \u001b[1müöÄ Starting total training timer\u001b[0m\n",
      "\u001b[32m03:05:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mhp_trainer.py:44\u001b[0m | \u001b[1müîß GPU 0 (Rank 0/1) | Model: unsloth/Qwen3-0.6b-bnb-4bit\u001b[0m\n",
      "\u001b[32m03:05:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mhp_trainer.py:50\u001b[0m | \u001b[1mTraining on GPU 0 with output_dir outputs/qwen3-8b-openthought-2gpus/\u001b[0m\n",
      "\u001b[32m03:05:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mhp_trainer.py:53\u001b[0m | \u001b[1müöÄ Starting total training timer\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.5.9: Fast Qwen3 patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.189 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "==((====))==  Unsloth 2025.5.9: Fast Qwen3 patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.189 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m03:06:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mlogging_config.py:407\u001b[0m | \u001b[1m‚è±Ô∏è  model_loading: 15.26s\u001b[0m\n",
      "\u001b[32m03:06:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mnccl_grad_sync.py:127\u001b[0m | \u001b[1m[GPU=1] NCCL env: RANK=1, WORLD_SIZE=2, MASTER_ADDR=127.0.0.1, MASTER_PORT=29500\u001b[0m\n",
      "\u001b[32m03:06:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mnccl_grad_sync.py:146\u001b[0m | \u001b[1m[GPU=1] Setting current CUDA device to:0\u001b[0m\n",
      "\u001b[32m03:06:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:407\u001b[0m | \u001b[1m‚è±Ô∏è  model_loading: 14.46s\u001b[0m\n",
      "\u001b[32m03:06:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mnccl_grad_sync.py:127\u001b[0m | \u001b[1m[GPU=0] NCCL env: RANK=0, WORLD_SIZE=2, MASTER_ADDR=127.0.0.1, MASTER_PORT=29500\u001b[0m\n",
      "\u001b[32m03:06:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mnccl_grad_sync.py:146\u001b[0m | \u001b[1m[GPU=0] Setting current CUDA device to:0\u001b[0m\n",
      "\u001b[32m03:06:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mnccl_grad_sync.py:160\u001b[0m | \u001b[1m[GPU=1] NCCL setup complete: rank=1, world_size=2, attempt=1\u001b[0m\n",
      "\u001b[32m03:06:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36minit_modules.py:49\u001b[0m | \u001b[1mModel loaded on device cuda:0, tokenizer: Qwen2TokenizerFast\u001b[0m\n",
      "\u001b[32m03:06:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mnccl_grad_sync.py:160\u001b[0m | \u001b[1m[GPU=0] NCCL setup complete: rank=0, world_size=2, attempt=1\u001b[0m\n",
      "\u001b[32m03:06:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:49\u001b[0m | \u001b[1mModel loaded on device cuda:0, tokenizer: Qwen2TokenizerFast\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m03:06:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:407\u001b[0m | \u001b[1m‚è±Ô∏è  model_init: 18.04s\u001b[0m\n",
      "\u001b[32m03:06:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mdataset_utils.py:283\u001b[0m | \u001b[1mLoading dataset... and tokenize\u001b[0m\n",
      "\u001b[32m03:06:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mdataset_utils.py:299\u001b[0m | \u001b[1mFinal dataset loaded with 1000 samples (cache_id: abb36c5638db...)\u001b[0m\n",
      "\u001b[32m03:06:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:131\u001b[0m | \u001b[1mCreating final SFTTrainer with prepared dataset...\u001b[0m\n",
      "\u001b[32m03:06:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mlogging_config.py:407\u001b[0m | \u001b[1m‚è±Ô∏è  model_init: 19.57s\u001b[0m\n",
      "\u001b[32m03:06:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mdataset_utils.py:283\u001b[0m | \u001b[1mLoading dataset... and tokenize\u001b[0m\n",
      "\u001b[32m03:06:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mdataset_utils.py:299\u001b[0m | \u001b[1mFinal dataset loaded with 1000 samples (cache_id: abb36c5638db...)\u001b[0m\n",
      "\u001b[32m03:06:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36minit_modules.py:131\u001b[0m | \u001b[1mCreating final SFTTrainer with prepared dataset...\u001b[0m\n",
      "\u001b[32m03:06:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:146\u001b[0m | \u001b[1mReplacing DataCollatorForLanguageModeling with DataCollatorForSeq2Seq for better sequence handling\u001b[0m\n",
      "\u001b[32m03:06:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:154\u001b[0m | \u001b[1mTrainer setup completed successfully\u001b[0m\n",
      "\u001b[32m03:06:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minner_training_loop.py:256\u001b[0m | \u001b[1mHyperSloth ultra-minimal patches applied successfully (rank 0/2)\u001b[0m\n",
      "\u001b[32m03:06:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:70\u001b[0m | \u001b[1müîß Patching Trainer to use RandomSamplerSeededByEpoch\u001b[0m\n",
      "\u001b[32m03:06:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:86\u001b[0m | \u001b[1mAdd callback ShuffleData to Trainer UnslothSFTTrainer\u001b[0m\n",
      "\u001b[32m03:06:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:407\u001b[0m | \u001b[1m‚è±Ô∏è  total_setup: 48.71s\u001b[0m\n",
      "\u001b[32m03:06:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:407\u001b[0m | \u001b[1m‚è±Ô∏è  model_and_training_setup: 48.76s\u001b[0m\n",
      "\u001b[32m03:06:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mnccl_grad_sync.py:49\u001b[0m | \u001b[1m[GPU=0] NCCLGradSyncCallback initialized for rank 0/2\u001b[0m\n",
      "\u001b[32m03:06:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mhp_trainer.py:72\u001b[0m | \u001b[1mUsing gradient sync callback for GPU 0\u001b[0m\n",
      "\u001b[32m03:06:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36minit_modules.py:146\u001b[0m | \u001b[1mReplacing DataCollatorForLanguageModeling with DataCollatorForSeq2Seq for better sequence handling\u001b[0m\n",
      "\u001b[32m03:06:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36minit_modules.py:154\u001b[0m | \u001b[1mTrainer setup completed successfully\u001b[0m\n",
      "\u001b[32m03:06:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36minner_training_loop.py:256\u001b[0m | \u001b[1mHyperSloth ultra-minimal patches applied successfully (rank 1/2)\u001b[0m\n",
      "\u001b[32m03:06:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mpatch_sampler.py:70\u001b[0m | \u001b[1müîß Patching Trainer to use RandomSamplerSeededByEpoch\u001b[0m\n",
      "\u001b[32m03:06:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mpatch_sampler.py:86\u001b[0m | \u001b[1mAdd callback ShuffleData to Trainer UnslothSFTTrainer\u001b[0m\n",
      "\u001b[32m03:06:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mlogging_config.py:407\u001b[0m | \u001b[1m‚è±Ô∏è  total_setup: 49.03s\u001b[0m\n",
      "\u001b[32m03:06:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mlogging_config.py:407\u001b[0m | \u001b[1m‚è±Ô∏è  model_and_training_setup: 49.07s\u001b[0m\n",
      "\u001b[32m03:06:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mnccl_grad_sync.py:49\u001b[0m | \u001b[1m[GPU=1] NCCLGradSyncCallback initialized for rank 1/2\u001b[0m\n",
      "\u001b[32m03:06:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mhp_trainer.py:72\u001b[0m | \u001b[1mUsing gradient sync callback for GPU 1\u001b[0m\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,000 | Num Epochs = 3 | Total steps = 189\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 8 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 5,046,272/6,000,000,000 (0.08% trained)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,000 | Num Epochs = 3 | Total steps = 189\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 8 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 5,046,272/6,000,000,000 (0.08% trained)\n",
      "\u001b[32m03:06:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:21\u001b[0m | \u001b[1müîÑ Starting epoch 1\u001b[0m\n",
      "\u001b[32m03:06:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1müé≤ Sampler epoch 0: emitting 1000 indices\n",
      "First ids: [776, 507, 895, 922, 33, 483, 85, 750, 354, 523]\n",
      "Last ids: [104, 754, 142, 228, 250, 281, 759, 25, 114, 654]\u001b[0m\n",
      "\u001b[32m03:06:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1müé≤ Sampler epoch 0: emitting 1000 indices\n",
      "First ids: [776, 507, 895, 922, 33, 483, 85, 750, 354, 523]\n",
      "Last ids: [104, 754, 142, 228, 250, 281, 759, 25, 114, 654]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXAMPLE #1 ===\n",
      "\u001b[92m<|im_start|>system\n",
      "You are an assistant that thoroughly explores questions through a systematic long thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarization, exploration, reassessment, reflection, backtracing, and iteration to develop a well-considered thinking process. Detail your reasoning process using the specified format: <think>thought with steps separated by '\n",
      "\n",
      "'</think> Each step should include detailed considerations such as analyzing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. Based on various attempts, explorations, and reflections from the thoughts, you should systematically present the final solution that you deem correct. The solution should remain a logical, accurate, concise expression style and detail necessary steps needed to reach the conclusion. Now, try to solve the following question through the above guidelines.<|im_end|>\n",
      "<|im_start|>user\n",
      "A ski lift carries a skier at a rate of 12 kilometers per hour. How many kilometers does the ski lift carry the skier in 10 minutes?\n",
      "(A) 120\n",
      "(B) 1.2\n",
      "(C) 2\n",
      "(D) 2.4\n",
      "(E) 1.67<|im_end|>\n",
      "<|im_start|>assistant\n",
      "... (truncated)\u001b[0m\u001b[93m<think>\n",
      "Okay, so I need to figure out how many kilometers the ski lift carries the skier in 10 minutes. The problem says the ski lift's rate is 12 kilometers per hour. Hmm, right. Let me start by understanding what the question is asking.\n",
      "\n",
      "First, speed is given in kilometers per hour, but the time we're looking at is in minutes. I remember that when dealing with speed, time, and distance, the formula is distance equals speed multiplied by time. But here, the units don't match‚Äîspeed is per hour, and time is in minutes. So, I need to convert either the time to hours or the speed to kilometers per minute. Maybe converting minutes to hours would be easier here.\n",
      "\n",
      "Wait, 10 minutes is a fraction of an hour. Since there are 60 minutes in an hour, 10 minutes would be 10/60 hours. Let me write that down. 10 divided by 60 is... let's see, 10/60 simplifies to 1/6, which is approximately 0.1667 hours. Yeah, that makes sense because 10 minutes is a sixth of an hour.\n",
      "\n",
      "So, if the ski lift is moving at 12 km/h, then in one hour, it would cover 12 km. But we only need a tenth of an hour? Wait, no‚Äî10 minutes is 1/6 of an hour, not a tenth. Wait, 10 divided by 60 is indeed 1/6. Right. So, maybe I confused that with 10 minutes being 0.1 hours, but no, 0.1 hours is 6 minutes. So 10 minutes is more than that. So 10/60 = 1/6 ‚âà 0.1667 hours. Got it.\n",
      "\n",
      "So, the distance would be speed multiplied by time. So that's 12 km/h multiplied by 1/6 hour. Let's compute that. 12 multiplied by 1/6 is 12/6, which is 2. So, 2 kilometers. Wait, but hold on. Let me check that again. 12 divided by 6 is 2, yes. So, 12 km/h times 1/6 hour equals 2 km. That seems straightforward.\n",
      "\n",
      "But let me make sure I didn't make a mistake in converting minutes to hours. Let's verify. 60 minutes = 1 hour. Therefore, 1 minute = 1/60 hour. So, 10 minutes = 10 * (1/60) = 10/60 = 1/6 hour. That's correct. So, 12 * (1/6) = 2. So, the answer should be 2 km, which is option C.\n",
      "\n",
      "Wait, but let me see the options again. The options are (A) 120, (B) 1.2, (C) 2, (D) 2.4, (E) 1.67. So, 2 is option C. Hmm. But another way to think about it is maybe the speed is 12 km per hour, so per minute it would be 12 divided by 60, which is 0.2 km per minute. Then, 0.2 km/min times 10 minutes equals 2 km. Yeah, that also gives the same answer. So either way, converting time to hours or converting speed to km per minute, the result is 2 km. Therefore, the correct answer is C.\n",
      "\n",
      "But wait, why is there an option E, 1.67? Maybe someone could miscalculate. Let me check if I did something wrong. For instance, if someone thought 10 minutes is 0.1 hours, then 12 * 0.1 would be 1.2, which is option B. But that's incorrect because 10 minutes is not 0.1 hours. 0.1 hours is 6 minutes. So, perhaps a common mistake is to divide by 100 instead of 60. But in this case, the correct conversion is 10/60, which is 1/6, so 12 * (1/6) is 2. So, answer C is correct. Therefore, the answer is C, 2.\n",
      "</think>\n",
      "\n",
      "To determine how many kilometers the ski lift carries the skier in 10 minutes, we use the formula:\n",
      "\n",
      "\\[\n",
      "\\text{Distance} = \\text{Speed} \\times \\text{Time}\n",
      "\\]\n",
      "\n",
      "**Given:**\n",
      "- Speed = 12 km/h  \n",
      "- Time = 10 minutes  \n",
      "\n",
      "**Step 1: Convert time to hours**  \n",
      "Since 1 hour = 60 minutes,  \n",
      "\\[\n",
      "10\\ \\text{minutes} = \\frac{10}{60}\\ \\text{hours} = \\frac{1}{6}\\ \\text{hours} \\approx 0.1667\\ \\text{hours}.\n",
      "\\]\n",
      "\n",
      "**Step 2: Calculate distance**  \n",
      "\\[\n",
      "\\text{Distance} = 12\\ \\text{km/h} \\times \\frac{1}{6}\\ \\text{hours} = 2\\ \\text{km}.\n",
      "\\]\n",
      "\n",
      "**Verification via alternative method:**  \n",
      "Convert speed to km/minute:  \n",
      "\\[\n",
      "12\\ \\text{km/h} = \\frac{12}{60}\\ \\text{km/min} = 0.2\\ \\text{km/min}.\n",
      "\\]\n",
      "\\[\n",
      "\\text{Distance} = 0.2\\ \\text{km/min} \\times 10\\ \\text{min} = 2\\ \\text{km}.\n",
      "\\]\n",
      "\n",
      "**Conclusion:**  \n",
      "The ski lift carries the skier **2 kilometers** in 10 minutes.  \n",
      "\n",
      "**Answer:** (C) 2<|im_end|>\n",
      "... (truncated)\u001b[0m\u001b[92m... padding 2044 tokens\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More training debug examples written to .log/dataloader_examples.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m03:06:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:28\u001b[0m | \u001b[1müìã Dataloader examples logged to .log/dataloader_examples.html\u001b[0m\n",
      "\u001b[32m03:06:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1müé≤ Sampler epoch 0: emitting 1000 indices\n",
      "First ids: [776, 507, 895, 922, 33, 483, 85, 750, 354, 523]\n",
      "Last ids: [104, 754, 142, 228, 250, 281, 759, 25, 114, 654]\u001b[0m\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n",
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 18/189 01:49 < 19:30, 0.15 it/s, Epoch 0.27/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.685200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.703900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.695100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.713000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.736300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 18/189 01:49 < 19:30, 0.15 it/s, Epoch 0.27/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m03:07:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minner_training_loop.py:100\u001b[0m | \u001b[1m\n",
      "üöÄ HyperSloth Token Efficiency Report (Rank 0)\n",
      "+---------------------+----------------+-----------------------+\n",
      "| Stage               |   Total Tokens |   Token Utilization % |\n",
      "+=====================+================+=======================+\n",
      "| Before Optimization |      1,283,882 |                67.39% |\n",
      "+---------------------+----------------+-----------------------+\n",
      "| After Optimization  |        865,203 |               100.00% |\n",
      "+---------------------+----------------+-----------------------+\n",
      "\u001b[0m\n",
      "\u001b[32m03:07:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36minner_training_loop.py:100\u001b[0m | \u001b[1m\n",
      "üöÄ HyperSloth Token Efficiency Report (Rank 1)\n",
      "+---------------------+----------------+-----------------------+\n",
      "| Stage               |   Total Tokens |   Token Utilization % |\n",
      "+=====================+================+=======================+\n",
      "| Before Optimization |      1,283,882 |                67.39% |\n",
      "+---------------------+----------------+-----------------------+\n",
      "| After Optimization  |        865,203 |               100.00% |\n",
      "+---------------------+----------------+-----------------------+\n",
      "\u001b[0m\n",
      "\u001b[32m03:08:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minner_training_loop.py:100\u001b[0m | \u001b[1m\n",
      "üöÄ HyperSloth Token Efficiency Report (Rank 0)\n",
      "+---------------------+----------------+-----------------------+\n",
      "| Stage               |   Total Tokens |   Token Utilization % |\n",
      "+=====================+================+=======================+\n",
      "| Before Optimization |      2,648,348 |                69.26% |\n",
      "+---------------------+----------------+-----------------------+\n",
      "| After Optimization  |      1,834,304 |               100.00% |\n",
      "+---------------------+----------------+-----------------------+\n",
      "\u001b[0m\n",
      "\u001b[32m03:08:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36minner_training_loop.py:100\u001b[0m | \u001b[1m\n",
      "üöÄ HyperSloth Token Efficiency Report (Rank 1)\n",
      "+---------------------+----------------+-----------------------+\n",
      "| Stage               |   Total Tokens |   Token Utilization % |\n",
      "+=====================+================+=======================+\n",
      "| Before Optimization |      2,648,348 |                69.26% |\n",
      "+---------------------+----------------+-----------------------+\n",
      "| After Optimization  |      1,834,304 |               100.00% |\n",
      "+---------------------+----------------+-----------------------+\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "setup_envs(hyper_config_model, training_config_model)\n",
    "\n",
    "run_multiprocess_training(\n",
    "    hyper_config_model.training.gpus, hyper_config_model, training_config_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db197e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
