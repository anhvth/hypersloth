{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1555d26d",
   "metadata": {},
   "source": [
    "# üöÄ OpenSloth Demo Training Notebook\n",
    "\n",
    "This notebook demonstrates how to fine-tune large language models using opensloth's multi-GPU capabilities. It's equivalent to running:\n",
    "\n",
    "```bash\n",
    "opensloth-train examples/example_sharegpt_lora_2gpus.py\n",
    "```\n",
    "\n",
    "## What This Demo Does\n",
    "\n",
    "- **Multi-GPU Training**: Uses 2 GPUs with NCCL synchronization\n",
    "- **Adaptive Batching**: Optimizes sequence sorting and padding\n",
    "- **LoRA Fine-tuning**: Efficient parameter updates with Low-Rank Adaptation\n",
    "- **Response-only Loss**: Calculates loss only on assistant responses\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. opensloth installed: `pip install git+https://github.com/anhvth/opensloth.git`\n",
    "2. At least 2 GPUs available (adjust `gpus=[0, 1]` if needed)\n",
    "3. Sufficient VRAM (reduce batch size if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed68946",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72a817bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• CUDA Available: True\n",
      "üî• GPU Count: 4\n",
      "   GPU 0: NVIDIA H100 80GB HBM3\n",
      "   GPU 1: NVIDIA H100 80GB HBM3\n",
      "   GPU 2: NVIDIA H100 80GB HBM3\n",
      "   GPU 3: NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "# Import opensloth configuration classes\n",
    "from opensloth.opensloth_config import *\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f'üî• CUDA Available: {torch.cuda.is_available()}')\n",
    "print(f'üî• GPU Count: {torch.cuda.device_count()}')\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f'   GPU {i}: {torch.cuda.get_device_name(i)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b606c272",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration Setup\n",
    "\n",
    "HyperSloth uses Pydantic models for type-safe configuration. We'll set up:\n",
    "\n",
    "1. **Data Configuration**: Dataset and tokenization settings\n",
    "2. **Training Configuration**: GPU allocation and loss calculation\n",
    "3. **Model Configuration**: Base model and LoRA parameters\n",
    "4. **Training Arguments**: Learning rate, batch size, and optimization settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6008af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global batch size: 32\n",
      "[MP] Running on 4 GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m12:42:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mopensloth_trainer.py:42\u001b[0m | \u001b[1mTraining on GPU 0 with output_dir outputs/qwen3-8b-FineTome-4gpus/\u001b[0m\n",
      "\u001b[32m12:42:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mopensloth_trainer.py:45\u001b[0m | \u001b[1müöÄ Starting total training timer\u001b[0m\n",
      "\u001b[32m12:42:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mopensloth_trainer.py:42\u001b[0m | \u001b[1mTraining on GPU 3 with output_dir outputs/qwen3-8b-FineTome-4gpus/\u001b[0m\n",
      "\u001b[32m12:42:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mopensloth_trainer.py:45\u001b[0m | \u001b[1müöÄ Starting total training timer\u001b[0m\n",
      "\u001b[32m12:42:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mopensloth_trainer.py:42\u001b[0m | \u001b[1mTraining on GPU 2 with output_dir outputs/qwen3-8b-FineTome-4gpus/\u001b[0m\n",
      "\u001b[32m12:42:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mopensloth_trainer.py:45\u001b[0m | \u001b[1müöÄ Starting total training timer\u001b[0m\n",
      "\u001b[32m12:42:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mopensloth_trainer.py:42\u001b[0m | \u001b[1mTraining on GPU 1 with output_dir outputs/qwen3-8b-FineTome-4gpus/\u001b[0m\n",
      "\u001b[32m12:42:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mopensloth_trainer.py:45\u001b[0m | \u001b[1müöÄ Starting total training timer\u001b[0m\n",
      "\u001b[32m12:42:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_loading: 19.08s\u001b[0m\n",
      "\u001b[32m12:42:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mnccl_grad_sync.py:128\u001b[0m | \u001b[1m[GPU=3] NCCL env: RANK=3, WORLD_SIZE=4, MASTER_ADDR=127.0.0.1, MASTER_PORT=29501\u001b[0m\n",
      "\u001b[32m12:42:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mnccl_grad_sync.py:132\u001b[0m | \u001b[1m[GPU=3] Setting current CUDA device to:0, os.environ['CUDA_VISIBLE_DEVICES']='3'\u001b[0m\n",
      "\u001b[32m12:42:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_loading: 19.43s\u001b[0m\n",
      "\u001b[32m12:42:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mnccl_grad_sync.py:128\u001b[0m | \u001b[1m[GPU=1] NCCL env: RANK=1, WORLD_SIZE=4, MASTER_ADDR=127.0.0.1, MASTER_PORT=29501\u001b[0m\n",
      "\u001b[32m12:42:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mnccl_grad_sync.py:132\u001b[0m | \u001b[1m[GPU=1] Setting current CUDA device to:0, os.environ['CUDA_VISIBLE_DEVICES']='1'\u001b[0m\n",
      "\u001b[32m12:42:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_loading: 21.04s\u001b[0m\n",
      "\u001b[32m12:42:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mnccl_grad_sync.py:128\u001b[0m | \u001b[1m[GPU=0] NCCL env: RANK=0, WORLD_SIZE=4, MASTER_ADDR=127.0.0.1, MASTER_PORT=29501\u001b[0m\n",
      "\u001b[32m12:42:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mnccl_grad_sync.py:132\u001b[0m | \u001b[1m[GPU=0] Setting current CUDA device to:0, os.environ['CUDA_VISIBLE_DEVICES']='0'\u001b[0m\n",
      "\u001b[32m12:42:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36minit_modules.py:50\u001b[0m | \u001b[1mModel loaded on device cuda:0, tokenizer: Qwen2TokenizerFast\u001b[0m\n",
      "\u001b[32m12:42:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_loading: 21.43s\u001b[0m\n",
      "\u001b[32m12:42:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mnccl_grad_sync.py:128\u001b[0m | \u001b[1m[GPU=2] NCCL env: RANK=2, WORLD_SIZE=4, MASTER_ADDR=127.0.0.1, MASTER_PORT=29501\u001b[0m\n",
      "\u001b[32m12:42:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mnccl_grad_sync.py:132\u001b[0m | \u001b[1m[GPU=2] Setting current CUDA device to:0, os.environ['CUDA_VISIBLE_DEVICES']='2'\u001b[0m\n",
      "\u001b[32m12:42:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36minit_modules.py:50\u001b[0m | \u001b[1mModel loaded on device cuda:0, tokenizer: Qwen2TokenizerFast\u001b[0m\n",
      "\u001b[32m12:42:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36minit_modules.py:50\u001b[0m | \u001b[1mModel loaded on device cuda:0, tokenizer: Qwen2TokenizerFast\u001b[0m\n",
      "\u001b[32m12:42:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:50\u001b[0m | \u001b[1mModel loaded on device cuda:0, tokenizer: Qwen2TokenizerFast\u001b[0m\n",
      "\u001b[32m12:42:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  lora_setup: 6.11s\u001b[0m\n",
      "\u001b[32m12:42:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_init: 27.25s\u001b[0m\n",
      "\u001b[32m12:42:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mdataset_utils.py:222\u001b[0m | \u001b[1mPreparing dataset 5d955d3f608a493e...\u001b[0m\n",
      "\u001b[32m12:42:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  lora_setup: 6.19s\u001b[0m\n",
      "\u001b[32m12:42:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_init: 27.63s\u001b[0m\n",
      "\u001b[32m12:42:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mdataset_utils.py:232\u001b[0m | \u001b[1mDataset 5d955d3f608a493e being prepared by another process, waiting...\u001b[0m\n",
      "\u001b[32m12:42:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  lora_setup: 6.16s\u001b[0m\n",
      "\u001b[32m12:42:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_init: 27.74s\u001b[0m\n",
      "\u001b[32m12:42:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mdataset_utils.py:232\u001b[0m | \u001b[1mDataset 5d955d3f608a493e being prepared by another process, waiting...\u001b[0m\n",
      "\u001b[32m12:42:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  lora_setup: 6.20s\u001b[0m\n",
      "\u001b[32m12:42:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_init: 27.77s\u001b[0m\n",
      "\u001b[32m12:42:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mdataset_utils.py:232\u001b[0m | \u001b[1mDataset 5d955d3f608a493e being prepared by another process, waiting...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "Using compiler location: .cache/unsloth_compiled_cache_3\n",
      "==((====))==  Unsloth 2025.5.9: Fast Qwen3 patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.189 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset (num_proc=52): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:10<00:00, 94.21 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 25213.88 examples/s]\n",
      "\u001b[32m12:43:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36minit_modules.py:148\u001b[0m | \u001b[1mCreating final SFTTrainer with prepared dataset...\u001b[0m\n",
      "\u001b[32m12:43:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36minit_modules.py:148\u001b[0m | \u001b[1mCreating final SFTTrainer with prepared dataset...\u001b[0m\n",
      "\u001b[32m12:43:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:148\u001b[0m | \u001b[1mCreating final SFTTrainer with prepared dataset...\u001b[0m\n",
      "\u001b[32m12:43:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36minit_modules.py:148\u001b[0m | \u001b[1mCreating final SFTTrainer with prepared dataset...\u001b[0m\n",
      "\u001b[32m12:43:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36minit_modules.py:161\u001b[0m | \u001b[1mReplacing DataCollatorForLanguageModeling with DataCollatorForSeq2Seq for better sequence handling\u001b[0m\n",
      "\u001b[32m12:43:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36minit_modules.py:169\u001b[0m | \u001b[1mTrainer setup completed successfully\u001b[0m\n",
      "\u001b[32m12:43:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  trainer_setup: 22.95s\u001b[0m\n",
      "\u001b[32m12:43:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36minit_modules.py:122\u001b[0m | \u001b[1mAdd callback ShuffleData to Trainer UnslothSFTTrainer\u001b[0m\n",
      "\u001b[32m12:43:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  trainer_creation: 22.96s\u001b[0m\n",
      "\u001b[32m12:43:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  total_setup: 1.0m\u001b[0m\n",
      "\u001b[32m12:43:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_and_training_setup: 1.0m\u001b[0m\n",
      "\u001b[32m12:43:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mnccl_grad_sync.py:48\u001b[0m | \u001b[1m[GPU=3] NCCLGradSyncCallback initialized for rank 3/4\u001b[0m\n",
      "\u001b[32m12:43:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mopensloth_trainer.py:64\u001b[0m | \u001b[1mUsing gradient sync callback for GPU 3\u001b[0m\n",
      "\u001b[32m12:43:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36minit_modules.py:161\u001b[0m | \u001b[1mReplacing DataCollatorForLanguageModeling with DataCollatorForSeq2Seq for better sequence handling\u001b[0m\n",
      "\u001b[32m12:43:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36minit_modules.py:169\u001b[0m | \u001b[1mTrainer setup completed successfully\u001b[0m\n",
      "\u001b[32m12:43:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  trainer_setup: 23.09s\u001b[0m\n",
      "\u001b[32m12:43:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36minit_modules.py:122\u001b[0m | \u001b[1mAdd callback ShuffleData to Trainer UnslothSFTTrainer\u001b[0m\n",
      "\u001b[32m12:43:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  trainer_creation: 23.10s\u001b[0m\n",
      "\u001b[32m12:43:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  total_setup: 1.0m\u001b[0m\n",
      "\u001b[32m12:43:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_and_training_setup: 1.0m\u001b[0m\n",
      "\u001b[32m12:43:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mnccl_grad_sync.py:48\u001b[0m | \u001b[1m[GPU=2] NCCLGradSyncCallback initialized for rank 2/4\u001b[0m\n",
      "\u001b[32m12:43:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mopensloth_trainer.py:64\u001b[0m | \u001b[1mUsing gradient sync callback for GPU 2\u001b[0m\n",
      "\u001b[32m12:43:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36minit_modules.py:161\u001b[0m | \u001b[1mReplacing DataCollatorForLanguageModeling with DataCollatorForSeq2Seq for better sequence handling\u001b[0m\n",
      "\u001b[32m12:43:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36minit_modules.py:169\u001b[0m | \u001b[1mTrainer setup completed successfully\u001b[0m\n",
      "\u001b[32m12:43:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  trainer_setup: 23.11s\u001b[0m\n",
      "\u001b[32m12:43:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36minit_modules.py:122\u001b[0m | \u001b[1mAdd callback ShuffleData to Trainer UnslothSFTTrainer\u001b[0m\n",
      "\u001b[32m12:43:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  trainer_creation: 23.11s\u001b[0m\n",
      "\u001b[32m12:43:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  total_setup: 1.0m\u001b[0m\n",
      "\u001b[32m12:43:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_and_training_setup: 1.0m\u001b[0m\n",
      "\u001b[32m12:43:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mnccl_grad_sync.py:48\u001b[0m | \u001b[1m[GPU=1] NCCLGradSyncCallback initialized for rank 1/4\u001b[0m\n",
      "\u001b[32m12:43:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mopensloth_trainer.py:64\u001b[0m | \u001b[1mUsing gradient sync callback for GPU 1\u001b[0m\n",
      "\u001b[32m12:43:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:161\u001b[0m | \u001b[1mReplacing DataCollatorForLanguageModeling with DataCollatorForSeq2Seq for better sequence handling\u001b[0m\n",
      "\u001b[32m12:43:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:169\u001b[0m | \u001b[1mTrainer setup completed successfully\u001b[0m\n",
      "\u001b[32m12:43:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  trainer_setup: 23.30s\u001b[0m\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]\u001b[32m12:43:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:122\u001b[0m | \u001b[1mAdd callback ShuffleData to Trainer UnslothSFTTrainer\u001b[0m\n",
      "\u001b[32m12:43:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU3\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1müé≤ Sampler epoch 0: emitting 1000 indices\n",
      "First ids dataset samples: [776, 507, 895, 922, 33, 483, 85, 750, 354, 523]\n",
      "...Last ids: [104, 754, 142, 228, 250, 281, 759, 25, 114, 654]\u001b[0m\n",
      "\u001b[32m12:43:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  trainer_creation: 23.34s\u001b[0m\n",
      "\u001b[32m12:43:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  total_setup: 1.0m\u001b[0m\n",
      "\u001b[32m12:43:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:140\u001b[0m | \u001b[1m‚è±Ô∏è  model_and_training_setup: 1.0m\u001b[0m\n",
      "\u001b[32m12:43:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mnccl_grad_sync.py:48\u001b[0m | \u001b[1m[GPU=0] NCCLGradSyncCallback initialized for rank 0/4\u001b[0m\n",
      "\u001b[32m12:43:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mopensloth_trainer.py:64\u001b[0m | \u001b[1mUsing gradient sync callback for GPU 0\u001b[0m\n",
      "Process Process-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/anhvth5/projects/opensloth/src/opensloth/scripts/opensloth_trainer.py\", line 69, in train_on_single_gpu\n",
      "    trainer.train()\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/site-packages/transformers/trainer.py\", line 2240, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/projects/opensloth/src/opensloth/patching/inner_training_loop.py\", line 346, in _inner_training_loop\n",
      "    batch_samples, num_items_in_batch = self.get_batch_samples(\n",
      "                                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/projects/opensloth/src/opensloth/patching/get_batch_samples.py\", line 193, in get_batch_samples\n",
      "    if ft_len > max_seq_len:  # check if we can pack it\n",
      "       ^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: '>' not supported between instances of 'int' and 'NoneType'\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOCAL_RANK=3] Patching log. Dir: outputs/qwen3-8b-FineTome-4gpus/, GPUs: 4\n",
      "[LOCAL_RANK=3] Log patch initialization complete.\n",
      "üîß Patching Trainer to use RandomSamplerSeededByEpoch\n",
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "Using compiler location: .cache/unsloth_compiled_cache_2\n",
      "==((====))==  Unsloth 2025.5.9: Fast Qwen3 patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.189 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "[LOCAL_RANK=2] Patching log. Dir: outputs/qwen3-8b-FineTome-4gpus/, GPUs: 4\n",
      "[LOCAL_RANK=2] Log patch initialization complete.\n",
      "üîß Patching Trainer to use RandomSamplerSeededByEpoch\n",
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "Using compiler location: .cache/unsloth_compiled_cache_1\n",
      "==((====))==  Unsloth 2025.5.9: Fast Qwen3 patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.189 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "[LOCAL_RANK=1] Patching log. Dir: outputs/qwen3-8b-FineTome-4gpus/, GPUs: 4\n",
      "[LOCAL_RANK=1] Log patch initialization complete.\n",
      "üîß Patching Trainer to use RandomSamplerSeededByEpoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]\u001b[32m12:43:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU2\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1müé≤ Sampler epoch 0: emitting 1000 indices\n",
      "First ids dataset samples: [776, 507, 895, 922, 33, 483, 85, 750, 354, 523]\n",
      "...Last ids: [104, 754, 142, 228, 250, 281, 759, 25, 114, 654]\u001b[0m\n",
      "Process Process-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/anhvth5/projects/opensloth/src/opensloth/scripts/opensloth_trainer.py\", line 69, in train_on_single_gpu\n",
      "    trainer.train()\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/site-packages/transformers/trainer.py\", line 2240, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/projects/opensloth/src/opensloth/patching/inner_training_loop.py\", line 346, in _inner_training_loop\n",
      "    batch_samples, num_items_in_batch = self.get_batch_samples(\n",
      "                                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/projects/opensloth/src/opensloth/patching/get_batch_samples.py\", line 193, in get_batch_samples\n",
      "    if ft_len > max_seq_len:  # check if we can pack it\n",
      "       ^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: '>' not supported between instances of 'int' and 'NoneType'\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]\u001b[32m12:43:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1müé≤ Sampler epoch 0: emitting 1000 indices\n",
      "First ids dataset samples: [776, 507, 895, 922, 33, 483, 85, 750, 354, 523]\n",
      "...Last ids: [104, 754, 142, 228, 250, 281, 759, 25, 114, 654]\u001b[0m\n",
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/anhvth5/projects/opensloth/src/opensloth/scripts/opensloth_trainer.py\", line 69, in train_on_single_gpu\n",
      "    trainer.train()\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/site-packages/transformers/trainer.py\", line 2240, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/projects/opensloth/src/opensloth/patching/inner_training_loop.py\", line 346, in _inner_training_loop\n",
      "    batch_samples, num_items_in_batch = self.get_batch_samples(\n",
      "                                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/projects/opensloth/src/opensloth/patching/get_batch_samples.py\", line 193, in get_batch_samples\n",
      "    if ft_len > max_seq_len:  # check if we can pack it\n",
      "       ^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: '>' not supported between instances of 'int' and 'NoneType'\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]\u001b[32m12:43:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:21\u001b[0m | \u001b[1müîÑ Starting epoch 1\u001b[0m\n",
      "\u001b[32m12:43:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1müé≤ Sampler epoch 0: emitting 1000 indices\n",
      "First ids dataset samples: [776, 507, 895, 922, 33, 483, 85, 750, 354, 523]\n",
      "...Last ids: [104, 754, 142, 228, 250, 281, 759, 25, 114, 654]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "Using compiler location: .cache/unsloth_compiled_cache_0\n",
      "==((====))==  Unsloth 2025.5.9: Fast Qwen3 patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.189 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "[LOCAL_RANK=0] Patching log. Dir: outputs/qwen3-8b-FineTome-4gpus/, GPUs: 4\n",
      "[LOCAL_RANK=0] Log patch initialization complete.\n",
      "üîß Patching Trainer to use RandomSamplerSeededByEpoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m12:43:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:28\u001b[0m | \u001b[1müìã Dataloader examples logged to .log/dataloader_examples.html\u001b[0m\n",
      "\u001b[32m12:43:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1müé≤ Sampler epoch 0: emitting 1000 indices\n",
      "First ids dataset samples: [776, 507, 895, 922, 33, 483, 85, 750, 354, 523]\n",
      "...Last ids: [104, 754, 142, 228, 250, 281, 759, 25, 114, 654]\u001b[0m\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/anhvth5/projects/opensloth/src/opensloth/scripts/opensloth_trainer.py\", line 69, in train_on_single_gpu\n",
      "    trainer.train()\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/site-packages/transformers/trainer.py\", line 2240, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/projects/opensloth/src/opensloth/patching/inner_training_loop.py\", line 346, in _inner_training_loop\n",
      "    batch_samples, num_items_in_batch = self.get_batch_samples(\n",
      "                                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/projects/opensloth/src/opensloth/patching/get_batch_samples.py\", line 193, in get_batch_samples\n",
      "    if ft_len > max_seq_len:  # check if we can pack it\n",
      "       ^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: '>' not supported between instances of 'int' and 'NoneType'\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXAMPLE #1 ===\n",
      "\u001b[92m<|im_start|>user\n",
      "How does the structure and function of the small intestine relate to nutrient absorption?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\u001b[0m\u001b[93m<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "The small intestine is a crucial organ in the digestive system, responsible for the absorption of nutrients from the food we consume. Its structure and function are intricately related to its role in nutrient absorption.\n",
      "\n",
      "1. Length and surface area: The small intestine is approximately 6 meters (20 feet) long, which provides an extensive surface area for nutrient absorption. The inner lining of the small intestine, known as the mucosa, is covered with tiny, finger-like projections called villi. These villi further increase the surface area for absorption. Each villus is covered with even smaller projections called microvilli, which form the brush border. This extensive surface area allows for efficient absorption of nutrients.\n",
      "\n",
      "2. Specialized cells: The small intestine is lined with specialized cells called enterocytes. These cells have specific transporters and enzymes on their surface that help in the absorption of various nutrients such as carbohydrates, proteins, and fats. For example, enterocytes have transporters for glucose,... (truncated)\u001b[0m\u001b[92m\u001b[0m\n",
      "\n",
      "More training debug examples written to .log/dataloader_examples.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank3]:[W609 12:43:23.433680377 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "[rank2]:[W609 12:43:23.588244372 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "[rank1]:[W609 12:43:23.601629776 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "[rank0]:[W609 12:43:24.507602721 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in training, terminating all processes\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error in training",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 79\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     78\u001b[39m     opensloth_config, training_config = get_configs()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[43mrun_mp_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopensloth_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopensloth_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/opensloth/src/opensloth/scripts/opensloth_trainer.py:239\u001b[39m, in \u001b[36mrun_mp_training\u001b[39m\u001b[34m(gpus, opensloth_config, training_config)\u001b[39m\n\u001b[32m    237\u001b[39m         p.terminate()\n\u001b[32m    238\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mError in training, terminating all processes\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mError in training\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    241\u001b[39m     processes.remove(proc)\n",
      "\u001b[31mException\u001b[39m: Error in training"
     ]
    }
   ],
   "source": [
    "# %%writefile train_opensloth.py\n",
    "from transformers.training_args import TrainingArguments\n",
    "from opensloth.scripts.opensloth_trainer import run_mp_training, setup_envs\n",
    "from opensloth.opensloth_config import (\n",
    "    OpenSlothConfig,\n",
    "    HFDatasetConfig,\n",
    "    GpuConfig,\n",
    "    FastModelArgs,\n",
    ")\n",
    "\n",
    "\n",
    "# # Main configuration using Pydantic models\n",
    "def get_configs() -> tuple[OpenSlothConfig, TrainingArguments]:\n",
    "    # Important: do not import transformers/unsloth related modules at the top level\n",
    "    from transformers import TrainingArguments\n",
    "\n",
    "    opensloth_config = OpenSlothConfig(\n",
    "        data=HFDatasetConfig(\n",
    "            tokenizer_name=\"Qwen/Qwen3-8B\",\n",
    "            chat_template=\"qwen3\",\n",
    "            instruction_part=\"<|im_start|>user\\n\",\n",
    "            response_part=\"<|im_start|>assistant\\n\",\n",
    "            num_samples=1000,\n",
    "            nproc=52,\n",
    "            max_seq_length=4096,\n",
    "            source_type=\"hf\",\n",
    "            dataset_name=\"mlabonne/FineTome-100k\",\n",
    "            split=\"train\",\n",
    "        ),\n",
    "        GpuConfig=GpuConfig(\n",
    "            gpus=[0, 1, 2, 3],\n",
    "        ),\n",
    "        fast_model_args=FastModelArgs(\n",
    "            model_name=\"unsloth/Qwen3-8B-bnb-4bit\",\n",
    "            max_seq_length=4096,\n",
    "            load_in_4bit=True,\n",
    "        ),\n",
    "        lora_args=LoraArgs(\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            target_modules=[\n",
    "                \"q_proj\",\n",
    "                \"k_proj\",\n",
    "                \"v_proj\",\n",
    "                \"o_proj\",\n",
    "                \"gate_proj\",\n",
    "                \"up_proj\",\n",
    "                \"down_proj\",\n",
    "            ],\n",
    "            lora_dropout=0,\n",
    "            bias=\"none\",\n",
    "            use_rslora=False,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # # Training arguments using Pydantic model\n",
    "    training_config = TrainingArguments(\n",
    "        output_dir=\"outputs/qwen3-8b-FineTome-4gpus/\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=8,\n",
    "        learning_rate=1e-5,\n",
    "        logging_steps=1,\n",
    "        num_train_epochs=1,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        warmup_steps=5,\n",
    "        save_total_limit=1,\n",
    "        weight_decay=0.01,\n",
    "        optim=\"adamw_8bit\",\n",
    "        seed=3407,\n",
    "        report_to=\"tensorboard\",  # tensorboard or wawndb\n",
    "    )\n",
    "    setup_envs(opensloth_config, training_config)\n",
    "    return opensloth_config, training_config\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    opensloth_config, training_config = get_configs()\n",
    "    run_mp_training(opensloth_config.GpuConfig.gpus, opensloth_config, training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7f33c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global batch size: 128\n",
      "[MP] Running on 4 GPUs\n",
      "Global batch size: 128\n",
      "[MP] Running on 4 GPUs\n",
      "Global batch size: 128\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "[MP] Running on 4 GPUs\n",
      "Global batch size: 128\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "[MP] Running on 4 GPUs\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/spawn.py\", line 131, in _main\n",
      "    prepare(preparation_data)\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/spawn.py\", line 246, in prepare\n",
      "    _fixup_main_from_path(data['init_main_from_path'])\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/spawn.py\", line 297, in _fixup_main_from_path\n",
      "Global batch size: 128\n",
      "    main_content = runpy.run_path(main_path,\n",
      "[MP] Running on 4 GPUs\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen runpy>\", line 291, in run_path\n",
      "  File \"<frozen runpy>\", line 98, in _run_module_code\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/anhvth5/projects/opensloth/train_opensloth.py\", line 67, in <module>\n",
      "    run_mp_training(\n",
      "  File \"/home/anhvth5/projects/opensloth/src/opensloth/scripts/opensloth_trainer.py\", line 228, in run_mp_training\n",
      "    p.start()\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/process.py\", line 121, in start\n",
      "Traceback (most recent call last):\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "                  ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/context.py\", line 224, in _Popen\n",
      "Traceback (most recent call last):\n",
      "    return _default_context.get_context().Process._Popen(process_obj)\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/context.py\", line 288, in _Popen\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    return Popen(process_obj)\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/spawn.py\", line 131, in _main\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "    prepare(preparation_data)\n",
      "    super().__init__(process_obj)\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/popen_fork.py\", line 19, in __init__\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/spawn.py\", line 246, in prepare\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/spawn.py\", line 131, in _main\n",
      "    self._launch(process_obj)\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n",
      "    _fixup_main_from_path(data['init_main_from_path'])\n",
      "    prep_data = spawn.get_preparation_data(process_obj._name)\n",
      "    prepare(preparation_data)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/spawn.py\", line 297, in _fixup_main_from_path\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/spawn.py\", line 246, in prepare\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/spawn.py\", line 164, in get_preparation_data\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    _check_not_importing_main()\n",
      "    _fixup_main_from_path(data['init_main_from_path'])\n",
      "    main_content = runpy.run_path(main_path,\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/spawn.py\", line 140, in _check_not_importing_main\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/spawn.py\", line 297, in _fixup_main_from_path\n",
      "  File \"<frozen runpy>\", line 291, in run_path\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"<frozen runpy>\", line 98, in _run_module_code\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "    raise RuntimeError('''\n",
      "  File \"/home/anhvth5/projects/opensloth/train_opensloth.py\", line 67, in <module>\n",
      "RuntimeError: \n",
      "        An attempt has been made to start a new process before the\n",
      "        current process has finished its bootstrapping phase.\n",
      "\n",
      "        This probably means that you are not using fork to start your\n",
      "        child processes and you have forgotten to use the proper idiom\n",
      "        in the main module:\n",
      "\n",
      "            if __name__ == '__main__':\n",
      "                freeze_support()\n",
      "                ...\n",
      "\n",
      "        The \"freeze_support()\" line can be omitted if the program\n",
      "        is not going to be frozen to produce an executable.\n",
      "\n",
      "        To fix this issue, refer to the \"Safe importing of main module\"\n",
      "        section in https://docs.python.org/3/library/multiprocessing.html\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "    main_content = runpy.run_path(main_path,\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/spawn.py\", line 131, in _main\n",
      "    run_mp_training(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/projects/opensloth/src/opensloth/scripts/opensloth_trainer.py\", line 228, in run_mp_training\n",
      "  File \"<frozen runpy>\", line 291, in run_path\n",
      "  File \"<frozen runpy>\", line 98, in _run_module_code\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/anhvth5/projects/opensloth/train_opensloth.py\", line 67, in <module>\n",
      "    prepare(preparation_data)\n",
      "    p.start()\n",
      "    run_mp_training(\n",
      "  File \"/home/anhvth5/projects/opensloth/src/opensloth/scripts/opensloth_trainer.py\", line 228, in run_mp_training\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/spawn.py\", line 246, in prepare\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/process.py\", line 121, in start\n",
      "    self._popen = self._Popen(self)\n",
      "    p.start()\n",
      "    _fixup_main_from_path(data['init_main_from_path'])\n",
      "                  ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/process.py\", line 121, in start\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/context.py\", line 224, in _Popen\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/spawn.py\", line 297, in _fixup_main_from_path\n",
      "    self._popen = self._Popen(self)\n",
      "    return _default_context.get_context().Process._Popen(process_obj)\n",
      "                  ^^^^^^^^^^^^^^^^^\n",
      "    main_content = runpy.run_path(main_path,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/context.py\", line 224, in _Popen\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/context.py\", line 288, in _Popen\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen runpy>\", line 291, in run_path\n",
      "  File \"<frozen runpy>\", line 98, in _run_module_code\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/anhvth5/projects/opensloth/train_opensloth.py\", line 67, in <module>\n",
      "    return _default_context.get_context().Process._Popen(process_obj)\n",
      "    return Popen(process_obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    run_mp_training(\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/context.py\", line 288, in _Popen\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/projects/opensloth/src/opensloth/scripts/opensloth_trainer.py\", line 228, in run_mp_training\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n",
      "    super().__init__(process_obj)\n",
      "    return Popen(process_obj)\n",
      "    p.start()\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/popen_fork.py\", line 19, in __init__\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/process.py\", line 121, in start\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n",
      "    self._launch(process_obj)\n",
      "    super().__init__(process_obj)\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/popen_fork.py\", line 19, in __init__\n",
      "                  ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/context.py\", line 224, in _Popen\n",
      "    prep_data = spawn.get_preparation_data(process_obj._name)\n",
      "    self._launch(process_obj)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/spawn.py\", line 164, in get_preparation_data\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n",
      "    return _default_context.get_context().Process._Popen(process_obj)\n",
      "    prep_data = spawn.get_preparation_data(process_obj._name)\n",
      "    _check_not_importing_main()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/context.py\", line 288, in _Popen\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/spawn.py\", line 140, in _check_not_importing_main\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/spawn.py\", line 164, in get_preparation_data\n",
      "    raise RuntimeError('''\n",
      "RuntimeError: \n",
      "        An attempt has been made to start a new process before the\n",
      "        current process has finished its bootstrapping phase.\n",
      "\n",
      "        This probably means that you are not using fork to start your\n",
      "        child processes and you have forgotten to use the proper idiom\n",
      "        in the main module:\n",
      "\n",
      "            if __name__ == '__main__':\n",
      "                freeze_support()\n",
      "                ...\n",
      "\n",
      "        The \"freeze_support()\" line can be omitted if the program\n",
      "        is not going to be frozen to produce an executable.\n",
      "\n",
      "        To fix this issue, refer to the \"Safe importing of main module\"\n",
      "        section in https://docs.python.org/3/library/multiprocessing.html\n",
      "            return Popen(process_obj)\n",
      "    _check_not_importing_main()\n",
      "\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/spawn.py\", line 140, in _check_not_importing_main\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n",
      "    super().__init__(process_obj)\n",
      "    raise RuntimeError('''\n",
      "RuntimeError: \n",
      "        An attempt has been made to start a new process before the\n",
      "        current process has finished its bootstrapping phase.\n",
      "\n",
      "        This probably means that you are not using fork to start your\n",
      "        child processes and you have forgotten to use the proper idiom\n",
      "        in the main module:\n",
      "\n",
      "            if __name__ == '__main__':\n",
      "                freeze_support()\n",
      "                ...\n",
      "\n",
      "        The \"freeze_support()\" line can be omitted if the program\n",
      "        is not going to be frozen to produce an executable.\n",
      "\n",
      "        To fix this issue, refer to the \"Safe importing of main module\"\n",
      "        section in https://docs.python.org/3/library/multiprocessing.html\n",
      "          File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/popen_fork.py\", line 19, in __init__\n",
      "\n",
      "    self._launch(process_obj)\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n",
      "    prep_data = spawn.get_preparation_data(process_obj._name)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/spawn.py\", line 164, in get_preparation_data\n",
      "    _check_not_importing_main()\n",
      "  File \"/home/anhvth5/miniconda3/envs/opensloth_env/lib/python3.11/multiprocessing/spawn.py\", line 140, in _check_not_importing_main\n",
      "    raise RuntimeError('''\n",
      "RuntimeError: \n",
      "        An attempt has been made to start a new process before the\n",
      "        current process has finished its bootstrapping phase.\n",
      "\n",
      "        This probably means that you are not using fork to start your\n",
      "        child processes and you have forgotten to use the proper idiom\n",
      "        in the main module:\n",
      "\n",
      "            if __name__ == '__main__':\n",
      "                freeze_support()\n",
      "                ...\n",
      "\n",
      "        The \"freeze_support()\" line can be omitted if the program\n",
      "        is not going to be frozen to produce an executable.\n",
      "\n",
      "        To fix this issue, refer to the \"Safe importing of main module\"\n",
      "        section in https://docs.python.org/3/library/multiprocessing.html\n",
      "        \n",
      "Error in training, terminating all processes\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anhvth5/projects/opensloth/train_opensloth.py\", line 67, in <module>\n",
      "    run_mp_training(\n",
      "  File \"/home/anhvth5/projects/opensloth/src/opensloth/scripts/opensloth_trainer.py\", line 239, in run_mp_training\n",
      "    raise Exception(\"Error in training\")\n",
      "Exception: Error in training\n"
     ]
    }
   ],
   "source": [
    "!python train_opensloth.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b45e70",
   "metadata": {},
   "source": [
    "### Compare with unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dacf623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from llm_utils import *\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ee347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'source', 'score', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad15291",
   "metadata": {},
   "source": [
    "# Unsloth Main\n",
    " now copy code from https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(14B)-Reasoning-Conversational.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7618414b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/train_unsloth.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/train_unsloth.py\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Set visible devices for training\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=\"unsloth/Qwen3-8B-bnb-4bit\",\n",
    "        max_seq_length=4096,\n",
    "        load_in_4bit=True,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "        use_rslora=False,\n",
    ")\n",
    "os.environ['HYPERSLOTH_LOCAL_RANK'] = '0'  # Set local rank for distributed training\n",
    "from opensloth.dataset_utils import get_tokenized_dataset\n",
    "\n",
    "# Get the tokenized dataset\n",
    "from opensloth.opensloth_config import HFDatasetConfig\n",
    "data = HFDatasetConfig(**{'tokenizer_name': 'Qwen/Qwen3-8B',\n",
    " 'chat_template': 'qwen3',\n",
    " 'instruction_part': '<|im_start|>user\\n',\n",
    " 'response_part': '<|im_start|>assistant\\n',\n",
    " 'num_samples': 10000,\n",
    " 'nproc': 52,\n",
    " 'max_seq_length': 4096,\n",
    " 'source_type': 'hf',\n",
    " 'dataset_name': 'mlabonne/FineTome-100k',\n",
    " 'split': 'train'})\n",
    "\n",
    "tokenized_dataset = get_tokenized_dataset(data)\n",
    "tokenized_dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = tokenized_dataset, # Use the tokenized dataset\n",
    "    eval_dataset = None, # Can set up evaluation!\n",
    "    args = SFTConfig(\n",
    "        output_dir = \"outputs/qwen3-8b-FineTome-unsloth/\",\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 8*4, # *4 to match 128 global batch size\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        learning_rate = 1e-5, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        # seed = 3407,\n",
    "        report_to = \"tensorboard\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "\n",
    "from opensloth.patching.patch_sampler import RandomSamplerSeededByEpoch\n",
    "from fastcore.all import patch\n",
    "\n",
    "\n",
    "@patch\n",
    "def _get_train_sampler(\n",
    "    self: type(trainer), train_dataset=None\n",
    ") -> RandomSamplerSeededByEpoch:\n",
    "    \"\"\"Get a custom sampler for the training dataset.\"\"\"\n",
    "    if train_dataset is None:\n",
    "        train_dataset = self.train_dataset\n",
    "\n",
    "    print(f\"Using custom sampler for {train_dataset.__class__.__name__}\")\n",
    "    return RandomSamplerSeededByEpoch(train_dataset)  # type: ignore\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71a4e41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.5.9: Fast Qwen3 patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.189 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth 2025.5.9 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n",
      "\u001b[32m10:12:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mdataset_utils.py:222\u001b[0m | \u001b[1mPreparing dataset 1d8d794fd6fc0ba8...\u001b[0m\n",
      "Tokenizing dataset (num_proc=52): 100%|‚ñà| 10000/10000 [00:12<00:00, 817.88 examp\n",
      "Saving the dataset (1/1 shards): 100%|‚ñà| 10000/10000 [00:00<00:00, 79043.41 exam\n",
      "Using custom sampler for Dataset\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 10,000 | Num Epochs = 1 | Total steps = 79\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 32\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 32 x 1) = 128\n",
      " \"-____-\"     Trainable parameters = 21,823,488/8,000,000,000 (0.27% trained)\n",
      "  0%|                                                    | 0/79 [00:00<?, ?it/s]\u001b[32m10:13:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1müé≤ Sampler epoch 0: emitting 10000 indices\n",
      "First ids: [3771, 6672, 7261, 760, 3779, 1772, 7509, 2679, 2305, 9215]\n",
      "...Last ids: [9674, 1424, 8935, 1679, 2286, 3657, 4012, 4506, 409, 1824]\u001b[0m\n",
      "Unsloth: Will smartly offload gradients to save VRAM!\n",
      "{'loss': 1.4315, 'grad_norm': 0.5605850219726562, 'learning_rate': 0.0, 'epoch': 0.01}\n",
      "{'loss': 1.4054, 'grad_norm': 0.5641970634460449, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.03}\n",
      "{'loss': 1.4316, 'grad_norm': 0.5379549860954285, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.04}\n",
      "{'loss': 1.3929, 'grad_norm': 0.5470368266105652, 'learning_rate': 6e-06, 'epoch': 0.05}\n",
      "{'loss': 1.3516, 'grad_norm': 0.5152116417884827, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.06}\n",
      "{'loss': 1.3976, 'grad_norm': 0.5382339358329773, 'learning_rate': 1e-05, 'epoch': 0.08}\n",
      "{'loss': 1.3823, 'grad_norm': 0.5207858681678772, 'learning_rate': 9.864864864864865e-06, 'epoch': 0.09}\n",
      "{'loss': 1.3293, 'grad_norm': 0.6039386987686157, 'learning_rate': 9.729729729729732e-06, 'epoch': 0.1}\n",
      "{'loss': 1.3821, 'grad_norm': 0.520772397518158, 'learning_rate': 9.594594594594594e-06, 'epoch': 0.12}\n",
      "{'loss': 1.4099, 'grad_norm': 0.5840117335319519, 'learning_rate': 9.45945945945946e-06, 'epoch': 0.13}\n",
      "{'loss': 1.3885, 'grad_norm': 0.6291170716285706, 'learning_rate': 9.324324324324325e-06, 'epoch': 0.14}\n",
      "{'loss': 1.4085, 'grad_norm': 0.6209155917167664, 'learning_rate': 9.189189189189191e-06, 'epoch': 0.15}\n",
      "{'loss': 1.3735, 'grad_norm': 0.672868013381958, 'learning_rate': 9.054054054054054e-06, 'epoch': 0.17}\n",
      "{'loss': 1.3221, 'grad_norm': 0.5535929799079895, 'learning_rate': 8.91891891891892e-06, 'epoch': 0.18}\n",
      "{'loss': 1.3658, 'grad_norm': 0.6181535720825195, 'learning_rate': 8.783783783783785e-06, 'epoch': 0.19}\n",
      "{'loss': 1.3175, 'grad_norm': 0.6390764713287354, 'learning_rate': 8.64864864864865e-06, 'epoch': 0.2}\n",
      "{'loss': 1.3799, 'grad_norm': 0.6691280007362366, 'learning_rate': 8.513513513513514e-06, 'epoch': 0.22}\n",
      "{'loss': 1.37, 'grad_norm': 0.6703071594238281, 'learning_rate': 8.378378378378378e-06, 'epoch': 0.23}\n",
      "{'loss': 1.4143, 'grad_norm': 0.6979341506958008, 'learning_rate': 8.243243243243245e-06, 'epoch': 0.24}\n",
      "{'loss': 1.3208, 'grad_norm': 0.6842060089111328, 'learning_rate': 8.108108108108109e-06, 'epoch': 0.26}\n",
      "{'loss': 1.3566, 'grad_norm': 0.7776806354522705, 'learning_rate': 7.972972972972974e-06, 'epoch': 0.27}\n",
      "{'loss': 1.2803, 'grad_norm': 0.7343313097953796, 'learning_rate': 7.837837837837838e-06, 'epoch': 0.28}\n",
      "{'loss': 1.4086, 'grad_norm': 0.7141842246055603, 'learning_rate': 7.702702702702704e-06, 'epoch': 0.29}\n",
      "{'loss': 1.4342, 'grad_norm': 0.7778894305229187, 'learning_rate': 7.567567567567569e-06, 'epoch': 0.31}\n",
      "{'loss': 1.3958, 'grad_norm': 0.7965384721755981, 'learning_rate': 7.4324324324324324e-06, 'epoch': 0.32}\n",
      "{'loss': 1.2916, 'grad_norm': 0.7536203861236572, 'learning_rate': 7.297297297297298e-06, 'epoch': 0.33}\n",
      "{'loss': 1.336, 'grad_norm': 0.8234443664550781, 'learning_rate': 7.162162162162163e-06, 'epoch': 0.35}\n",
      "{'loss': 1.3186, 'grad_norm': 0.7428470849990845, 'learning_rate': 7.027027027027028e-06, 'epoch': 0.36}\n",
      "{'loss': 1.2597, 'grad_norm': 0.7508196830749512, 'learning_rate': 6.891891891891892e-06, 'epoch': 0.37}\n",
      "{'loss': 1.3121, 'grad_norm': 0.8129947185516357, 'learning_rate': 6.7567567567567575e-06, 'epoch': 0.38}\n",
      "{'loss': 1.3492, 'grad_norm': 0.8423631191253662, 'learning_rate': 6.621621621621622e-06, 'epoch': 0.4}\n",
      "{'loss': 1.3428, 'grad_norm': 0.7696203589439392, 'learning_rate': 6.486486486486487e-06, 'epoch': 0.41}\n",
      "{'loss': 1.289, 'grad_norm': 0.7877718210220337, 'learning_rate': 6.351351351351351e-06, 'epoch': 0.42}\n",
      "{'loss': 1.2467, 'grad_norm': 0.8627036213874817, 'learning_rate': 6.2162162162162164e-06, 'epoch': 0.44}\n",
      "{'loss': 1.2756, 'grad_norm': 0.7700051069259644, 'learning_rate': 6.081081081081082e-06, 'epoch': 0.45}\n",
      "{'loss': 1.3139, 'grad_norm': 0.8254755735397339, 'learning_rate': 5.945945945945947e-06, 'epoch': 0.46}\n",
      "{'loss': 1.287, 'grad_norm': 0.874988853931427, 'learning_rate': 5.810810810810811e-06, 'epoch': 0.47}\n",
      "{'loss': 1.2887, 'grad_norm': 0.8422302603721619, 'learning_rate': 5.675675675675676e-06, 'epoch': 0.49}\n",
      "{'loss': 1.2971, 'grad_norm': 0.8408057689666748, 'learning_rate': 5.540540540540541e-06, 'epoch': 0.5}\n",
      "{'loss': 1.262, 'grad_norm': 0.7886468172073364, 'learning_rate': 5.405405405405406e-06, 'epoch': 0.51}\n",
      "{'loss': 1.2468, 'grad_norm': 0.7758446931838989, 'learning_rate': 5.2702702702702705e-06, 'epoch': 0.52}\n",
      "{'loss': 1.334, 'grad_norm': 0.8818156123161316, 'learning_rate': 5.135135135135135e-06, 'epoch': 0.54}\n",
      "{'loss': 1.2665, 'grad_norm': 0.8695828914642334, 'learning_rate': 5e-06, 'epoch': 0.55}\n",
      "{'loss': 1.1955, 'grad_norm': 0.8037291169166565, 'learning_rate': 4.864864864864866e-06, 'epoch': 0.56}\n",
      "{'loss': 1.2323, 'grad_norm': 0.8066659569740295, 'learning_rate': 4.72972972972973e-06, 'epoch': 0.58}\n",
      "{'loss': 1.2319, 'grad_norm': 0.8392208814620972, 'learning_rate': 4.594594594594596e-06, 'epoch': 0.59}\n",
      "{'loss': 1.2587, 'grad_norm': 0.8837226629257202, 'learning_rate': 4.45945945945946e-06, 'epoch': 0.6}\n",
      "{'loss': 1.1955, 'grad_norm': 0.8095787167549133, 'learning_rate': 4.324324324324325e-06, 'epoch': 0.61}\n",
      "{'loss': 1.2209, 'grad_norm': 0.8844156265258789, 'learning_rate': 4.189189189189189e-06, 'epoch': 0.63}\n",
      "{'loss': 1.2977, 'grad_norm': 0.8354104161262512, 'learning_rate': 4.0540540540540545e-06, 'epoch': 0.64}\n",
      "{'loss': 1.2105, 'grad_norm': 0.8006124496459961, 'learning_rate': 3.918918918918919e-06, 'epoch': 0.65}\n",
      "{'loss': 1.2151, 'grad_norm': 0.8618671894073486, 'learning_rate': 3.7837837837837844e-06, 'epoch': 0.67}\n",
      "{'loss': 1.1754, 'grad_norm': 0.8520645499229431, 'learning_rate': 3.648648648648649e-06, 'epoch': 0.68}\n",
      "{'loss': 1.2225, 'grad_norm': 0.7846698760986328, 'learning_rate': 3.513513513513514e-06, 'epoch': 0.69}\n",
      "{'loss': 1.2059, 'grad_norm': 0.8117104768753052, 'learning_rate': 3.3783783783783788e-06, 'epoch': 0.7}\n",
      "{'loss': 1.2171, 'grad_norm': 0.8705743551254272, 'learning_rate': 3.2432432432432437e-06, 'epoch': 0.72}\n",
      "{'loss': 1.2043, 'grad_norm': 0.8538356423377991, 'learning_rate': 3.1081081081081082e-06, 'epoch': 0.73}\n",
      "{'loss': 1.1724, 'grad_norm': 0.8408896327018738, 'learning_rate': 2.9729729729729736e-06, 'epoch': 0.74}\n",
      "{'loss': 1.2198, 'grad_norm': 0.812709629535675, 'learning_rate': 2.837837837837838e-06, 'epoch': 0.76}\n",
      "{'loss': 1.1405, 'grad_norm': 0.8620516061782837, 'learning_rate': 2.702702702702703e-06, 'epoch': 0.77}\n",
      "{'loss': 1.13, 'grad_norm': 0.8522079586982727, 'learning_rate': 2.5675675675675675e-06, 'epoch': 0.78}\n",
      "{'loss': 1.0935, 'grad_norm': 0.7529408931732178, 'learning_rate': 2.432432432432433e-06, 'epoch': 0.79}\n",
      "{'loss': 1.1634, 'grad_norm': 0.8132103681564331, 'learning_rate': 2.297297297297298e-06, 'epoch': 0.81}\n",
      "{'loss': 1.2178, 'grad_norm': 0.7810198068618774, 'learning_rate': 2.1621621621621623e-06, 'epoch': 0.82}\n",
      "{'loss': 1.2228, 'grad_norm': 0.8255438804626465, 'learning_rate': 2.0270270270270273e-06, 'epoch': 0.83}\n",
      "{'loss': 1.1568, 'grad_norm': 0.8186894655227661, 'learning_rate': 1.8918918918918922e-06, 'epoch': 0.84}\n",
      "{'loss': 1.2242, 'grad_norm': 0.8622767329216003, 'learning_rate': 1.756756756756757e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1955, 'grad_norm': 0.7387193441390991, 'learning_rate': 1.6216216216216219e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1751, 'grad_norm': 0.8728275895118713, 'learning_rate': 1.4864864864864868e-06, 'epoch': 0.88}\n",
      "{'loss': 1.219, 'grad_norm': 0.7533606886863708, 'learning_rate': 1.3513513513513515e-06, 'epoch': 0.9}\n",
      "{'loss': 1.1637, 'grad_norm': 0.7901483774185181, 'learning_rate': 1.2162162162162164e-06, 'epoch': 0.91}\n",
      "{'loss': 1.2317, 'grad_norm': 0.789584755897522, 'learning_rate': 1.0810810810810812e-06, 'epoch': 0.92}\n",
      "{'loss': 1.1399, 'grad_norm': 0.8187168836593628, 'learning_rate': 9.459459459459461e-07, 'epoch': 0.93}\n",
      "{'loss': 1.1672, 'grad_norm': 0.8506714701652527, 'learning_rate': 8.108108108108109e-07, 'epoch': 0.95}\n",
      "{'loss': 1.1798, 'grad_norm': 0.8555465340614319, 'learning_rate': 6.756756756756758e-07, 'epoch': 0.96}\n",
      "{'loss': 1.1448, 'grad_norm': 0.7535891532897949, 'learning_rate': 5.405405405405406e-07, 'epoch': 0.97}\n",
      "{'loss': 1.1707, 'grad_norm': 0.7992974519729614, 'learning_rate': 4.0540540540540546e-07, 'epoch': 0.99}\n",
      "{'loss': 1.1694, 'grad_norm': 0.8263909220695496, 'learning_rate': 2.702702702702703e-07, 'epoch': 1.0}\n",
      " 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 78/79 [31:57<00:24, 24.76s/it]\u001b[32m10:45:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:61\u001b[0m | \u001b[1müé≤ Sampler epoch 0: dataset_size=10000\n",
      "   üìã First 10 indices: [3771, 6672, 7261, 760, 3779, 1772, 7509, 2679, 2305, 9215]\n",
      "   üìã Last 10 indices: [9674, 1424, 8935, 1679, 2286, 3657, 4012, 4506, 409, 1824]\u001b[0m\n",
      "{'loss': 1.3639, 'grad_norm': 1.067948341369629, 'learning_rate': 1.3513513513513515e-07, 'epoch': 1.0}\n",
      "{'train_runtime': 1922.5289, 'train_samples_per_second': 5.201, 'train_steps_per_second': 0.041, 'train_loss': 1.277385526065585, 'epoch': 1.0}\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [32:02<00:00, 24.34s/it]\n"
     ]
    }
   ],
   "source": [
    "!python /tmp/train_unsloth.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626094db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
