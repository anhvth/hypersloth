{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1555d26d",
   "metadata": {},
   "source": [
    "# üöÄ HyperSloth Demo Training Notebook\n",
    "\n",
    "This notebook demonstrates how to fine-tune large language models using HyperSloth's multi-GPU capabilities. It's equivalent to running:\n",
    "\n",
    "```bash\n",
    "hypersloth-train examples/example_sharegpt_lora_2gpus.py\n",
    "```\n",
    "\n",
    "## What This Demo Does\n",
    "\n",
    "- **Multi-GPU Training**: Uses 2 GPUs with NCCL synchronization\n",
    "- **Adaptive Batching**: Optimizes sequence sorting and padding\n",
    "- **LoRA Fine-tuning**: Efficient parameter updates with Low-Rank Adaptation\n",
    "- **Response-only Loss**: Calculates loss only on assistant responses\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. HyperSloth installed: `pip install git+https://github.com/anhvth/HyperSloth.git`\n",
    "2. At least 2 GPUs available (adjust `gpus=[0, 1]` if needed)\n",
    "3. Sufficient VRAM (reduce batch size if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72a817bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• CUDA Available: True\n",
      "üî• GPU Count: 4\n",
      "   GPU 0: NVIDIA H100 80GB HBM3\n",
      "   GPU 1: NVIDIA H100 80GB HBM3\n",
      "   GPU 2: NVIDIA H100 80GB HBM3\n",
      "   GPU 3: NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "# Import HyperSloth configuration classes\n",
    "from HyperSloth.hypersloth_config import *\n",
    "from HyperSloth.scripts.hp_trainer import run_multiprocess_training, setup_envs\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f'üî• CUDA Available: {torch.cuda.is_available()}')\n",
    "print(f'üî• GPU Count: {torch.cuda.device_count()}')\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f'   GPU {i}: {torch.cuda.get_device_name(i)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b606c272",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration Setup\n",
    "\n",
    "HyperSloth uses Pydantic models for type-safe configuration. We'll set up:\n",
    "\n",
    "1. **Data Configuration**: Dataset and tokenization settings\n",
    "2. **Training Configuration**: GPU allocation and loss calculation\n",
    "3. **Model Configuration**: Base model and LoRA parameters\n",
    "4. **Training Arguments**: Learning rate, batch size, and optimization settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7f33c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from HyperSloth.hypersloth_config import *\n",
    "from HyperSloth.scripts.hp_trainer import run_multiprocess_training, setup_envs\n",
    "\n",
    "# Main configuration using Pydantic models\n",
    "hyper_config_model = HyperConfig(\n",
    "    data=HFDatasetConfig(\n",
    "        dataset_name=\"llamafactory/OpenThoughts-114k\",\n",
    "        split=\"train\",\n",
    "        tokenizer_name=\"Qwen/Qwen3-8B\",  # does not matter same family qwen3\n",
    "        num_samples=1000,\n",
    "        instruction_part=\"<|im_start|>user\\n\",\n",
    "        response_part=\"<|im_start|>assistant\\n\",\n",
    "        chat_template=\"chatml\",\n",
    "    ),\n",
    "    training=TrainingConfig(\n",
    "        gpus=[0, 1],\n",
    "        loss_type=\"response_only\",\n",
    "    ),\n",
    "    fast_model_args=FastModelArgs(\n",
    "        model_name=\"unsloth/Qwen3-0.6b-bnb-4bit\",\n",
    "        max_seq_length=32_000,\n",
    "        load_in_4bit=True,\n",
    "    ),\n",
    "    lora_args=LoraArgs(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "        use_rslora=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Training arguments using Pydantic model\n",
    "training_config_model = TrainingArgsConfig(\n",
    "    output_dir=\"outputs/qwen3-8b-openthought-2gpus/\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-5,\n",
    "    logging_steps=3,\n",
    "    num_train_epochs=3,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=5,\n",
    "    save_total_limit=2,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_8bit\",\n",
    "    seed=3407,\n",
    "    report_to=\"none\",  # tensorboard or wawndb\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1d9548",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Training Arguments\n",
    "\n",
    "Configure the training hyperparameters for optimal performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53b45e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global batch size: 16\n",
      "[MP] Running on 2 GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m03:31:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mhp_trainer.py:44\u001b[0m | \u001b[1müîß GPU 1 (Rank 1/1) | Model: unsloth/Qwen3-0.6b-bnb-4bit\u001b[0m\n",
      "\u001b[32m03:31:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mhp_trainer.py:50\u001b[0m | \u001b[1mTraining on GPU 1 with output_dir outputs/qwen3-8b-openthought-2gpus/\u001b[0m\n",
      "\u001b[32m03:31:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU1\u001b[0m | \u001b[36mhp_trainer.py:53\u001b[0m | \u001b[1müöÄ Starting total training timer\u001b[0m\n",
      "\u001b[32m03:31:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mhp_trainer.py:44\u001b[0m | \u001b[1müîß GPU 0 (Rank 0/1) | Model: unsloth/Qwen3-0.6b-bnb-4bit\u001b[0m\n",
      "\u001b[32m03:31:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mhp_trainer.py:50\u001b[0m | \u001b[1mTraining on GPU 0 with output_dir outputs/qwen3-8b-openthought-2gpus/\u001b[0m\n",
      "\u001b[32m03:31:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mhp_trainer.py:53\u001b[0m | \u001b[1müöÄ Starting total training timer\u001b[0m\n",
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anhvth5/miniconda3/envs/unsloth_env/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/anhvth5/miniconda3/envs/unsloth_env/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process Process-1:\n",
      "  File \"/home/anhvth5/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/fastcore/parallel.py\", line 29, in g\n",
      "    res = f(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anhvth5/projects/hypersloth/HyperSloth/scripts/hp_trainer.py\", line 223, in run_in_process\n",
      "    train_on_single_gpu(*args, **kwargs)\n",
      "  File \"/home/anhvth5/miniconda3/envs/unsloth_env/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/anhvth5/projects/hypersloth/HyperSloth/scripts/hp_trainer.py\", line 59, in train_on_single_gpu\n",
      "    trainer, model, tokenizer = setup_model_and_training(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/miniconda3/envs/unsloth_env/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/anhvth5/projects/hypersloth/HyperSloth/hp_trainer_setup.py\", line 50, in setup_model_and_training\n",
      "    _change_compiler_location()\n",
      "  File \"/home/anhvth5/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/fastcore/parallel.py\", line 29, in g\n",
      "    res = f(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/projects/hypersloth/HyperSloth/hp_trainer_setup.py\", line 19, in _change_compiler_location\n",
      "    import unsloth  # type: ignore\n",
      "    ^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/projects/hypersloth/HyperSloth/scripts/hp_trainer.py\", line 223, in run_in_process\n",
      "    train_on_single_gpu(*args, **kwargs)\n",
      "  File \"/home/anhvth5/projects/hypersloth/HyperSloth/scripts/hp_trainer.py\", line 59, in train_on_single_gpu\n",
      "    trainer, model, tokenizer = setup_model_and_training(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/__init__.py\", line 142, in <module>\n",
      "    major_version, minor_version = torch.cuda.get_device_capability()\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/projects/hypersloth/HyperSloth/hp_trainer_setup.py\", line 50, in setup_model_and_training\n",
      "    _change_compiler_location()\n",
      "  File \"/home/anhvth5/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 560, in get_device_capability\n",
      "    prop = get_device_properties(device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/projects/hypersloth/HyperSloth/hp_trainer_setup.py\", line 19, in _change_compiler_location\n",
      "    import unsloth  # type: ignore\n",
      "    ^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 576, in get_device_properties\n",
      "    _lazy_init()  # will define _get_device_properties\n",
      "    ^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/__init__.py\", line 142, in <module>\n",
      "    major_version, minor_version = torch.cuda.get_device_capability()\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 358, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "  File \"/home/anhvth5/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 560, in get_device_capability\n",
      "    prop = get_device_properties(device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
      "  File \"/home/anhvth5/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 576, in get_device_properties\n",
      "    _lazy_init()  # will define _get_device_properties\n",
      "    ^^^^^^^^^^^^\n",
      "  File \"/home/anhvth5/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 358, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in training, terminating all processes\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error in training",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m setup_envs(hyper_config_model, training_config_model)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mrun_multiprocess_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhyper_config_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyper_config_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_config_model\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/hypersloth/HyperSloth/scripts/hp_trainer.py:241\u001b[39m, in \u001b[36mrun_multiprocess_training\u001b[39m\u001b[34m(gpus, hyper_config, training_config)\u001b[39m\n\u001b[32m    239\u001b[39m         p.terminate()\n\u001b[32m    240\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mError in training, terminating all processes\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mError in training\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    243\u001b[39m     processes.remove(proc)\n",
      "\u001b[31mException\u001b[39m: Error in training"
     ]
    }
   ],
   "source": [
    "\n",
    "setup_envs(hyper_config_model, training_config_model)\n",
    "\n",
    "run_multiprocess_training(\n",
    "    hyper_config_model.training.gpus, hyper_config_model, training_config_model\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
